{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90ff4337-9948-4116-bd24-a848c71598f8",
   "metadata": {},
   "source": [
    "News Source:\n",
    "\n",
    "https://www.kaggle.com/datasets/notlucasp/financial-news-headlines/data\n",
    "\n",
    "Contents:\n",
    "\n",
    "Data scraped from CNBC contains the Headliness, last updated date, and the preview text of articles from the end of December 2017 to July 19th, 2020.\n",
    "\n",
    "Data scraped from the Guardian Business contains the Headliness and last updated date of articles from the end of December 2017 to July 19th, 2020 since the Guardian Business does not offer preview text.\n",
    "\n",
    "Data scraped from Reuters contains the Headliness, last updated date, and the preview text of articles from the end of March 2018 to July 19th, 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5680aa6-39c9-43bf-a8da-d03f0c00863d",
   "metadata": {},
   "source": [
    "Sentiment via TextBlob:\n",
    "\n",
    "not an ML-trained model like BERT or FinBERT. It’s dictionary-based → works okay for simple financial headlines but may miss sarcasm, jargon, or complex finance tone.\n",
    "\n",
    "# Financial News Agentic AI Pipeline Summary\n",
    "\n",
    "### 1. Dataset Preparation\n",
    "- Load multiple news datasets (CNBC, Guardian, Reuters).\n",
    "- Handle differences in columns (e.g., some may not have `Description`).\n",
    "\n",
    "### 2. Text Preprocessing\n",
    "- Lowercase all text.\n",
    "- Keep alphanumeric characters only.\n",
    "- Remove stopwords.\n",
    "- Output: `cleaned_text`.\n",
    "\n",
    "### 3. Topic Tagging\n",
    "- Text-based classification of each headline:\n",
    "  - `earnings`: earnings, revenue, EPS, quarter\n",
    "  - `market`: stock, shares, price\n",
    "  - `macro`: fed, inflation, GDP, rate\n",
    "  - `general`: default\n",
    "\n",
    "### 4. Sentiment Analysis\n",
    "- Use **TextBlob** model to compute polarity:\n",
    "  - `positive` if polarity > 0.1  \n",
    "  - `negative` if polarity < -0.1  \n",
    "  - `neutral` otherwise\n",
    "- Output: sentiment label per headline.\n",
    "\n",
    "### 5. Routing to Specialist Agents\n",
    "- **Earnings agent:** receives topic + sentiment → `eps_signal`, `revenue_signal`\n",
    "- **Market agent:** topic + sentiment → `market_signal`\n",
    "- **Macro agent:** topic + sentiment → `macro_signal`\n",
    "- **General agent:** topic + sentiment → `general_signal` (always 0)\n",
    "- Agents are **rule-based**; only TextBlob sentiment is model-based.\n",
    "\n",
    "### 6. Tesla Filtering\n",
    "- Select headlines mentioning `\"Tesla\"` in `Headlines` or `Description` (if exists).\n",
    "\n",
    "### 7. Weighted Aggregation\n",
    "- Combine all agent outputs for Tesla headlines.\n",
    "- Signals are multiplied by predefined weights:\n",
    "  - `eps_signal` → weight 3  \n",
    "  - `revenue_signal` → weight 2  \n",
    "  - `market_signal` → weight 2  \n",
    "  - `macro_signal` → weight 1  \n",
    "  - `general_signal` → weight 0\n",
    "\n",
    "### 8. Trade Suggestion\n",
    "- Sum weighted signals to compute total score.\n",
    "- Map total score → trade action:\n",
    "  - Total ≥ 3 → **BUY**\n",
    "  - Total ≤ -2 → **SELL**\n",
    "  - Otherwise → **HOLD**\n",
    "\n",
    "### 9. Reporting\n",
    "- Print per dataset:\n",
    "  - Number of Tesla headlines\n",
    "  - Aggregated trade suggestion (BUY/HOLD/SELL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd005694-d746-4fc9-825c-c6c30ccbd29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import os\n",
    "import shutil\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc64f87d-b358-445c-aab2-49b7332198b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache path: C:\\Users\\Administrator\\.cache\\kagglehub\\datasets\\notlucasp\\financial-news-headlines\\versions\\2\n",
      "Files moved to: C:\\Users\\Administrator\\Documents\\GitHub\\aai-520-final-project-group5\\news-datasets\\kaggle-headlines-data\n",
      "Number of files: 3\n",
      "Filenames: ['cnbc_headlines.csv', 'guardian_headlines.csv', 'reuters_headlines.csv']\n"
     ]
    }
   ],
   "source": [
    "# ------------------------\n",
    "# Setup\n",
    "# ------------------------\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# ------------------------\n",
    "# Download dataset (cached)\n",
    "# ------------------------\n",
    "path = kagglehub.dataset_download(\"notlucasp/financial-news-headlines\")\n",
    "print(\"Cache path:\", path)\n",
    "\n",
    "# ------------------------\n",
    "# Define target folder\n",
    "# ------------------------\n",
    "target_dir = os.path.abspath(os.path.join(\"..\", \"news-datasets\", \"kaggle-headlines-data\"))\n",
    "if os.path.exists(target_dir):\n",
    "    shutil.rmtree(target_dir)\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# ------------------------\n",
    "# Copy all files from versioned folder to target\n",
    "# ------------------------\n",
    "for item in os.listdir(path):\n",
    "    src_file = os.path.join(path, item)\n",
    "    if os.path.isfile(src_file):\n",
    "        shutil.copy(src_file, target_dir)\n",
    "\n",
    "# ------------------------\n",
    "# Show where files were moved, count, and filenames\n",
    "# ------------------------\n",
    "files = os.listdir(target_dir)\n",
    "print(\"Files moved to:\", target_dir)\n",
    "print(\"Number of files:\", len(files))\n",
    "print(\"Filenames:\", files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a017276-6a45-40e3-8c32-a4ac1f168f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnbc_headlines.csv\n",
      "                                          Headlines                           Time                                         Description\n",
      "Jim Cramer: A better way to invest in the Covid-19…  7:51  PM ET Fri, 17 July 2020 \"Mad Money\" host Jim Cramer recommended buying fou…\n",
      "    Cramer's lightning round: I would own Teradyne…  7:33  PM ET Fri, 17 July 2020 \"Mad Money\" host Jim Cramer rings the lightning ro…\n",
      "                                                  …                            NaN                                                   …\n",
      "Cramer's week ahead: Big week for earnings, even b…  7:25  PM ET Fri, 17 July 2020 \"We'll pay more for the earnings of the non-Covid …\n",
      "IQ Capital CEO Keith Bliss says tech and healthcar…  4:24  PM ET Fri, 17 July 2020 Keith Bliss, IQ Capital CEO, joins \"Closing Bell\" …\n"
     ]
    }
   ],
   "source": [
    "# Load cnbc CSV into dataframe\n",
    "file_path = os.path.join(target_dir, files[0])\n",
    "cnbc_raw_df = pd.read_csv(file_path)\n",
    "\n",
    "# Truncate text for display\n",
    "cnbc_display = cnbc_raw_df.copy()\n",
    "cnbc_display[\"Headlines\"] = cnbc_display[\"Headlines\"].fillna(\"\").str[:50] + \"…\"\n",
    "if \"Description\" in cnbc_display.columns:\n",
    "    cnbc_display[\"Description\"] = cnbc_display[\"Description\"].fillna(\"\").str[:50] + \"…\"\n",
    "\n",
    "# Print truncated dataframe\n",
    "print(files[0])\n",
    "print(cnbc_display.head().to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acb7aad7-fd6a-4277-bc2b-6d681aef7fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3080 entries, 0 to 3079\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Headlines    2800 non-null   object\n",
      " 1   Time         2800 non-null   object\n",
      " 2   Description  2800 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 72.3+ KB\n"
     ]
    }
   ],
   "source": [
    "cnbc_raw_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "747d2924-370a-479e-9aaf-157a511bade7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guardian_headlines.csv\n",
      "     Time                                              Headlines\n",
      "18-Jul-20      Johnson is asking Santa for a Christmas recovery…\n",
      "18-Jul-20    ‘I now fear the worst’: four grim tales of working…\n",
      "18-Jul-20    Five key areas Sunak must tackle to serve up econo…\n",
      "18-Jul-20    Covid-19 leaves firms ‘fatally ill-prepared’ for n…\n",
      "18-Jul-20 The Week in Patriarchy  \\n\\n\\n  Bacardi's 'lady vodka…\n"
     ]
    }
   ],
   "source": [
    "# Load guardian CSV into dataframe\n",
    "file_path = os.path.join(target_dir, files[1])\n",
    "guardian_raw_df = pd.read_csv(file_path)\n",
    "\n",
    "# Truncate text for display\n",
    "guardian_display = guardian_raw_df.copy()\n",
    "guardian_display[\"Headlines\"] = guardian_display[\"Headlines\"].fillna(\"\").str[:50] + \"…\"\n",
    "\n",
    "# Print truncated dataframe\n",
    "print(files[1])\n",
    "print(guardian_display.head().to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d205943-119d-48fe-bb08-5da0c8a8ab06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17800 entries, 0 to 17799\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Time       17800 non-null  object\n",
      " 1   Headlines  17800 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 278.3+ KB\n"
     ]
    }
   ],
   "source": [
    "guardian_raw_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "188ebdc9-713e-41b4-905a-46300ea44a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reuters_headlines.csv\n",
      "                                          Headlines        Time                                         Description\n",
      "TikTok considers London and other locations for he… Jul 18 2020 TikTok has been in discussions with the UK governm…\n",
      "Disney cuts ad spending on Facebook amid growing b… Jul 18 2020 Walt Disney  has become the latest company to slas…\n",
      "Trail of missing Wirecard executive leads to Belar… Jul 18 2020 Former Wirecard  chief operating officer Jan Marsa…\n",
      "Twitter says attackers downloaded data from up to … Jul 18 2020 Twitter Inc said on Saturday that hackers were abl…\n",
      "U.S. Republicans seek liability protections as cor… Jul 17 2020 A battle in the U.S. Congress over a new coronavir…\n"
     ]
    }
   ],
   "source": [
    "# Load reuters CSV into dataframe\n",
    "file_path = os.path.join(target_dir, files[2])\n",
    "reuters_raw_df = pd.read_csv(file_path)\n",
    "\n",
    "# Truncate to first 50 characters directly using .str\n",
    "reuters_display = reuters_raw_df.copy()\n",
    "reuters_display[\"Headlines\"] = reuters_display[\"Headlines\"].str[:50] + \"…\"\n",
    "if \"Description\" in reuters_display.columns:\n",
    "    reuters_display[\"Description\"] = reuters_display[\"Description\"].str[:50] + \"…\"\n",
    "\n",
    "print(files[2])\n",
    "print(reuters_display.head().to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d338fe6-f585-445d-8034-3ee56c28de83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32770 entries, 0 to 32769\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Headlines    32770 non-null  object\n",
      " 1   Time         32770 non-null  object\n",
      " 2   Description  32770 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 768.2+ KB\n"
     ]
    }
   ],
   "source": [
    "reuters_raw_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26c111b9-30a3-42a8-8f9c-abe57faab332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# Define company to filter\n",
    "# ------------------------\n",
    "company_name = \"Tesla\"  # Change this to filter a different company\n",
    "\n",
    "# ------------------------\n",
    "# Define datasets dictionary\n",
    "# ------------------------\n",
    "datasets = {\n",
    "    \"cnbc\": cnbc_raw_df,\n",
    "    \"guardian\": guardian_raw_df,\n",
    "    \"reuters\": reuters_raw_df\n",
    "}\n",
    "\n",
    "\n",
    "# Agent functions for processing financial text in the current dataset workflow\n",
    "def earnings_agent(text, sentiment):\n",
    "    eps_signal = 1 if sentiment == \"positive\" else 0\n",
    "    revenue_signal = 1 if sentiment == \"positive\" else 0\n",
    "    return {\"eps_signal\": eps_signal, \"revenue_signal\": revenue_signal}\n",
    "\n",
    "def market_agent(text, sentiment):\n",
    "    market_signal = 1 if sentiment == \"positive\" else (-1 if sentiment == \"negative\" else 0)\n",
    "    return {\"market_signal\": market_signal}\n",
    "\n",
    "def macro_agent(text, sentiment):\n",
    "    macro_signal = 1 if sentiment == \"positive\" else (-1 if sentiment == \"negative\" else 0)\n",
    "    return {\"macro_signal\": macro_signal}\n",
    "\n",
    "def general_agent(text, sentiment):\n",
    "    return {\"general_signal\": 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efec512d-4f8c-47a5-b0a5-10587e57eedc",
   "metadata": {},
   "source": [
    "## Orchestrator Class\n",
    "\n",
    "- executes data retrieval, planner, memory store???\n",
    "- routing data to process different data\n",
    "- executes aggregator\n",
    "- routes data to evaluator then optimizer\n",
    "- returns final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf381a2c-94e2-41e8-98d9-3e6b90bffa1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Dataset: cnbc --- Total Tesla headlines count: 36 Trade suggestion: SELL ---\n",
      "Non-general agent weighted scores per article:\n",
      "    Article 12: Agent: market | Weighted Article Score: -2 | Agent Output: {'market_signal': -1}\n",
      "    Article 17: Agent: market | Weighted Article Score: -2 | Agent Output: {'market_signal': -1}\n",
      "    Article 20: Agent: market | Weighted Article Score: -2 | Agent Output: {'market_signal': -1}\n",
      "    Article 25: Agent: market | Weighted Article Score: -2 | Agent Output: {'market_signal': -1}\n",
      "    Article 31: Agent: market | Weighted Article Score: 2 | Agent Output: {'market_signal': 1}\n",
      "    Article 32: Agent: market | Weighted Article Score: 2 | Agent Output: {'market_signal': 1}\n",
      "\n",
      "\n",
      "--- Dataset: guardian --- Total Tesla headlines count: 78 Trade suggestion: BUY ---\n",
      "Non-general agent weighted scores per article:\n",
      "    Article 26: Agent: earnings | Weighted Article Score: 5 | Agent Output: {'eps_signal': 1, 'revenue_signal': 1}\n",
      "    Article 38: Agent: earnings | Weighted Article Score: 5 | Agent Output: {'eps_signal': 1, 'revenue_signal': 1}\n",
      "    Article 45: Agent: market | Weighted Article Score: 2 | Agent Output: {'market_signal': 1}\n",
      "    Article 59: Agent: macro | Weighted Article Score: -1 | Agent Output: {'macro_signal': -1}\n",
      "\n",
      "\n",
      "--- Dataset: reuters --- Total Tesla headlines count: 699 Trade suggestion: BUY ---\n",
      "Non-general agent weighted scores per article:\n",
      "    Article 18: Agent: market | Weighted Article Score: 2 | Agent Output: {'market_signal': 1}\n",
      "    Article 23: Agent: market | Weighted Article Score: 2 | Agent Output: {'market_signal': 1}\n",
      "    Article 39: Agent: market | Weighted Article Score: -2 | Agent Output: {'market_signal': -1}\n",
      "    Article 46: Agent: market | Weighted Article Score: 2 | Agent Output: {'market_signal': 1}\n",
      "    Article 66: Agent: market | Weighted Article Score: 2 | Agent Output: {'market_signal': 1}\n",
      "    Article 85: Agent: market | Weighted Article Score: 2 | Agent Output: {'market_signal': 1}\n",
      "    Article 101: Agent: macro | Weighted Article Score: 1 | Agent Output: {'macro_signal': 1}\n",
      "    Article 135: Agent: market | Weighted Article Score: 2 | Agent Output: {'market_signal': 1}\n",
      "    Article 137: Agent: earnings | Weighted Article Score: 5 | Agent Output: {'eps_signal': 1, 'revenue_signal': 1}\n",
      "    Article 141: Agent: earnings | Weighted Article Score: 5 | Agent Output: {'eps_signal': 1, 'revenue_signal': 1}\n",
      "    Article 148: Agent: market | Weighted Article Score: -2 | Agent Output: {'market_signal': -1}\n",
      "    Article 159: Agent: market | Weighted Article Score: -2 | Agent Output: {'market_signal': -1}\n",
      "    Article 176: Agent: market | Weighted Article Score: -2 | Agent Output: {'market_signal': -1}\n",
      "    Article 235: Agent: earnings | Weighted Article Score: 5 | Agent Output: {'eps_signal': 1, 'revenue_signal': 1}\n",
      "    Article 311: Agent: market | Weighted Article Score: -2 | Agent Output: {'market_signal': -1}\n",
      "    Article 317: Agent: earnings | Weighted Article Score: 5 | Agent Output: {'eps_signal': 1, 'revenue_signal': 1}\n",
      "    Article 318: Agent: earnings | Weighted Article Score: 5 | Agent Output: {'eps_signal': 1, 'revenue_signal': 1}\n",
      "    Article 328: Agent: market | Weighted Article Score: 2 | Agent Output: {'market_signal': 1}\n",
      "    Article 350: Agent: earnings | Weighted Article Score: 5 | Agent Output: {'eps_signal': 1, 'revenue_signal': 1}\n",
      "    Article 374: Agent: earnings | Weighted Article Score: 5 | Agent Output: {'eps_signal': 1, 'revenue_signal': 1}\n",
      "    Article 457: Agent: market | Weighted Article Score: 2 | Agent Output: {'market_signal': 1}\n",
      "    Article 530: Agent: market | Weighted Article Score: 2 | Agent Output: {'market_signal': 1}\n",
      "    Article 558: Agent: market | Weighted Article Score: -2 | Agent Output: {'market_signal': -1}\n",
      "    Article 569: Agent: market | Weighted Article Score: 2 | Agent Output: {'market_signal': 1}\n",
      "    Article 600: Agent: market | Weighted Article Score: 2 | Agent Output: {'market_signal': 1}\n",
      "    Article 641: Agent: market | Weighted Article Score: -2 | Agent Output: {'market_signal': -1}\n",
      "    Article 666: Agent: market | Weighted Article Score: -2 | Agent Output: {'market_signal': -1}\n",
      "    Article 692: Agent: market | Weighted Article Score: -2 | Agent Output: {'market_signal': -1}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Orchestrator:\n",
    "    def __init__(self, datasets, stop_words, company_name):\n",
    "        self.datasets = datasets\n",
    "        self.stop_words = stop_words\n",
    "        self.company_name = company_name\n",
    "        \n",
    "        # Define agent lookup for routing\n",
    "        self.agent_map = {\n",
    "            \"earnings\": earnings_agent,\n",
    "            \"market\": market_agent,\n",
    "            \"macro\": macro_agent,\n",
    "            \"general\": general_agent\n",
    "        }\n",
    "        \n",
    "        # Define weights for evaluation\n",
    "        self.weights = {\n",
    "            \"eps_signal\": 3,\n",
    "            \"revenue_signal\": 2,\n",
    "            \"market_signal\": 2,\n",
    "            \"macro_signal\": 1,\n",
    "            \"general_signal\": 0\n",
    "        }\n",
    "    \n",
    "    def preprocess_and_tag(self, text):\n",
    "        # 1. Lowercase\n",
    "        text = text.lower()\n",
    "        # 2. Keep alphanumeric only\n",
    "        text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "        # 3. Remove stopwords\n",
    "        tokens = [w for w in text.split() if w not in self.stop_words]\n",
    "        cleaned_text = \" \".join(tokens)\n",
    "        # 4. Topic tagging\n",
    "        if any(word in cleaned_text for word in [\"earnings\", \"quarter\", \"revenue\", \"eps\"]):\n",
    "            topic = \"earnings\"\n",
    "        elif any(word in cleaned_text for word in [\"market\", \"stock\", \"shares\", \"price\"]):\n",
    "            topic = \"market\"\n",
    "        elif any(word in cleaned_text for word in [\"fed\", \"inflation\", \"gdp\", \"rate\"]):\n",
    "            topic = \"macro\"\n",
    "        else:\n",
    "            topic = \"general\"\n",
    "        # 5. Sentiment tagging\n",
    "        polarity = TextBlob(cleaned_text).sentiment.polarity\n",
    "        if polarity > 0.1:\n",
    "            sentiment = \"positive\"\n",
    "        elif polarity < -0.1:\n",
    "            sentiment = \"negative\"\n",
    "        else:\n",
    "            sentiment = \"neutral\"\n",
    "        return cleaned_text, topic, sentiment\n",
    "    \n",
    "    def route_to_agent(self, row):\n",
    "        topic = row[\"topic\"]\n",
    "        sentiment = row[\"sentiment\"]\n",
    "        return self.agent_map.get(topic, general_agent)(row[\"cleaned_text\"], sentiment)\n",
    "    \n",
    "    def process_dataset(self, df, label):\n",
    "        df = df.copy()\n",
    "        # Preprocess\n",
    "        df[[\"cleaned_text\", \"topic\", \"sentiment\"]] = df[\"Headlines\"].apply(\n",
    "            lambda x: pd.Series(self.preprocess_and_tag(str(x)))\n",
    "        )\n",
    "        # Route to agents\n",
    "        agent_outputs = []\n",
    "        agent_names = []\n",
    "        for idx, row in df.iterrows():\n",
    "            out = self.route_to_agent(row)\n",
    "            agent_outputs.append(out)\n",
    "            agent_names.append(row[\"topic\"])\n",
    "        df[\"agent_output\"] = agent_outputs\n",
    "        df[\"agent_name\"] = agent_names\n",
    "        # Filter company\n",
    "        if \"Description\" in df.columns:\n",
    "            company_df = df[\n",
    "                df[\"Headlines\"].str.contains(self.company_name, case=False, na=False) |\n",
    "                df[\"Description\"].str.contains(self.company_name, case=False, na=False)\n",
    "            ]\n",
    "        else:\n",
    "            company_df = df[df[\"Headlines\"].str.contains(self.company_name, case=False, na=False)]\n",
    "        # Compute weighted scores\n",
    "        weighted_scores = [\n",
    "            sum(value * self.weights.get(k, 0) for k, value in out.items())\n",
    "            for out in company_df[\"agent_output\"]\n",
    "        ]\n",
    "        total_score = sum(weighted_scores)\n",
    "        if total_score >= 3:\n",
    "            trade_signal = \"BUY\"\n",
    "        elif total_score <= -2:\n",
    "            trade_signal = \"SELL\"\n",
    "        else:\n",
    "            trade_signal = \"HOLD\"\n",
    "        \n",
    "        # ------------------------\n",
    "        # Simulate Evaluator and Optimizer (no changes)\n",
    "        # ------------------------\n",
    "        # evaluator(company_df) → optimizer(company_df)\n",
    "        final_company_df = company_df.copy()\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"--- Dataset: {label} --- Total {self.company_name} headlines count: {len(company_df)} Trade suggestion: {trade_signal} ---\")\n",
    "        print(\"Non-general agent weighted scores per article:\")\n",
    "        for idx, (row, score) in enumerate(zip(company_df.itertuples(), weighted_scores), start=1):\n",
    "            filtered_out = {k: v for k, v in row.agent_output.items() if k != \"general_signal\" and v != 0}\n",
    "            if filtered_out:\n",
    "                print(f\"    Article {idx}: Agent: {row.agent_name} | Weighted Article Score: {score} | Agent Output: {filtered_out}\")\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        return final_company_df\n",
    "    \n",
    "    def run_all(self):\n",
    "        results = {}\n",
    "        for label, df in self.datasets.items():\n",
    "            results[label] = self.process_dataset(df, label)\n",
    "        return results\n",
    "\n",
    "# ------------------------\n",
    "# Run Orchestrator\n",
    "# ------------------------\n",
    "orchestrator = Orchestrator(datasets, stop_words, company_name)\n",
    "processed_dfs_with_agent_scores = orchestrator.run_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd08fb4-4446-46a1-bc4f-619cf35f8287",
   "metadata": {},
   "source": [
    "# Module 7 lab for reference only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45752035-3603-4093-9c3d-32594aaa57fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai\n",
    "# import time\n",
    "\n",
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "# from openai import OpenAI, APIConnectionError\n",
    "\n",
    "# # ------------------------- 1 - Load environment & Connect to OpenAI\n",
    "# def init_openai():\n",
    "#     load_dotenv('database.env')\n",
    "#     os.environ[\"OPENAI_API_KEY\"] = os.getenv('openaikey')\n",
    "#     client = OpenAI()\n",
    "#     try:\n",
    "#         # Attempt to list models to verify connection\n",
    "#         _ = client.models.list()\n",
    "#         # No print on success\n",
    "#     except APIConnectionError as e:\n",
    "#         print(\"OpenAI API connection error:\", e)\n",
    "#         traceback.print_exc()\n",
    "#         exit()\n",
    "#     return client\n",
    "\n",
    "# client = init_openai()    \n",
    "\n",
    "\n",
    "# # BASE AGENT CLASS\n",
    "\n",
    "# class Agent:\n",
    "#     def __init__(self, name, role, model=\"gpt-3.5-turbo\"): # Model defined here can be changed\n",
    "#         # Defines a general purpose of the with name\n",
    "#         # which is identiy of the agent\n",
    "#         self.name = name\n",
    "#         self.role = role # What the agent is specialized in\n",
    "#         self.model = model # Which LLM model to use\n",
    "#         self.memory = [] # Stores past task and response for traceability\n",
    "    \n",
    "#     # Function for core agent methods\n",
    "#     # Gets a prompt and sends to LLM to get response\n",
    "#     def call_llm(self, prompt):\n",
    "\n",
    "#         try:\n",
    "#             response = client.chat.completions.create(\n",
    "#                 model=self.model,\n",
    "#                 # Sends to other agent for the messages\n",
    "#                 messages=[\n",
    "#                     {\"role\": \"system\", \"content\": f\"You are {self.name}, a {self.role} agent.\"},\n",
    "#                     {\"role\": \"user\", \"content\": prompt}\n",
    "#                 ],\n",
    "#                 # Model parameters\n",
    "#                 max_tokens=300,\n",
    "#                 temperature=0.7\n",
    "#             )\n",
    "#             result = response.choices[0].message.content\n",
    "#             print(f\"{self.name}: {result[:60]}...\")\n",
    "#             return result\n",
    "#         except Exception as e:\n",
    "#             print(f\" API failed for {self.name}: {e}\")\n",
    "#             return f\"Mock response from {self.name}: {prompt[:50]}...\"\n",
    "    \n",
    "#     # Wraps the task into the prompt and stores the resuslts, gets result\n",
    "#     def process(self, task):\n",
    "\n",
    "#         prompt = f\"As a {self.role}, {task}\"\n",
    "#         result = self.call_llm(prompt)\n",
    "#         self.memory.append({\"task\": task, \"result\": result})\n",
    "#         return result\n",
    "#     # After storing the result, the result is sent to function which is sent to other agents in terms of messages\n",
    "#     # Enables agents to communicate by sending tasks to each other\n",
    "#     # Communication between the LLMs or other sub agents to convey the message to them\n",
    "#     # Shouldn't this message content be based on certain categories???****\n",
    "#     # ROUTING?\n",
    "#     def send_to(self, other_agent, message):\n",
    "\n",
    "#         print(f\" {self.name} → {other_agent.name}: {message[:40]}...\")\n",
    "#         return other_agent.process(f\"Process this from {self.name}: {message}\")\n",
    "\n",
    "# # ------------------\n",
    "# # SPECIALIZED AGENTS\n",
    "# # -----------------\n",
    "# # Domain specific agents which the coordinator used gpt-4\n",
    "\n",
    "# # This coordinator can add_agent as a team\n",
    "# class Coordinator(Agent):\n",
    "#     def __init__(self):\n",
    "#         super().__init__(\"Boss\", \"project coordinator\", \"gpt-4\")\n",
    "#         self.team = []\n",
    "#     # Has a team that can append and \n",
    "\n",
    "#     def add_agent(self, agent):\n",
    "#         self.team.append(agent)\n",
    "#         print(f\"➕ Added {agent.name} to team\")\n",
    "    \n",
    "#     # Delegate project tasks to the team\n",
    "#     def delegate_project(self, project):\n",
    "#         print(f\"\\n COORDINATING PROJECT: {project}\")\n",
    "#         plan = self.process(f\"Create a plan for: {project}\")\n",
    "\n",
    "#         results = {}\n",
    "#         for agent in self.team:\n",
    "#             task = f\"Work on project '{project}' using your {agent.role} skills\"\n",
    "#             results[agent.name] = agent.process(task)\n",
    "\n",
    "#         return {\"plan\": plan, \"results\": results}\n",
    "\n",
    "# # ------------------------------\n",
    "# # Researcher, analyst, writer agents\n",
    "# # ----------------------------------\n",
    "\n",
    "# # Gather information using researcher agent about specific topic via gpt-3.5-turbao (fast)\n",
    "# #  Gets \"data\" ?\n",
    "# class Researcher(Agent):\n",
    "#     def __init__(self):\n",
    "#         super().__init__(\"Scout\", \"researcher\", \"gpt-3.5-turbo\")\n",
    "\n",
    "#     def research(self, topic):\n",
    "#         return self.process(f\"research and gather information about: {topic}\")\n",
    "\n",
    "# # Analyzes the data with analyze method via gpt-4\n",
    "# # Gets \"content\" ?\n",
    "# class Analyst(Agent):\n",
    "#     def __init__(self):\n",
    "#         super().__init__(\"Brain\", \"data analyst\", \"gpt-4\")\n",
    "\n",
    "#     def analyze(self, data):\n",
    "#         return self.process(f\"analyze this data and provide insights: {data}\")\n",
    "\n",
    "# # Writer: Writes a report based on the data contents using write_report method\n",
    "# #  Gets \"report\"?\n",
    "# class Writer(Agent):\n",
    "#     def __init__(self):\n",
    "#         super().__init__(\"Pen\", \"content writer\", \"gpt-3.5-turbo\")\n",
    "    \n",
    "#     # Write report method\n",
    "#     def write_report(self, content):\n",
    "#         return self.process(f\"write a professional report based on: {content}\")\n",
    "\n",
    "\n",
    "# # MULTIAGENT SYSTEM\n",
    "#         # Creates a team of agents and runs a full project\n",
    "\n",
    "# class MultiAgentTeam:\n",
    "#     def __init__(self):\n",
    "#         # Create team\n",
    "#         self.coordinator = Coordinator()\n",
    "#         self.researcher = Researcher()\n",
    "#         self.analyst = Analyst()\n",
    "#         self.writer = Writer()\n",
    "\n",
    "#         # Setup team\n",
    "#         self.coordinator.add_agent(self.researcher)\n",
    "#         self.coordinator.add_agent(self.analyst)\n",
    "#         self.coordinator.add_agent(self.writer)\n",
    "\n",
    "#         print(\" Multiagent team created!\")\n",
    "    \n",
    "#     # The execute_project Coordinates, plan the project, researches, gathers info, sends to analyst, writes report\n",
    "#     def execute_project(self, project_description):\n",
    "#         print(f\"\\n EXECUTING PROJECT\")\n",
    "#         print(\"=\" * 40)\n",
    "\n",
    "#         # Step 1: Coordinate\n",
    "#         coordination = self.coordinator.delegate_project(project_description)\n",
    "\n",
    "#         # Step 2: Research\n",
    "#         research_data = self.researcher.research(project_description)\n",
    "\n",
    "#         # Step 3: Analyze research\n",
    "#         # Sends to analyst for analyzing\n",
    "#         analysis = self.researcher.send_to(self.analyst, research_data)\n",
    "\n",
    "#         # Step 4: Write report\n",
    "#         final_report = self.analyst.send_to(self.writer, analysis)\n",
    "        \n",
    "#         # Print task for each agent to show status, how it looks\n",
    "#         print(f\"\\n PROJECT COMPLETED!\")\n",
    "#         return {\n",
    "#             \"coordination\": coordination,\n",
    "#             \"research\": research_data,\n",
    "#             \"analysis\": analysis,\n",
    "#             \"report\": final_report\n",
    "#         }\n",
    "#     # Prints task counts for each agent of the status of the team\n",
    "#     def show_team_status(self):\n",
    "#         print(f\"\\n TEAM STATUS\")\n",
    "#         agents = [self.coordinator, self.researcher, self.analyst, self.writer]\n",
    "#         for agent in agents:\n",
    "#             print(f\"  {agent.name} ({agent.role}): {len(agent.memory)} tasks completed\")\n",
    "\n",
    "\n",
    "# # DEMO FUNCTIONS\n",
    "# # Runs the full multi-agent pipeline on\n",
    "# # Example project = \"analyze AI market treends and create business strategy\"\n",
    "\n",
    "# def demo_basic_multiagent():\n",
    "#     print(\" MULTIAGENT SYSTEM DEMO\")\n",
    "#     print(\"=\" * 40)\n",
    "\n",
    "#     # Create team\n",
    "#     team = MultiAgentTeam()\n",
    "\n",
    "#     # Execute project\n",
    "#     project = \"Analyze AI market trends and create business strategy\"\n",
    "#     results = team.execute_project(project)\n",
    "\n",
    "#     # Show status\n",
    "#     team.show_team_status()\n",
    "\n",
    "#     return results\n",
    "\n",
    "# # Analyst communication\n",
    "# # Communication between the researcher and analyst ***\n",
    "# # Working through information the researcher gathered and \n",
    "# # The analysis that has been done by analyst\n",
    "# def demo_agent_communication():\n",
    "#     print(f\"\\n AGENT COMMUNICATION\")\n",
    "#     print(\"-\" * 30)\n",
    "\n",
    "#     researcher = Researcher()\n",
    "#     analyst = Analyst()\n",
    "\n",
    "#     # Research → Analysis chain\n",
    "#     research = researcher.research(\"artificial intelligence trends\")\n",
    "#     analysis = researcher.send_to(analyst, research)\n",
    "\n",
    "#     print(\" Communication chain completed!\")\n",
    "\n",
    "\n",
    "# # Show different models tackling the same task\n",
    "# def demo_different_models():\n",
    "\n",
    "#     print(f\"\\n DIFFERENT LLM MODELS\")\n",
    "#     print(\"-\" * 30)\n",
    "\n",
    "#     fast_agent = Agent(\"Fast\", \"assistant\", \"gpt-3.5-turbo\")\n",
    "#     smart_agent = Agent(\"Smart\", \"analyst\", \"gpt-4\")\n",
    "\n",
    "#     task = \"Evaluate autonomous vehicles\"\n",
    "\n",
    "#     fast_result = fast_agent.process(task)\n",
    "#     smart_result = smart_agent.process(task)\n",
    "\n",
    "#     print(\" Different models demonstrated!\")\n",
    "\n",
    "# # -------------------------------------\n",
    "# # MAIN EXECUTION\n",
    "# # Ties everything together\n",
    "# # Shows agent to agent communication\n",
    "# # Demonstrates model comparisons\n",
    "# # Gives us the idea of how multiagent works from start to beginning\n",
    "# # ---------------------------------------\n",
    "\n",
    "# def run_multiagent_demo():\n",
    "#     print(\"MULTIAGENT SYSTEM WITH REAL LLMs\")\n",
    "#     print(\"=\" * 50)\n",
    "#     print(\"  Set your OpenAI API key at the top!\")\n",
    "\n",
    "#     try:\n",
    "#         # Main demo\n",
    "#         # Ties everything together\n",
    "#         results = demo_basic_multiagent()\n",
    "\n",
    "#         # Additional demos\n",
    "#         # Shows agent to agent communication\n",
    "#         demo_agent_communication()\n",
    "#         # Demonstrates model comparisons\n",
    "#         demo_different_models()\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\" Demo failed: {e}\")\n",
    "#         print(\" Check your API key!\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Execute run_multiagent_demo\n",
    "#     run_multiagent_demo()\n",
    "\n",
    "\n",
    "# # Output shows joining agents to the team, \n",
    "# # Project goal \" Analyze AI market trends and create business strategy\"\n",
    "# # Phase 1 - provide some input and do the market research, identify key resarch areas, provide some kind of report\n",
    "# # Move forward with what other models can create or for example again you see it has summary based on \n",
    "# # T-key product that provided by Scout\n",
    "# # Shows project completed and hows team status for count of tasks completed per agent, explained each agent's purpose\n",
    "# # Explains communication between each agent ( \"Agent Communication\" )\n",
    "# # Shows demo of different outputs with different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39172278-7015-4285-8dbe-8dcc276d8460",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
