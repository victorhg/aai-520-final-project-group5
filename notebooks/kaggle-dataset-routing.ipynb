{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90ff4337-9948-4116-bd24-a848c71598f8",
   "metadata": {},
   "source": [
    "News Source:\n",
    "\n",
    "https://www.kaggle.com/datasets/notlucasp/financial-news-headlines/data\n",
    "\n",
    "Contents:\n",
    "\n",
    "Data scraped from CNBC contains the Headliness, last updated date, and the preview text of articles from the end of December 2017 to July 19th, 2020.\n",
    "\n",
    "Data scraped from the Guardian Business contains the Headliness and last updated date of articles from the end of December 2017 to July 19th, 2020 since the Guardian Business does not offer preview text.\n",
    "\n",
    "Data scraped from Reuters contains the Headliness, last updated date, and the preview text of articles from the end of March 2018 to July 19th, 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5680aa6-39c9-43bf-a8da-d03f0c00863d",
   "metadata": {},
   "source": [
    "Sentiment via TextBlob:\n",
    "\n",
    "not an ML-trained model like BERT or FinBERT. It’s dictionary-based → works okay for simple financial headlines but may miss sarcasm, jargon, or complex finance tone.\n",
    "\n",
    "# Financial News Agentic AI Pipeline Summary\n",
    "\n",
    "### 1. Dataset Preparation\n",
    "- Load multiple news datasets (CNBC, Guardian, Reuters).\n",
    "- Handle differences in columns (e.g., some may not have `Description`).\n",
    "\n",
    "### 2. Text Preprocessing\n",
    "- Lowercase all text.\n",
    "- Keep alphanumeric characters only.\n",
    "- Remove stopwords.\n",
    "- Output: `cleaned_text`.\n",
    "\n",
    "### 3. Topic Tagging\n",
    "- Text-based classification of each headline:\n",
    "  - `earnings`: earnings, revenue, EPS, quarter\n",
    "  - `market`: stock, shares, price\n",
    "  - `macro`: fed, inflation, GDP, rate\n",
    "  - `general`: default\n",
    "\n",
    "### 4. Sentiment Analysis\n",
    "- Use **TextBlob** model to compute polarity:\n",
    "  - `positive` if polarity > 0.1  \n",
    "  - `negative` if polarity < -0.1  \n",
    "  - `neutral` otherwise\n",
    "- Output: sentiment label per headline.\n",
    "\n",
    "### 5. Routing to Specialist Agents\n",
    "- **Earnings agent:** receives topic + sentiment → `eps_signal`, `revenue_signal`\n",
    "- **Market agent:** topic + sentiment → `market_signal`\n",
    "- **Macro agent:** topic + sentiment → `macro_signal`\n",
    "- **General agent:** topic + sentiment → `general_signal` (always 0)\n",
    "- Agents are **rule-based**; only TextBlob sentiment is model-based.\n",
    "\n",
    "### 6. Tesla Filtering\n",
    "- Select headlines mentioning `\"Tesla\"` in `Headlines` or `Description` (if exists).\n",
    "\n",
    "### 7. Weighted Aggregation\n",
    "- Combine all agent outputs for Tesla headlines.\n",
    "- Signals are multiplied by predefined weights:\n",
    "  - `eps_signal` → weight 3  \n",
    "  - `revenue_signal` → weight 2  \n",
    "  - `market_signal` → weight 2  \n",
    "  - `macro_signal` → weight 1  \n",
    "  - `general_signal` → weight 0\n",
    "\n",
    "### 8. Trade Suggestion\n",
    "- Sum weighted signals to compute total score.\n",
    "- Map total score → trade action:\n",
    "  - Total ≥ 3 → **BUY**\n",
    "  - Total ≤ -2 → **SELL**\n",
    "  - Otherwise → **HOLD**\n",
    "\n",
    "### 9. Reporting\n",
    "- Print per dataset:\n",
    "  - Number of Tesla headlines\n",
    "  - Aggregated trade suggestion (BUY/HOLD/SELL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd005694-d746-4fc9-825c-c6c30ccbd29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import os\n",
    "import shutil\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc64f87d-b358-445c-aab2-49b7332198b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache path: C:\\Users\\Administrator\\.cache\\kagglehub\\datasets\\notlucasp\\financial-news-headlines\\versions\\2\n",
      "3 Files in dataset:\n",
      " ['cnbc_headlines.csv', 'guardian_headlines.csv', 'reuters_headlines.csv']\n"
     ]
    }
   ],
   "source": [
    "# ------------------------\n",
    "# Setup\n",
    "# ------------------------\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# ------------------------\n",
    "# Download dataset (cached)\n",
    "# ------------------------\n",
    "path = kagglehub.dataset_download(\"notlucasp/financial-news-headlines\")\n",
    "print(\"Cache path:\", path)\n",
    "\n",
    "# ------------------------\n",
    "# Define target folder\n",
    "# ------------------------\n",
    "target_dir = os.path.abspath(os.path.join(\"..\", \"news-datasets\", \"kaggle-headlines-data\"))\n",
    "if os.path.exists(target_dir):\n",
    "    shutil.rmtree(target_dir)\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# ------------------------\n",
    "# Copy all files from versioned folder to target\n",
    "# ------------------------\n",
    "for item in os.listdir(path):\n",
    "    src_file = os.path.join(path, item)\n",
    "    if os.path.isfile(src_file):\n",
    "        shutil.copy(src_file, target_dir)\n",
    "\n",
    "# ------------------------\n",
    "# List dataset files\n",
    "# ------------------------\n",
    "files = os.listdir(target_dir)\n",
    "print(f\"{len(files)} Files in dataset:\\n\", files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a017276-6a45-40e3-8c32-a4ac1f168f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnbc_headlines.csv\n",
      "                                                                Headlines                           Time                                                                                                                                        Description\n",
      "     Jim Cramer: A better way to invest in the Covid-19 vaccine gold rush  7:51  PM ET Fri, 17 July 2020                                              \"Mad Money\" host Jim Cramer recommended buying four companies that are supporting vaccine developers.\n",
      "                           Cramer's lightning round: I would own Teradyne  7:33  PM ET Fri, 17 July 2020        \"Mad Money\" host Jim Cramer rings the lightning round bell, which means he's giving his answers to callers' stock questions at rapid speed.\n",
      "                                                                      NaN                            NaN                                                                                                                                                NaN\n",
      "Cramer's week ahead: Big week for earnings, even bigger week for vaccines  7:25  PM ET Fri, 17 July 2020 \"We'll pay more for the earnings of the non-Covid companies if The Lancet publishes some good news from AstraZeneca's vaccine trial,\" Cramer said.\n",
      "           IQ Capital CEO Keith Bliss says tech and healthcare will rally  4:24  PM ET Fri, 17 July 2020      Keith Bliss, IQ Capital CEO, joins \"Closing Bell\" to talk about the broader markets, including the performance of the S&P 500 and the Nasdaq.\n"
     ]
    }
   ],
   "source": [
    "# Load cnbc CSV into dataframe\n",
    "file_path = os.path.join(target_dir, files[0])\n",
    "cnbc_raw_df = pd.read_csv(file_path)\n",
    "print(files[0])\n",
    "print(cnbc_raw_df.head().to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acb7aad7-fd6a-4277-bc2b-6d681aef7fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3080 entries, 0 to 3079\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Headlines    2800 non-null   object\n",
      " 1   Time         2800 non-null   object\n",
      " 2   Description  2800 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 72.3+ KB\n"
     ]
    }
   ],
   "source": [
    "cnbc_raw_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "747d2924-370a-479e-9aaf-157a511bade7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guardian_headlines.csv\n",
      "     Time                                                                                                         Headlines\n",
      "18-Jul-20                                                                  Johnson is asking Santa for a Christmas recovery\n",
      "18-Jul-20                                       ‘I now fear the worst’: four grim tales of working life upended by Covid-19\n",
      "18-Jul-20                                                    Five key areas Sunak must tackle to serve up economic recovery\n",
      "18-Jul-20                                                   Covid-19 leaves firms ‘fatally ill-prepared’ for no-deal Brexit\n",
      "18-Jul-20 The Week in Patriarchy  \\n\\n\\n  Bacardi's 'lady vodka': the latest in a long line of depressing gendered products\n"
     ]
    }
   ],
   "source": [
    "# Load guardian CSV into dataframe\n",
    "file_path = os.path.join(target_dir, files[1])\n",
    "guardian_raw_df = pd.read_csv(file_path)\n",
    "print(files[1])\n",
    "print(guardian_raw_df.head().to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d205943-119d-48fe-bb08-5da0c8a8ab06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17800 entries, 0 to 17799\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Time       17800 non-null  object\n",
      " 1   Headlines  17800 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 278.3+ KB\n"
     ]
    }
   ],
   "source": [
    "guardian_raw_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "188ebdc9-713e-41b4-905a-46300ea44a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reuters_headlines.csv\n",
      "                                                                    Headlines        Time                                                                                                                                                                                                                                                                                      Description\n",
      "                 TikTok considers London and other locations for headquarters Jul 18 2020                                                                TikTok has been in discussions with the UK government over the past few months to locate its headquarters in London, a source familiar with the matter said, as part of a strategy to distance itself from its Chinese ownership.\n",
      "                Disney cuts ad spending on Facebook amid growing boycott: WSJ Jul 18 2020 Walt Disney  has become the latest company to slash its advertising spending on Facebook Inc  as the social media giant faces an ad boycott over its handling of hate speech and controversial content, the Wall Street Journal reported on Saturday, citing people familiar with the situation.\n",
      "    Trail of missing Wirecard executive leads to Belarus, Der Spiegel reports Jul 18 2020                                                                                                              Former Wirecard  chief operating officer Jan Marsalek travelled to Minsk soon after he was suspended and may still be in Belarus or Russia, a German magazine reported on Saturday.\n",
      "Twitter says attackers downloaded data from up to eight non-verified accounts Jul 18 2020                                                                                        Twitter Inc said on Saturday that hackers were able to download account information for up to eight accounts involved in the hack of its systems this week, but said none of them were verified accounts.\n",
      "  U.S. Republicans seek liability protections as coronavirus aid battle looms Jul 17 2020                                                 A battle in the U.S. Congress over a new coronavirus-aid bill began on Friday as Republicans were putting the finishing touches on provisions granting liability protections for a wide range of entities resuming operations amid the pandemic.\n"
     ]
    }
   ],
   "source": [
    "# Load reuters CSV into dataframe\n",
    "file_path = os.path.join(target_dir, files[2])\n",
    "reuters_raw_df = pd.read_csv(file_path)\n",
    "print(files[2])\n",
    "print(reuters_raw_df.head().to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d338fe6-f585-445d-8034-3ee56c28de83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32770 entries, 0 to 32769\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Headlines    32770 non-null  object\n",
      " 1   Time         32770 non-null  object\n",
      " 2   Description  32770 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 768.2+ KB\n"
     ]
    }
   ],
   "source": [
    "reuters_raw_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "26c111b9-30a3-42a8-8f9c-abe57faab332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Dataset: cnbc ---\n",
      "Tesla headlines count: 36\n",
      "Trade suggestion: SELL\n",
      "\n",
      "--- Dataset: guardian ---\n",
      "Tesla headlines count: 78\n",
      "Trade suggestion: BUY\n",
      "\n",
      "--- Dataset: reuters ---\n",
      "Tesla headlines count: 699\n",
      "Trade suggestion: BUY\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------\n",
    "# Function: Preprocessing + Tagging\n",
    "# ------------------------\n",
    "def preprocess_and_tag(text):\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "    # 2. Keep alphanumeric only\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    # 3. Remove stopwords\n",
    "    tokens = [w for w in text.split() if w not in stop_words]\n",
    "    cleaned_text = \" \".join(tokens)\n",
    "    \n",
    "    # 4. Topic tagging\n",
    "    if any(word in cleaned_text for word in [\"earnings\", \"quarter\", \"revenue\", \"eps\"]):\n",
    "        topic = \"earnings\"\n",
    "    elif any(word in cleaned_text for word in [\"market\", \"stock\", \"shares\", \"price\"]):\n",
    "        topic = \"market\"\n",
    "    elif any(word in cleaned_text for word in [\"fed\", \"inflation\", \"gdp\", \"rate\"]):\n",
    "        topic = \"macro\"\n",
    "    else:\n",
    "        topic = \"general\"\n",
    "    \n",
    "    # 5. Sentiment tagging using TextBlob\n",
    "    polarity = TextBlob(cleaned_text).sentiment.polarity\n",
    "    if polarity > 0.1:\n",
    "        sentiment = \"positive\"\n",
    "    elif polarity < -0.1:\n",
    "        sentiment = \"negative\"\n",
    "    else:\n",
    "        sentiment = \"neutral\"\n",
    "    \n",
    "    return cleaned_text, topic, sentiment\n",
    "\n",
    "# ------------------------\n",
    "# Specialist Agent Functions\n",
    "# ------------------------\n",
    "def earnings_agent(text, sentiment):\n",
    "    eps_signal = 1 if sentiment == \"positive\" else 0\n",
    "    revenue_signal = 1 if sentiment == \"positive\" else 0\n",
    "    return {\"eps_signal\": eps_signal, \"revenue_signal\": revenue_signal}\n",
    "\n",
    "def market_agent(text, sentiment):\n",
    "    market_signal = 1 if sentiment == \"positive\" else (-1 if sentiment == \"negative\" else 0)\n",
    "    return {\"market_signal\": market_signal}\n",
    "\n",
    "def macro_agent(text, sentiment):\n",
    "    macro_signal = 1 if sentiment == \"positive\" else (-1 if sentiment == \"negative\" else 0)\n",
    "    return {\"macro_signal\": macro_signal}\n",
    "\n",
    "def general_agent(text, sentiment):\n",
    "    return {\"general_signal\": 0}\n",
    "\n",
    "# ------------------------\n",
    "# Routing Function\n",
    "# ------------------------\n",
    "def route_to_agent(row):\n",
    "    topic = row[\"topic\"]\n",
    "    sentiment = row[\"sentiment\"]\n",
    "    text = row[\"cleaned_text\"]\n",
    "    \n",
    "    if topic == \"earnings\":\n",
    "        return earnings_agent(text, sentiment)\n",
    "    elif topic == \"market\":\n",
    "        return market_agent(text, sentiment)\n",
    "    elif topic == \"macro\":\n",
    "        return macro_agent(text, sentiment)\n",
    "    else:\n",
    "        return general_agent(text, sentiment)\n",
    "\n",
    "# ------------------------\n",
    "# Function: Full processing for a single dataset\n",
    "# ------------------------\n",
    "def process_dataset(df, label):\n",
    "    df = df.copy()\n",
    "    # Preprocess Headlines\n",
    "    df[[\"cleaned_text\", \"topic\", \"sentiment\"]] = df[\"Headlines\"].apply(\n",
    "        lambda x: pd.Series(preprocess_and_tag(str(x)))\n",
    "    )\n",
    "    # Route to agents\n",
    "    df[\"agent_output\"] = df.apply(route_to_agent, axis=1)\n",
    "    \n",
    "    # ------------------------\n",
    "    # Filter Tesla-specific headlines\n",
    "    # ------------------------\n",
    "    if \"Description\" in df.columns:\n",
    "        tesla_df = df[\n",
    "            df[\"Headlines\"].str.contains(\"Tesla\", case=False, na=False) |\n",
    "            df[\"Description\"].str.contains(\"Tesla\", case=False, na=False)\n",
    "        ]\n",
    "    else:\n",
    "        tesla_df = df[\n",
    "            df[\"Headlines\"].str.contains(\"Tesla\", case=False, na=False)\n",
    "        ]\n",
    "    \n",
    "    # ------------------------\n",
    "    # Aggregation + Weighted Trade Signal\n",
    "    # ------------------------\n",
    "    def determine_trade_signal(agent_outputs):\n",
    "        weights = {\n",
    "            \"eps_signal\": 3,\n",
    "            \"revenue_signal\": 2,\n",
    "            \"market_signal\": 2,\n",
    "            \"macro_signal\": 1,\n",
    "            \"general_signal\": 0\n",
    "        }\n",
    "        total_score = 0\n",
    "        for out in agent_outputs:\n",
    "            for key, value in out.items():\n",
    "                total_score += value * weights.get(key, 0)\n",
    "        if total_score >= 3:\n",
    "            return \"BUY\"\n",
    "        elif total_score <= -2:\n",
    "            return \"SELL\"\n",
    "        else:\n",
    "            return \"HOLD\"\n",
    "    \n",
    "    tesla_outputs = list(tesla_df[\"agent_output\"])\n",
    "    trade_signal = determine_trade_signal(tesla_outputs)\n",
    "    \n",
    "    # ------------------------\n",
    "    # Print dataset-specific results\n",
    "    # ------------------------\n",
    "    print(f\"--- Dataset: {label} ---\")\n",
    "    print(f\"Tesla headlines count: {len(tesla_df)}\")\n",
    "    print(f\"Trade suggestion: {trade_signal}\\n\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ------------------------\n",
    "# Process all datasets with labels\n",
    "# ------------------------\n",
    "datasets = {\n",
    "    \"cnbc\": cnbc_raw_df,\n",
    "    \"guardian\": guardian_raw_df,\n",
    "    \"reuters\": reuters_raw_df\n",
    "}\n",
    "\n",
    "processed_dfs = {}\n",
    "for label, df in datasets.items():\n",
    "    processed_dfs[label] = process_dataset(df, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a1ded9-93b8-4c13-84d3-d38997a81625",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
