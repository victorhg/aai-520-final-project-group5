{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45de57cb-1a4c-4092-a9a7-4e263931d43a",
   "metadata": {},
   "source": [
    "USD MS AAI - 510 Machine Learning - Final Project\n",
    "\n",
    "## Financial Agentic System\n",
    "\n",
    "Group 5: Antonio Recalde, Ajmal Jalal, Darin Verduzco\n",
    "\n",
    "GitHub: https://github.com/victorhg/aai-520-final-project-group5\n",
    "\n",
    "News sources: Yahoo Finance and NewsAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107abe8c-5774-48d6-b863-3789dbade447",
   "metadata": {},
   "source": [
    "## Overall project workflow\n",
    "Planner and routing via Orchestrator -> Data Retrieval and Preprocessing via Ingestion -> Summarizer -> Memory worker -> Evaluator - > Optimizer -> Final output -> Memory worker "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "320f9da7-a5eb-4512-93a7-2088e306475f",
   "metadata": {},
   "source": [
    "Agent Functions \n",
    "1. Plans its research steps for a given stock symbol.\n",
    "2. Uses tools dynamically (APIs, datasets, retrieval).\n",
    "3. Self-reflects to assess the quality of its output.\n",
    "4. Learns across runs (e.g., keeps brief memories or notes to improve future analyses).\n",
    "\n",
    "Workflow Patterns \n",
    "1. Prompt Chaining: Ingest News → Preprocess → Classify → Extract → Summarize\n",
    "2. Routing: Direct content to the right specialist (e.g., earnings, news, or market analyzers).\n",
    "3. Evaluator–Optimizer: Generate analysis → evaluate quality → refine using feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7349fe60-ab27-4896-b466-1a343645e7ed",
   "metadata": {},
   "source": [
    "## Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b89df8b8-5dde-4d19-828c-fcacd5573f3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: yfinance in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (0.2.66)\n",
      "Requirement already satisfied: feedparser in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (6.0.12)\n",
      "Requirement already satisfied: requests in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (2.32.5)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (1.1.0)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (4.15.0)\n",
      "Requirement already satisfied: pydantic in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (2.11.5)\n",
      "Requirement already satisfied: langgraph in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (0.6.10)\n",
      "Requirement already satisfied: IPython in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (9.5.0)\n",
      "Requirement already satisfied: langchain-openai in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (0.3.35)\n",
      "Requirement already satisfied: pandas>=1.3.0 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from yfinance) (2.3.0)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from yfinance) (2.3.3)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from yfinance) (0.0.12)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from yfinance) (4.3.8)\n",
      "Requirement already satisfied: pytz>=2022.5 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from yfinance) (2025.2)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from yfinance) (2.4.6)\n",
      "Requirement already satisfied: peewee>=3.16.2 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from yfinance) (3.18.2)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from yfinance) (4.13.4)\n",
      "Requirement already satisfied: curl_cffi>=0.7 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from yfinance) (0.13.0)\n",
      "Requirement already satisfied: protobuf>=3.19.0 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from yfinance) (5.29.5)\n",
      "Requirement already satisfied: websockets>=13.0 in c:\\program files\\python312\\lib\\site-packages (from yfinance) (15.0.1)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from feedparser) (1.0.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from requests) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from requests) (2025.10.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from pydantic) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from pydantic) (0.4.1)\n",
      "Requirement already satisfied: langchain-core>=0.1 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from langgraph) (0.3.79)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from langgraph) (2.1.2)\n",
      "Requirement already satisfied: langgraph-prebuilt<0.7.0,>=0.6.0 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from langgraph) (0.6.4)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from langgraph) (0.2.9)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from langgraph) (3.5.0)\n",
      "Requirement already satisfied: ormsgpack>=1.10.0 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph) (1.11.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.10.18)\n",
      "Requirement already satisfied: colorama in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from IPython) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from IPython) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from IPython) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from IPython) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from IPython) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from IPython) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from IPython) (2.19.1)\n",
      "Requirement already satisfied: stack_data in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from IPython) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from IPython) (5.14.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->IPython) (0.2.13)\n",
      "Requirement already satisfied: openai<3.0.0,>=1.104.2 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from langchain-openai) (2.3.0)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from langchain-openai) (0.12.0)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core>=0.1->langgraph) (0.3.45)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core>=0.1->langgraph) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core>=0.1->langgraph) (6.0.3)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core>=0.1->langgraph) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=0.1->langgraph) (3.0.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (0.23.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.16.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.9.18)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.7)\n",
      "Requirement already satisfied: cffi>=1.12.0 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from curl_cffi>=0.7->yfinance) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.22)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from jedi>=0.16->IPython) (0.8.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from pandas>=1.3.0->yfinance) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from stack_data->IPython) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from stack_data->IPython) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from stack_data->IPython) (0.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install yfinance feedparser requests python-dotenv typing_extensions pydantic langgraph IPython langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bc3a04-5c2e-4581-a002-205c3a2e54fb",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf510d52-8d34-41ac-b2b2-d4908db6a394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Data Handling & I/O\n",
    "# ---------------------------\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import html\n",
    "from datetime import datetime, timezone\n",
    "import yfinance as yf\n",
    "import feedparser\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ---------------------------\n",
    "# Typing & Preprocessing Utilities\n",
    "# ---------------------------\n",
    "from __future__ import annotations\n",
    "from typing import Dict, Any, List, Optional, Union, TypedDict, Literal\n",
    "from typing_extensions import Annotated\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# ---------------------------\n",
    "# Models / Evaluation\n",
    "# ---------------------------\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display, Markdown\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Load \"news_openai.env\" environment key file\n",
    "load_dotenv(\"news_openai.env\")\n",
    "# Set NewsAPI key\n",
    "NEWS_API_KEY = os.getenv(\"NEWS_API_KEY\")\n",
    "# OpenAI API key loaded via \"_initialize_openai_llm\" with env variable \"OPENAI_API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef09596-8c37-451f-b512-a863fc0bfac0",
   "metadata": {},
   "source": [
    "# Step 1 - Ingestion\n",
    "Combines data from financial_data.py (Yahoo Finance) and news_data.py (NewsAPI) using \"Ingestion\" worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e301856-1e5b-44f9-9acb-8531eead0979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# worker/base_worker.py\n",
    "# ---------------------------\n",
    "class BaseWorker:\n",
    "    def execute(self, *inputs):\n",
    "        \"\"\"Execute the worker's main function. To be overridden by subclasses.\"\"\"\n",
    "        raise NotImplementedError(\"This method should be overridden by subclasses.\")\n",
    "\n",
    "# ---------------------------\n",
    "# (step 1)\n",
    "# ingestion/financial_data.py (file 1 of 3)\n",
    "# (Yahoo Finance data fetching, metrics calculation, and error handling, data structuring.)\n",
    "# ---------------------------\n",
    "\n",
    "\"\"\"\n",
    "NLP-5: Financial Data Ingestion\n",
    "Fetch stock data from Yahoo Finance API (yfinance)\n",
    "\"\"\"\n",
    "\n",
    "class FinancialDataIngestion(BaseWorker):\n",
    "    \"\"\"\n",
    "    Fetches financial data from Yahoo Finance.\n",
    "    \n",
    "    Responsibilities:\n",
    "    - Fetch historical OHLCV (Open, High, Low, Close, Volume) data\n",
    "    - Fetch company information and fundamentals\n",
    "    - Calculate basic metrics from raw data\n",
    "    - Handle API errors gracefully\n",
    "    - Return structured financial data\n",
    "    \"\"\"\n",
    "    \n",
    "    def execute(self, *inputs) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Fetch financial data for a given stock symbol.\n",
    "        \n",
    "        Args:\n",
    "            inputs[0] (str): Stock ticker symbol (e.g., \"AAPL\")\n",
    "            inputs[1] (str, optional): Time period (default: \"1mo\")\n",
    "                Valid periods: 1d, 5d, 1mo, 3mo, 6mo, 1y, 2y, 5y, 10y, ytd, max\n",
    "        \n",
    "        Returns:\n",
    "            dict: Financial data bundle containing:\n",
    "                - symbol: Stock ticker\n",
    "                - price metrics: Current price, changes, highs/lows\n",
    "                - volume metrics: Current and average volume\n",
    "                - volatility: Calculated volatility\n",
    "                - fundamentals: P/E ratio, market cap, beta, etc.\n",
    "                - company info: Sector, industry, summary\n",
    "                - historical_data: Recent OHLCV records\n",
    "                - status: Success or error status\n",
    "        \"\"\"\n",
    "        try:\n",
    "            symbol = inputs[0]\n",
    "            period = inputs[1] if len(inputs) > 1 else \"1mo\"\n",
    "\n",
    "            stock = yf.Ticker(symbol)\n",
    "\n",
    "            # Get historical data and info\n",
    "            hist = stock.history(period=period)\n",
    "            \n",
    "            if hist.empty:\n",
    "                return {\n",
    "                    \"source\": \"yahoo_finance\",\n",
    "                    \"symbol\": symbol,\n",
    "                    \"data\": None,\n",
    "                    \"status\": \"error\",\n",
    "                    \"error\": f\"No data found for symbol {symbol}\",\n",
    "                    \"timestamp\": datetime.now().isoformat()\n",
    "                }\n",
    "            \n",
    "            info = stock.info\n",
    "            \n",
    "            # Calculate basic metrics\n",
    "            current_price = hist['Close'].iloc[-1]\n",
    "            prev_close = info.get('previousClose', hist['Close'].iloc[-2] if len(hist) > 1 else current_price)\n",
    "            price_change = current_price - prev_close\n",
    "            price_change_pct = (price_change / prev_close) * 100 if prev_close != 0 else 0\n",
    "            \n",
    "            # Calculate volatility (30-day annualized)\n",
    "            returns = hist['Close'].pct_change().dropna()\n",
    "            volatility = returns.tail(30).std() * (252 ** 0.5) if len(returns) > 0 else 0\n",
    "            \n",
    "            # Get volume metrics\n",
    "            avg_volume = hist['Volume'].tail(30).mean()\n",
    "            current_volume = hist['Volume'].iloc[-1]\n",
    "\n",
    "            # Raw data result from yf\n",
    "            result = {\n",
    "                \"source\": \"yahoo_finance\",\n",
    "                \"symbol\": symbol,\n",
    "                \"data\": {\n",
    "                    \"current_price\": float(current_price),\n",
    "                    \"price_change\": float(price_change),\n",
    "                    \"price_change_pct\": float(price_change_pct),\n",
    "                    \"volume\": int(current_volume),\n",
    "                    \"avg_volume_30d\": float(avg_volume),\n",
    "                    \"volatility_30d\": float(volatility),\n",
    "                    \"market_cap\": info.get(\"marketCap\"),\n",
    "                    \"pe_ratio\": info.get(\"forwardPE\"),\n",
    "                    \"dividend_yield\": info.get(\"dividendYield\"),\n",
    "                    \"52_week_high\": info.get(\"fiftyTwoWeekHigh\"),\n",
    "                    \"52_week_low\": info.get(\"fiftyTwoWeekLow\"),\n",
    "                    \"beta\": info.get(\"beta\"),\n",
    "                    \"sector\": info.get(\"sector\"),\n",
    "                    \"industry\": info.get(\"industry\"),\n",
    "                    \"company_summary\": info.get(\"longBusinessSummary\", \"\")[:500],\n",
    "                    \"historical_data\": hist.tail(30).to_dict('records')\n",
    "                },\n",
    "                \"status\": \"success\",\n",
    "                \"error\": None,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"source\": \"yahoo_finance\",\n",
    "                \"symbol\": inputs[0] if len(inputs) > 0 else \"UNKNOWN\",\n",
    "                \"data\": None,\n",
    "                \"status\": \"error\",\n",
    "                \"error\": str(e),\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "# ---------------------------\n",
    "# (step 1)\n",
    "# ingestion/news_data.py (file 2 of 3)\n",
    "# ---------------------------\n",
    "\n",
    "\"\"\"\n",
    "NLP-6: News Data Ingestion\n",
    "Fetch news articles from NewsAPI and Yahoo Finance RSS\n",
    "\"\"\"\n",
    "\n",
    "class NewsDataIngestion(BaseWorker):\n",
    "    \"\"\"\n",
    "    Fetches news data from multiple sources.\n",
    "    \n",
    "    Responsibilities:\n",
    "    - Fetch news articles from NewsAPI\n",
    "    - Fetch news from Yahoo Finance RSS feeds\n",
    "    - Handle API errors and rate limits gracefully\n",
    "    - Return structured list of articles\n",
    "    \"\"\"\n",
    "    \n",
    "    DEFAULT_LIMIT = 10\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def execute(self, *inputs) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Fetch news articles for a given stock symbol.\n",
    "        \n",
    "        Args:\n",
    "            inputs[0] (str): Stock ticker symbol (e.g., \"AAPL\")\n",
    "            inputs[1] (int, optional): Number of articles to fetch (default: 10)\n",
    "        \n",
    "        Returns:\n",
    "            dict: News data bundle containing:\n",
    "                - articles: List of articles from all sources\n",
    "                - sources_queried: Which sources were successfully queried\n",
    "                - total_count: Total number of articles fetched\n",
    "                - status: Success or error status\n",
    "        \"\"\"\n",
    "        try:\n",
    "            symbol = inputs[0]\n",
    "            limit = inputs[1] if len(inputs) > 1 else self.DEFAULT_LIMIT\n",
    "            \n",
    "            articles = []\n",
    "            sources_queried = []\n",
    "            errors = []\n",
    "            \n",
    "            # Fetch from Yahoo Finance RSS (half the limit)\n",
    "            try:\n",
    "                yahoo_articles = self._fetch_from_yahoo_rss(symbol, limit // 2)\n",
    "                articles.extend(yahoo_articles)\n",
    "                sources_queried.append(\"yahoo_rss\")\n",
    "            except Exception as e:\n",
    "                errors.append({\"source\": \"yahoo_rss\", \"error\": str(e)})\n",
    "            \n",
    "            # Fetch from NewsAPI (half the limit)\n",
    "            if NEWS_API_KEY:\n",
    "                try:\n",
    "                    newsapi_articles = self._fetch_from_newsapi(symbol, limit // 2)\n",
    "                    articles.extend(newsapi_articles)\n",
    "                    sources_queried.append(\"newsapi\")\n",
    "                except Exception as e:\n",
    "                    errors.append({\"source\": \"newsapi\", \"error\": str(e)})\n",
    "            else:\n",
    "                errors.append({\"source\": \"newsapi\", \"error\": \"NEWS_API_KEY not found in environment\"})\n",
    "            \n",
    "            return {\n",
    "                \"source\": \"news_aggregated\",\n",
    "                \"symbol\": symbol,\n",
    "                \"data\": {\n",
    "                    \"articles\": articles,\n",
    "                    \"sources_queried\": sources_queried,\n",
    "                    \"total_count\": len(articles)\n",
    "                },\n",
    "                \"status\": \"success\" if len(articles) > 0 else \"partial_success\",\n",
    "                \"error\": errors if len(errors) > 0 else None,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"source\": \"news_aggregated\",\n",
    "                \"symbol\": inputs[0] if len(inputs) > 0 else \"UNKNOWN\",\n",
    "                \"data\": {\n",
    "                    \"articles\": [],\n",
    "                    \"sources_queried\": [],\n",
    "                    \"total_count\": 0\n",
    "                },\n",
    "                \"status\": \"error\",\n",
    "                \"error\": str(e),\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "    \n",
    "    def _preprocess_text(self, text: str) -> str:\n",
    "        # Remove <script>...</script> blocks (case-insensitive, dot matches newline)\n",
    "        text = re.sub(r'(?is)<script.*?>.*?</script>', ' ', text)\n",
    "\n",
    "        # Remove any remaining HTML tags\n",
    "        text = re.sub(r'<[^>]+>', ' ', text)\n",
    "\n",
    "        # Remove javascript: URIs and inline event handlers like onload=, onclick= etc.\n",
    "        text = re.sub(r'(?i)javascript\\s*:', '', text)\n",
    "        text = re.sub(r'(?i)on\\w+\\s*=\\s*[\"\\'].*?[\"\\']', ' ', text)\n",
    "\n",
    "        # Remove control characters\n",
    "        text = re.sub(r'[\\x00-\\x1f\\x7f]', ' ', text)\n",
    "\n",
    "        # Unescape HTML entities then escape to ensure safe plain text\n",
    "        text = html.unescape(text)\n",
    "        text = html.escape(text)\n",
    "\n",
    "        # Collapse multiple whitespace to single space and trim\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "        return text\n",
    "    \n",
    "    def _fetch_from_newsapi(self, symbol: str, limit: int) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Fetch articles from NewsAPI.\n",
    "        \n",
    "        Args:\n",
    "            symbol: Stock ticker symbol\n",
    "            limit: Maximum number of articles\n",
    "            \n",
    "        Returns:\n",
    "            List of article dictionaries\n",
    "        \"\"\"\n",
    "        url = \"https://newsapi.org/v2/everything\"\n",
    "        params = {\n",
    "            \"q\": f\"{symbol} stock\",\n",
    "            \"language\": \"en\",\n",
    "            \"sortBy\": \"publishedAt\",\n",
    "            \"pageSize\": limit,\n",
    "            \"apiKey\": NEWS_API_KEY\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            articles = []\n",
    "            for article in data.get(\"articles\", []):\n",
    "                processed_summary = self._preprocess_text(article.get(\"description\", \"\"))   \n",
    "                articles.append({\n",
    "                    \"title\": article.get(\"title\", \"\"),\n",
    "                    \"link\": article.get(\"url\", \"\"),\n",
    "                    \"published\": article.get(\"publishedAt\", \"\"),\n",
    "                    \"summary\": processed_summary,\n",
    "                    \"source\": article.get(\"source\", {}).get(\"name\", \"NewsAPI\")\n",
    "                })\n",
    "            return articles\n",
    "        else:\n",
    "            raise Exception(f\"NewsAPI request failed with status {response.status_code}\")\n",
    "    \n",
    "    def _fetch_from_yahoo_rss(self, symbol: str, limit: int) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Fetch articles from Yahoo Finance RSS feed.\n",
    "        \n",
    "        Args:\n",
    "            symbol: Stock ticker symbol\n",
    "            limit: Maximum number of articles\n",
    "            \n",
    "        Returns:\n",
    "            List of article dictionaries\n",
    "        \"\"\"\n",
    "        yahoo_rss = f\"https://feeds.finance.yahoo.com/rss/2.0/headline?s={symbol}&region=US&lang=en-US\"\n",
    "        feed = feedparser.parse(yahoo_rss)\n",
    "        \n",
    "        articles = []\n",
    "        for entry in feed.entries[:limit]:\n",
    "            processed_summary = self._preprocess_text(entry.get(\"summary\", \"\"))\n",
    "            articles.append({\n",
    "                \"title\": entry.get(\"title\", \"\"),\n",
    "                \"link\": entry.get(\"link\", \"\"),\n",
    "                \"published\": entry.get(\"published\", \"\"),\n",
    "                \"summary\": processed_summary,\n",
    "                \"source\": \"Yahoo Finance\"\n",
    "            })\n",
    "        \n",
    "        return articles\n",
    "\n",
    "# ---------------------------\n",
    "# (step 1)\n",
    "# ingestion/ingestion.py (file 3 of 3)\n",
    "# ---------------------------\n",
    "\"\"\"\n",
    "Main Ingestion Coordinator\n",
    "Orchestrates parallel data fetching from financial and news sources (simplified)\n",
    "\"\"\"\n",
    "\n",
    "class Ingestion(BaseWorker):\n",
    "    \"\"\"\n",
    "    Main Ingestion Coordinator that fetches data from financial and news sources in parallel.\n",
    "    \n",
    "    Responsibilities:\n",
    "    - Coordinate parallel data fetching from financial and news sources\n",
    "    - Combine results into single bundle\n",
    "    - Handle partial failures gracefully\n",
    "    - Track errors from each source\n",
    "    - Return complete data bundle\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize data ingestors.\"\"\"\n",
    "        self.financial_ingestor = FinancialDataIngestion()\n",
    "        self.news_ingestor = NewsDataIngestion()\n",
    "    \n",
    "    def execute(self, *inputs) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Execute parallel data ingestion from all sources.\n",
    "        \n",
    "        Args:\n",
    "            inputs[0] (str): Stock ticker symbol (e.g., \"AAPL\")\n",
    "            inputs[1] (str, optional): Time period for historical data (default: \"1mo\")\n",
    "            inputs[2] (int, optional): Number of news articles (default: 10)\n",
    "        \n",
    "        Returns:\n",
    "            dict: Complete data bundle with financial and news data\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract parameters\n",
    "            symbol = inputs[0] if len(inputs) > 0 else \"AAPL\"\n",
    "            period = inputs[1] if len(inputs) > 1 else \"1mo\"\n",
    "            news_limit = inputs[2] if len(inputs) > 2 else 10\n",
    "            \n",
    "            # Execute parallel fetching\n",
    "            results = self._execute_parallel(symbol, period, news_limit)\n",
    "            \n",
    "            # Combine results\n",
    "            bundle = self._combine_results(symbol, results)\n",
    "            \n",
    "            return bundle\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"symbol\": inputs[0] if len(inputs) > 0 else \"UNKNOWN\",\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"financial_data\": None,\n",
    "                \"news_data\": None,\n",
    "                \"errors\": [{\"source\": \"ingestion_coordinator\", \"error\": str(e)}],\n",
    "                \"status\": \"error\"\n",
    "            }\n",
    "    \n",
    "    def _execute_parallel(self, symbol: str, period: str, news_limit: int) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Execute all ingestion tasks in parallel.\n",
    "        \n",
    "        Args:\n",
    "            symbol: Stock ticker symbol\n",
    "            period: Time period for historical data\n",
    "            news_limit: Number of news articles\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with results from all sources\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            \"financial\": None,\n",
    "            \"news\": None\n",
    "        }\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "            # Submit tasks\n",
    "            future_to_source = {\n",
    "                executor.submit(self.financial_ingestor.execute, symbol, period): \"financial\",\n",
    "                executor.submit(self.news_ingestor.execute, symbol, news_limit): \"news\"\n",
    "            }\n",
    "            \n",
    "            # Collect results as they complete\n",
    "            for future in as_completed(future_to_source):\n",
    "                source = future_to_source[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    results[source] = result\n",
    "                except Exception as e:\n",
    "                    results[source] = {\n",
    "                        \"source\": source,\n",
    "                        \"data\": None,\n",
    "                        \"status\": \"error\",\n",
    "                        \"error\": str(e),\n",
    "                        \"timestamp\": datetime.now().isoformat()\n",
    "                    }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _combine_results(self, symbol: str, results: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Combine results from all ingestors into single bundle.\n",
    "        \n",
    "        Args:\n",
    "            symbol: Stock ticker symbol\n",
    "            results: Dictionary with results from each ingestor\n",
    "            \n",
    "        Returns:\n",
    "            Combined data bundle\n",
    "        \"\"\"\n",
    "        errors = []\n",
    "        \n",
    "        # Extract financial data\n",
    "        financial_result = results.get(\"financial\", {})\n",
    "        financial_data = financial_result.get(\"data\") if financial_result.get(\"status\") == \"success\" else None\n",
    "        if financial_result.get(\"error\"):\n",
    "            errors.append({\"source\": \"financial\", \"error\": financial_result.get(\"error\")})\n",
    "        \n",
    "        # Extract news data\n",
    "        news_result = results.get(\"news\", {})\n",
    "        news_data = news_result.get(\"data\") if news_result.get(\"status\") in [\"success\", \"partial_success\"] else None\n",
    "        if news_result.get(\"error\"):\n",
    "            errors.append({\"source\": \"news\", \"error\": news_result.get(\"error\")})\n",
    "        \n",
    "        # Determine overall status\n",
    "        status = self._determine_status(results)\n",
    "        \n",
    "        return {\n",
    "            \"symbol\": symbol,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"financial_data\": financial_data,\n",
    "            \"news_data\": news_data,\n",
    "            \"errors\": errors,\n",
    "            \"status\": status\n",
    "        }\n",
    "    \n",
    "    def _determine_status(self, results: Dict[str, Any]) -> str:\n",
    "        \"\"\"\n",
    "        Determine overall ingestion status based on results.\n",
    "        \n",
    "        Args:\n",
    "            results: Dictionary with results from each ingestor\n",
    "            \n",
    "        Returns:\n",
    "            Status string: \"success\", \"partial_success\", or \"error\"\n",
    "        \"\"\"\n",
    "        success_count = 0\n",
    "        total_count = len(results)\n",
    "        \n",
    "        for source, result in results.items():\n",
    "            if result and result.get(\"status\") in [\"success\", \"partial_success\"]:\n",
    "                success_count += 1\n",
    "        \n",
    "        if success_count == total_count:\n",
    "            return \"success\"\n",
    "        elif success_count > 0:\n",
    "            return \"partial_success\"\n",
    "        else:\n",
    "            return \"error\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c32775-fd8c-469b-ba12-901703ea82c7",
   "metadata": {},
   "source": [
    "# STEP 2 - Summarizer \n",
    "- Formats data into structured insight with routed notes and confidence scores\n",
    "- stored as summary_result[\"summary\"]\n",
    "\n",
    "# STEP 3 - Memory \n",
    "- stores retrieves and searches short text memory with tags for agent persistence\n",
    "- saves to data/agent_memory.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4282c88-d6bd-4620-b8fe-fe25bb90f3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# (step 2)\n",
    "# src/summarizer/summarizer.py\n",
    "# ---------------------------\n",
    "\n",
    "ROUTES = {\n",
    "    \"earnings\": [\"eps\", \"guidance\", \"revenue\", \"call\", \"forecast\", \"beat\", \"miss\", \"margin\"],\n",
    "    \"macro\":    [\"fed\", \"rate\", \"cpi\", \"inflation\", \"jobs\", \"gdp\", \"unemployment\", \"yields\", \"oil\"],\n",
    "    \"company\":  [\"product\", \"launch\", \"recall\", \"supply\", \"lawsuit\", \"merger\", \"partnership\", \"contract\"],\n",
    "}\n",
    "\n",
    "def _route(text: str) -> str:\n",
    "    t = (text or \"\").lower()\n",
    "    for route, keys in ROUTES.items():\n",
    "        if any(k in t for k in keys):\n",
    "            return route\n",
    "    return \"company\"\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"You are a pragmatic equity analyst.\n",
    "Goal: {goal}\n",
    "Symbol: {symbol}\n",
    "\n",
    "Context (recent daily stats + sampled headlines):\n",
    "{context}\n",
    "\n",
    "Write 5–8 concise bullets on likely near-term price *drivers* and 2 bullets on *key risks*.\n",
    "Avoid hype; be specific. Include dates or sources inline when present.\n",
    "\"\"\"\n",
    "\n",
    "class SummarizerWorker(BaseWorker):\n",
    "    def __init__(self, name=\"summarizer\", role=\"news_summary\", model: str | None = None):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.role = role\n",
    "        self.model = model\n",
    "\n",
    "    def execute(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        symbol: str = inputs[\"symbol\"]\n",
    "        news_daily = inputs.get(\"news_daily\")\n",
    "        raw_news   = inputs.get(\"raw_news\")\n",
    "        window     = int(inputs.get(\"window\", 7))\n",
    "        goal       = inputs.get(\"analysis_goal\", f\"Next-week price drivers for {symbol}\")\n",
    "\n",
    "        context = self._format_context(news_daily, raw_news, window)\n",
    "        routed  = self._route_headlines(raw_news)\n",
    "        prompt  = PROMPT_TEMPLATE.format(goal=goal, symbol=symbol, context=context)\n",
    "\n",
    "        summary_text = (\n",
    "            \"(Stubbed summary — replace with your LLM call)\\n\"\n",
    "            + prompt\n",
    "            + \"\\n- Headlines cluster around a few catalysts; monitor official updates.\\n\"\n",
    "              \"- Tone is slightly positive; momentum sensitive to macro prints.\\n\"\n",
    "              \"- Risks: guidance/margin pressure; policy surprises.\"\n",
    "        )\n",
    "\n",
    "        confidence = self._confidence_from_news(news_daily, window)\n",
    "        memory_writes = [\n",
    "            f\"[{symbol}] {window}d summary (conf={confidence:.2f})\",\n",
    "            f\"[{symbol}] Routes: \" + \", \".join([k for k, v in routed.items() if v])\n",
    "        ]\n",
    "\n",
    "        return {\n",
    "            \"symbol\": symbol,\n",
    "            \"summary\": summary_text,\n",
    "            \"routed_notes\": routed,\n",
    "            \"confidence\": confidence,\n",
    "            \"artifacts\": {\"prompt\": prompt},\n",
    "            \"memory_writes\": memory_writes,\n",
    "        }\n",
    "\n",
    "    # -------- helpers --------\n",
    "    def _format_context(self, news_daily, raw_news: Union[List[dict], \"pd.DataFrame\", None], window: int) -> str:\n",
    "        parts: List[str] = []\n",
    "        if news_daily is not None and hasattr(news_daily, \"tail\") and len(news_daily) > 0:\n",
    "            tail = news_daily.tail(window)\n",
    "            parts.append(\"Daily sentiment (most recent first):\")\n",
    "            for idx, row in tail.iloc[::-1].iterrows():\n",
    "                parts.append(\n",
    "                    f\"- {idx.date()}: count={int(row.get('news_count', 0))}, \"\n",
    "                    f\"sent_mean={row.get('sent_mean', 0):+.3f}, decay={row.get('sent_decay', 0):+.3f}\"\n",
    "                )\n",
    "        if raw_news is not None:\n",
    "            try:\n",
    "                import pandas as pd\n",
    "                df = raw_news if isinstance(raw_news, pd.DataFrame) else pd.DataFrame(raw_news)\n",
    "                ts = \"published\" if \"published\" in df.columns else (df.columns[0] if len(df.columns) else None)\n",
    "                if ts:\n",
    "                    df = df.sort_values(by=ts).tail(12)\n",
    "                parts.append(\"Recent headlines:\")\n",
    "                for _, r in df.iterrows():\n",
    "                    ttl = str(r.get(\"title\", \"\"))[:160]\n",
    "                    src = r.get(\"source\", \"\") or \"news\"\n",
    "                    dt  = r.get(\"published\", r.get(\"date\", \"\"))\n",
    "                    parts.append(f\"- [{dt}] ({src}) {ttl}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        return \"\\n\".join(parts) if parts else \"No recent news.\"\n",
    "\n",
    "    def _route_headlines(self, raw_news) -> dict:\n",
    "        routed = {\"earnings\": [], \"macro\": [], \"company\": []}\n",
    "        if raw_news is None:\n",
    "            return routed\n",
    "        try:\n",
    "            import pandas as pd\n",
    "            df = raw_news if isinstance(raw_news, pd.DataFrame) else pd.DataFrame(raw_news)\n",
    "            for _, r in df.tail(50).iterrows():\n",
    "                ttl = str(r.get(\"title\", \"\")) or \"\"\n",
    "                routed[_route(ttl)].append(ttl)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return {k: v[:5] for k, v in routed.items()}\n",
    "\n",
    "    def _confidence_from_news(self, news_daily, window: int) -> float:\n",
    "        if news_daily is None or not hasattr(news_daily, \"tail\") or len(news_daily) == 0:\n",
    "            return 0.50\n",
    "        try:\n",
    "            avg_cnt = float(news_daily.tail(window)[\"news_count\"].mean())\n",
    "            return round(min(1.0, 0.5 + 0.05 * avg_cnt), 2)\n",
    "        except Exception:\n",
    "            return 0.50\n",
    "\n",
    "# ---------------------------\n",
    "# (step 3)\n",
    "# src/memory/memory.py\n",
    "# ---------------------------\n",
    "\n",
    "# Simple in-memory store for this example. For persistence, use a file or DB.\n",
    "_memory_store: List[Dict[str, Any]] = []\n",
    "MEMORY_FILE = \"./data/agent_memory.json\"\n",
    "\n",
    "def _load_memory():\n",
    "    \"\"\"Loads memories from the JSON file into the in-memory list.\"\"\"\n",
    "    global _memory_store\n",
    "    if not os.path.exists(MEMORY_FILE):\n",
    "        _memory_store = []\n",
    "        return\n",
    "    try:\n",
    "        with open(MEMORY_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            _memory_store = json.load(f)\n",
    "    except (json.JSONDecodeError, FileNotFoundError):\n",
    "        _memory_store = []\n",
    "\n",
    "def _save_memory():\n",
    "    \"\"\"Saves the in-memory list of memories to the JSON file.\"\"\"\n",
    "    os.makedirs(os.path.dirname(MEMORY_FILE), exist_ok=True)\n",
    "    with open(MEMORY_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(_memory_store, f, indent=2)\n",
    "\n",
    "# Load memory when module is imported\n",
    "_load_memory()\n",
    "\n",
    "class MemoryWorker(BaseWorker):\n",
    "    \"\"\"\n",
    "    A BaseWorker implementation for managing agent memories. It stores short text\n",
    "    memories with optional tags in a JSON file so the agent can learn across runs.\n",
    "    \"\"\"\n",
    "    def execute(self, *inputs) -> Any:\n",
    "        \"\"\"\n",
    "        Manages agent memories. The first input is the operation ('add', 'search', 'get_recent').\n",
    "\n",
    "        Usage:\n",
    "            - execute('add', 'some memory text', ['tag1', 'tag2'])\n",
    "            - execute('search', 'query text')\n",
    "            - execute('get_recent', 5)\n",
    "        \"\"\"\n",
    "        if not inputs:\n",
    "            raise ValueError(\"MemoryWorker requires at least one input for the operation.\")\n",
    "\n",
    "        operation = inputs[0]\n",
    "\n",
    "        if operation == 'add':\n",
    "            if len(inputs) < 2:\n",
    "                raise ValueError(\"The 'add' operation requires text for the memory.\")\n",
    "            text = inputs[1]\n",
    "            tags = inputs[2] if len(inputs) > 2 else []\n",
    "            entry = {\n",
    "                \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "                \"text\": text,\n",
    "                \"tags\": tags,\n",
    "            }\n",
    "            _memory_store.append(entry)\n",
    "            _save_memory()\n",
    "            return entry\n",
    "        \n",
    "        elif operation == 'search':\n",
    "            if len(inputs) < 2:\n",
    "                raise ValueError(\"The 'search' operation requires a query string.\")\n",
    "            query = inputs[1].lower()\n",
    "            top_k = inputs[2] if len(inputs) > 2 else 5\n",
    "            \n",
    "            matches = [\n",
    "                m for m in reversed(_memory_store) \n",
    "                if query in m['text'].lower() or any(query in t.lower() for t in m.get('tags', []))\n",
    "            ]\n",
    "            return matches[:top_k]\n",
    "\n",
    "        elif operation == 'get_recent':\n",
    "            n = inputs[1] if len(inputs) > 1 else 5\n",
    "            return list(reversed(_memory_store))[:n]\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown operation: {operation}. Available operations: 'add', 'search', 'get_recent'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd32208-5143-4ade-a759-52d608c46854",
   "metadata": {},
   "source": [
    "# Step 4 - Evaluator-optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fccc6dbd-dcc8-47aa-9afa-351f05f2fea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# (step 4)\n",
    "# evaluator_optimizer/init.py\n",
    "# ---------------------------\n",
    "\n",
    "\"\"\"\n",
    "Evaluator-Optimizer Module\n",
    "\n",
    "Implements the Evaluator-Optimizer workflow pattern for iterative refinement\n",
    "of investment research summaries.\n",
    "\"\"\"\n",
    "\n",
    "__all__ = [\"EvaluatorOptimizer\", \"Feedback\", \"State\"]\n",
    "\n",
    "# ---------------------------\n",
    "# (step 4)\n",
    "# evaluator_optimizer/evaluator_optimizer.py\n",
    "# ---------------------------\n",
    "\n",
    "\"\"\"\n",
    "Evaluator-Optimizer Module\n",
    "\n",
    "Implements the Evaluator-Optimizer workflow pattern where:\n",
    "1. Generator creates an investment research summary\n",
    "2. Evaluator assesses quality and provides feedback\n",
    "3. Loop continues until quality passes or max iterations reached\n",
    "4. Final summary is stored in memory\n",
    "\n",
    "Based on LangGraph pattern: https://langchain-ai.github.io/langgraph/tutorials/workflows/#evaluator-optimizer\n",
    "\"\"\"\n",
    "\n",
    "# --- State Definition ---\n",
    "class State(TypedDict):\n",
    "    \"\"\"Graph state for the Evaluator-Optimizer workflow\"\"\"\n",
    "    symbol: str  # Stock symbol being analyzed\n",
    "    instructions: str  # User's request/query about the stock\n",
    "    context: Dict[str, Any]  # Financial data context (news, prices, etc.)\n",
    "    summary: str  # Current investment research summary\n",
    "    feedback: str  # Feedback from evaluator\n",
    "    grade: str  # Quality grade: \"pass\" or \"fail\"\n",
    "    quality_score: float  # Numeric quality score (0-10)\n",
    "    issues: List[str]  # List of identified issues\n",
    "    iteration: int  # Current iteration number\n",
    "    max_iterations: int  # Maximum allowed iterations\n",
    "    history: List[Dict[str, Any]]  # History of iterations for tracking\n",
    "\n",
    "# --- Structured Output Schema for Evaluation ---\n",
    "class Feedback(BaseModel):\n",
    "    \"\"\"Structured evaluation feedback from the Evaluator\"\"\"\n",
    "    \n",
    "    grade: Literal[\"pass\", \"fail\"] = Field(\n",
    "        description=\"Overall quality assessment: 'pass' if summary meets quality criteria, 'fail' otherwise\"\n",
    "    )\n",
    "    quality_score: float = Field(\n",
    "        description=\"Numeric quality score from 0-10, where 10 is excellent\",\n",
    "        ge=0.0,\n",
    "        le=10.0\n",
    "    )\n",
    "    feedback: str = Field(\n",
    "        description=\"Detailed, actionable feedback for improving the summary if grade is 'fail'\"\n",
    "    )\n",
    "    issues: List[str] = Field(\n",
    "        description=\"List of specific issues identified in the summary\",\n",
    "        default_factory=list\n",
    "    )\n",
    "\n",
    "# --- Evaluator-Optimizer Class ---\n",
    "class EvaluatorOptimizer(BaseWorker):\n",
    "    \"\"\"\n",
    "    Implements the Evaluator-Optimizer workflow pattern for investment research summaries.\n",
    "    \n",
    "    The workflow:\n",
    "    1. Generator creates an initial summary from context data\n",
    "    2. Evaluator assesses quality against defined criteria\n",
    "    3. If quality fails, provides feedback and loops back to Generator\n",
    "    4. Continues until quality passes or max iterations reached\n",
    "    5. Returns final optimized summary\n",
    "    \"\"\"\n",
    "    \n",
    "    # Quality criteria for investment research summaries\n",
    "    QUALITY_CRITERIA = \"\"\"\n",
    "    A high-quality investment research summary should:\n",
    "    1. COMPLETENESS: Cover key financial metrics, sentiment analysis, and risk factors\n",
    "    2. CLARITY: Be well-structured, concise, and easy to understand\n",
    "    3. ACTIONABILITY: Include a clear investment recommendation (buy/sell/hold) with rationale\n",
    "    4. EVIDENCE-BASED: Back claims with specific data from news and financial metrics\n",
    "    5. COHERENCE: Have logical flow without contradictions\n",
    "    6. RISK AWARENESS: Acknowledge both opportunities and risks\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm: Optional[ChatOpenAI] = None, max_iterations: int = 3):\n",
    "        \"\"\"\n",
    "        Initialize the Evaluator-Optimizer.\n",
    "        \n",
    "        Args:\n",
    "            llm: Language model instance (if None, initializes OpenAI LLM from environment)\n",
    "            max_iterations: Maximum refinement iterations before stopping\n",
    "        \"\"\"\n",
    "        # Initialize LLM\n",
    "        if llm is not None:\n",
    "            self.llm = llm\n",
    "        else:\n",
    "            self.llm = self._initialize_openai_llm()\n",
    "        \n",
    "        self.max_iterations = max_iterations\n",
    "        \n",
    "        # Create structured output LLM for evaluation\n",
    "        self.evaluator_llm = self.llm.with_structured_output(Feedback)\n",
    "        \n",
    "        # Build the workflow graph\n",
    "        self.workflow = self._build_workflow()\n",
    "    \n",
    "    def _initialize_openai_llm(self) -> ChatOpenAI:\n",
    "        \"\"\"Initialize OpenAI LLM with API key from environment\"\"\"\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        llm = ChatOpenAI(\n",
    "            model=\"gpt-4o-mini\",  # Using gpt-4o-mini for cost efficiency\n",
    "            temperature=0.7,\n",
    "            api_key=api_key\n",
    "        )\n",
    "        return llm\n",
    "    \n",
    "    def _build_workflow(self) -> StateGraph:\n",
    "        \"\"\"Builds the LangGraph workflow for Evaluator-Optimizer pattern\"\"\"\n",
    "        \n",
    "        # Create the graph\n",
    "        builder = StateGraph(State)\n",
    "        \n",
    "        # Add nodes\n",
    "        builder.add_node(\"generator\", self._generator_node)\n",
    "        builder.add_node(\"evaluator\", self._evaluator_node)\n",
    "        \n",
    "        # Add edges\n",
    "        builder.add_edge(START, \"generator\")\n",
    "        builder.add_edge(\"generator\", \"evaluator\")\n",
    "        \n",
    "        # Conditional edge: loop back to generator or end\n",
    "        builder.add_conditional_edges(\n",
    "            \"evaluator\",\n",
    "            self._should_continue,\n",
    "            {\n",
    "                \"continue\": \"generator\",  # Loop back with feedback\n",
    "                \"end\": END  # Quality passed or max iterations reached\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Compile the workflow\n",
    "        return builder.compile()\n",
    "    \n",
    "    def _generator_node(self, state: State) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generator Node: Creates or refines the investment research summary.\n",
    "        \n",
    "        On first iteration: creates initial summary from context\n",
    "        On subsequent iterations: refines summary based on evaluator feedback\n",
    "        \"\"\"\n",
    "        symbol = state[\"symbol\"]\n",
    "        context = state[\"context\"]\n",
    "        instructions = state.get(\"instructions\", \"\")\n",
    "        feedback = state.get(\"feedback\", \"\")\n",
    "        iteration = state.get(\"iteration\", 0)\n",
    "        \n",
    "        # Format the context data\n",
    "        formatted_context = self._format_context(context)\n",
    "        \n",
    "        # Build the prompt\n",
    "        if iteration == 0:\n",
    "            # Initial summary generation\n",
    "            prompt = f\"\"\"You are an expert financial analyst. Create a comprehensive investment research summary for {symbol}.\n",
    "\n",
    "USER REQUEST:\n",
    "{instructions}\n",
    "\n",
    "AVAILABLE DATA:\n",
    "{formatted_context}\n",
    "\n",
    "QUALITY REQUIREMENTS:\n",
    "{self.QUALITY_CRITERIA}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Directly address the user's request/questions in your analysis\n",
    "2. Use the provided financial data, news, and market context to support your analysis\n",
    "3. If the user asked specific questions, answer them explicitly\n",
    "4. If data is missing or unavailable, acknowledge it and work with what's available\n",
    "5. Provide a clear investment recommendation (buy/sell/hold) with detailed rationale\n",
    "6. Structure your response professionally with clear sections\n",
    "7. Back all claims with specific data points from the context\n",
    "\n",
    "Generate a well-structured investment research summary that meets all quality criteria and addresses the user's request.\"\"\"\n",
    "        else:\n",
    "            # Refinement based on feedback\n",
    "            current_summary = state[\"summary\"]\n",
    "            prompt = f\"\"\"You are refining an investment research summary for {symbol}. \n",
    "\n",
    "USER REQUEST:\n",
    "{instructions}\n",
    "\n",
    "CURRENT SUMMARY:\n",
    "{current_summary}\n",
    "\n",
    "EVALUATOR FEEDBACK:\n",
    "{feedback}\n",
    "\n",
    "ISSUES IDENTIFIED:\n",
    "{', '.join(state.get('issues', []))}\n",
    "\n",
    "AVAILABLE DATA:\n",
    "{formatted_context}\n",
    "\n",
    "QUALITY REQUIREMENTS:\n",
    "{self.QUALITY_CRITERIA}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Address all issues identified by the evaluator\n",
    "2. Ensure the user's original request is still fully addressed\n",
    "3. Improve clarity, completeness, and actionability\n",
    "4. Add missing data points or analysis where needed\n",
    "5. Maintain professional structure and tone\n",
    "\n",
    "Generate an improved version that addresses all feedback and meets quality standards.\"\"\"\n",
    "        \n",
    "        # Generate summary\n",
    "        response = self.llm.invoke(prompt)\n",
    "        new_summary = response.content\n",
    "        \n",
    "        # Update iteration count\n",
    "        new_iteration = iteration + 1\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"GENERATOR - Iteration {new_iteration}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"User Request: {instructions[:80]}...\")\n",
    "        print(f\"Summary generated ({len(new_summary)} characters)\")\n",
    "        if iteration > 0:\n",
    "            print(f\"Addressing feedback: {feedback[:100]}...\")\n",
    "        \n",
    "        return {\n",
    "            \"summary\": new_summary,\n",
    "            \"iteration\": new_iteration\n",
    "        }\n",
    "    \n",
    "    def _evaluator_node(self, state: State) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Evaluator Node: Assesses the quality of the summary and provides feedback.\n",
    "        \n",
    "        Uses structured output to return:\n",
    "        - grade: \"pass\" or \"fail\"\n",
    "        - quality_score: 0-10\n",
    "        - feedback: actionable improvement suggestions\n",
    "        - issues: specific problems identified\n",
    "        \"\"\"\n",
    "        summary = state[\"summary\"]\n",
    "        symbol = state[\"symbol\"]\n",
    "        instructions = state.get(\"instructions\", \"\")\n",
    "        iteration = state[\"iteration\"]\n",
    "        \n",
    "        # Evaluation prompt\n",
    "        prompt = f\"\"\"You are a senior financial analyst evaluating an investment research summary for {symbol}.\n",
    "\n",
    "USER'S ORIGINAL REQUEST:\n",
    "{instructions}\n",
    "\n",
    "SUMMARY TO EVALUATE:\n",
    "{summary}\n",
    "\n",
    "EVALUATION CRITERIA:\n",
    "{self.QUALITY_CRITERIA}\n",
    "\n",
    "ASSESSMENT REQUIREMENTS:\n",
    "1. Does the summary directly address the user's request/questions?\n",
    "2. Is the analysis backed by specific data points?\n",
    "3. Are all quality criteria met (completeness, clarity, actionability, evidence-based, coherence, risk awareness)?\n",
    "4. Is the investment recommendation clear and well-justified?\n",
    "5. Are there any contradictions or unsupported claims?\n",
    "6. Is the structure professional and easy to follow?\n",
    "\n",
    "Provide:\n",
    "- grade: \"pass\" if the summary meets professional standards and addresses the user's request, \"fail\" if significant improvements needed\n",
    "- quality_score: 0-10 (be generous with 7+ for good work, reserve 9+ for exceptional analysis)\n",
    "- feedback: Specific, actionable suggestions for improvement (if grade is \"fail\")\n",
    "- issues: List specific problems (missing data, unclear reasoning, unanswered questions, etc.)\n",
    "\n",
    "Be fair but thorough. A passing grade means the summary is publication-ready and fully addresses the user's needs.\"\"\"\n",
    "        \n",
    "        # Get structured evaluation\n",
    "        evaluation = self.evaluator_llm.invoke(prompt)\n",
    "        \n",
    "        # Track iteration history\n",
    "        history = state.get(\"history\", [])\n",
    "        history.append({\n",
    "            \"iteration\": iteration,\n",
    "            \"summary_length\": len(summary),\n",
    "            \"grade\": evaluation.grade,\n",
    "            \"quality_score\": evaluation.quality_score,\n",
    "            \"issues_count\": len(evaluation.issues)\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"EVALUATOR - Iteration {iteration}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Grade: {evaluation.grade.upper()}\")\n",
    "        print(f\"Quality Score: {evaluation.quality_score}/10\")\n",
    "        print(f\"Issues Found: {len(evaluation.issues)}\")\n",
    "        if evaluation.issues:\n",
    "            for i, issue in enumerate(evaluation.issues, 1):\n",
    "                print(f\"  {i}. {issue}\")\n",
    "        print(f\"Feedback: {evaluation.feedback[:150]}...\")\n",
    "        \n",
    "        return {\n",
    "            \"grade\": evaluation.grade,\n",
    "            \"quality_score\": evaluation.quality_score,\n",
    "            \"feedback\": evaluation.feedback,\n",
    "            \"issues\": evaluation.issues,\n",
    "            \"history\": history\n",
    "        }\n",
    "    \n",
    "    def _should_continue(self, state: State) -> Literal[\"continue\", \"end\"]:\n",
    "        \"\"\"\n",
    "        Conditional routing: decide whether to continue refinement or end.\n",
    "        \n",
    "        Continue if:\n",
    "        - Grade is \"fail\" AND\n",
    "        - Haven't reached max iterations\n",
    "        \n",
    "        End if:\n",
    "        - Grade is \"pass\" OR\n",
    "        - Max iterations reached\n",
    "        \"\"\"\n",
    "        grade = state[\"grade\"]\n",
    "        iteration = state[\"iteration\"]\n",
    "        max_iterations = state[\"max_iterations\"]\n",
    "        \n",
    "        if grade == \"pass\":\n",
    "            print(f\"\\nQuality PASSED - Ending optimization\")\n",
    "            return \"end\"\n",
    "        elif iteration >= max_iterations:\n",
    "            print(f\"\\nMax iterations ({max_iterations}) reached - Ending optimization\")\n",
    "            return \"end\"\n",
    "        else:\n",
    "            print(f\"\\nQuality FAILED - Continuing to iteration {iteration + 1}\")\n",
    "            return \"continue\"\n",
    "    \n",
    "    def _format_context(self, context: Dict[str, Any]) -> str:\n",
    "        \"\"\"Formats context data into a readable string for prompts\"\"\"\n",
    "        formatted = []\n",
    "        \n",
    "        # Format financial data with key metrics\n",
    "        if \"financial_data\" in context:\n",
    "            fin_data = context[\"financial_data\"]\n",
    "            formatted.append(\"=== FINANCIAL METRICS ===\")\n",
    "            formatted.append(f\"Symbol: {fin_data.get('symbol', 'N/A')}\")\n",
    "            formatted.append(f\"Current Price: ${fin_data.get('current_price', 0):.2f}\")\n",
    "            formatted.append(f\"Price Change: ${fin_data.get('price_change', 0):.2f} ({fin_data.get('price_change_pct', 0):.2f}%)\")\n",
    "            formatted.append(f\"Volume: {fin_data.get('volume', 0):,}\")\n",
    "            formatted.append(f\"Market Cap: ${fin_data.get('market_cap', 0):,}\")\n",
    "            formatted.append(f\"P/E Ratio: {fin_data.get('pe_ratio', 0):.2f}\")\n",
    "            formatted.append(f\"Dividend Yield: {fin_data.get('dividend_yield', 0):.2f}%\")\n",
    "            formatted.append(f\"52-Week Range: ${fin_data.get('52_week_low', 0):.2f} - ${fin_data.get('52_week_high', 0):.2f}\")\n",
    "            formatted.append(f\"Beta: {fin_data.get('beta', 0):.3f}\")\n",
    "            formatted.append(f\"Sector: {fin_data.get('sector', 'N/A')}\")\n",
    "            formatted.append(f\"Industry: {fin_data.get('industry', 'N/A')}\")\n",
    "            \n",
    "            # Add volatility if available\n",
    "            if 'volatility_30d' in fin_data:\n",
    "                formatted.append(f\"30-Day Volatility: {fin_data['volatility_30d']:.2%}\")\n",
    "            \n",
    "            # Add company summary if available\n",
    "            if 'company_summary' in fin_data and fin_data['company_summary']:\n",
    "                formatted.append(f\"\\nCompany Overview: {fin_data['company_summary'][:300]}...\")\n",
    "            \n",
    "            # Add recent price trends from historical data\n",
    "            if 'historical_data' in fin_data and fin_data['historical_data']:\n",
    "                hist = fin_data['historical_data']\n",
    "                if len(hist) >= 5:\n",
    "                    formatted.append(\"\\nRecent Price Trend (Last 5 Days):\")\n",
    "                    for i, day in enumerate(hist[-5:], 1):\n",
    "                        formatted.append(f\"  Day {i}: Close=${day.get('Close', 0):.2f}, Volume={day.get('Volume', 0):,}\")\n",
    "        \n",
    "        # Format news data\n",
    "        if \"news_data\" in context:\n",
    "            news = context[\"news_data\"]\n",
    "            if isinstance(news, dict) and \"articles\" in news:\n",
    "                articles = news[\"articles\"]\n",
    "                total = news.get(\"total_count\", 0)\n",
    "                formatted.append(f\"\\n=== NEWS ANALYSIS ===\")\n",
    "                formatted.append(f\"Total Articles Found: {total}\")\n",
    "                \n",
    "                if articles and len(articles) > 0:\n",
    "                    formatted.append(\"\\nRecent Headlines:\")\n",
    "                    for i, article in enumerate(articles[:5], 1):\n",
    "                        title = article.get('title', 'No title')\n",
    "                        date = article.get('publishedAt', article.get('date', 'N/A'))\n",
    "                        formatted.append(f\"  {i}. {title} ({date})\")\n",
    "                else:\n",
    "                    formatted.append(\"No recent news articles available.\")\n",
    "            else:\n",
    "                formatted.append(\"\\n=== NEWS ANALYSIS ===\")\n",
    "                formatted.append(\"News data format not recognized or unavailable.\")\n",
    "        \n",
    "        # Format sentiment if available\n",
    "        if \"sentiment\" in context:\n",
    "            formatted.append(f\"\\n=== SENTIMENT ANALYSIS ===\")\n",
    "            formatted.append(f\"Overall Sentiment: {context['sentiment']}\")\n",
    "        \n",
    "        # Add any errors encountered\n",
    "        if \"errors\" in context and context[\"errors\"]:\n",
    "            formatted.append(f\"\\n=== DATA COLLECTION NOTES ===\")\n",
    "            for error in context[\"errors\"]:\n",
    "                source = error.get(\"source\", \"unknown\")\n",
    "                error_detail = error.get(\"error\", \"Unknown error\")\n",
    "                formatted.append(f\"Note: {source} - {error_detail}\")\n",
    "        \n",
    "        # Add timestamp\n",
    "        if \"timestamp\" in context:\n",
    "            formatted.append(f\"\\nData Retrieved: {context['timestamp']}\")\n",
    "        \n",
    "        return \"\\n\".join(formatted) if formatted else \"No context data available\"\n",
    "    \n",
    "    def execute(self, symbol: str, context: Dict[str, Any], instructions: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Execute the Evaluator-Optimizer workflow.\n",
    "        \n",
    "        Args:\n",
    "            symbol: Stock symbol to analyze\n",
    "            context: Dictionary containing financial data, news, sentiment, etc.\n",
    "            instructions: User's request/query (e.g., \"Should I buy AAPL?\" or \"Analyze the potential for AAPL stock\")\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with:\n",
    "                - final_summary: The optimized summary\n",
    "                - quality_score: Final quality score\n",
    "                - iterations: Number of iterations performed\n",
    "                - history: Detailed iteration history\n",
    "                - passed: Whether quality criteria were met\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'#'*60}\")\n",
    "        print(f\"EVALUATOR-OPTIMIZER WORKFLOW\")\n",
    "        print(f\"Symbol: {symbol}\")\n",
    "        print(f\"User Request: {instructions}\")\n",
    "        print(f\"Max Iterations: {self.max_iterations}\")\n",
    "        print(f\"{'#'*60}\")\n",
    "        \n",
    "        # Initialize state\n",
    "        initial_state = {\n",
    "            \"symbol\": symbol,\n",
    "            \"instructions\": instructions,\n",
    "            \"context\": context,\n",
    "            \"summary\": summary_result[\"summary\"], # Add initial summary from summarizer\n",
    "            \"feedback\": \"\",\n",
    "            \"grade\": \"\",\n",
    "            \"quality_score\": 0.0,\n",
    "            \"issues\": [],\n",
    "            \"iteration\": 0,\n",
    "            \"max_iterations\": self.max_iterations,\n",
    "            \"history\": []\n",
    "        }\n",
    "        \n",
    "        # Run the workflow\n",
    "        final_state = self.workflow.invoke(initial_state)\n",
    "        \n",
    "        # Prepare results\n",
    "        results = {\n",
    "            \"final_summary\": final_state[\"summary\"],\n",
    "            \"quality_score\": final_state[\"quality_score\"],\n",
    "            \"iterations\": final_state[\"iteration\"],\n",
    "            \"history\": final_state[\"history\"],\n",
    "            \"passed\": final_state[\"grade\"] == \"pass\",\n",
    "            \"final_grade\": final_state[\"grade\"],\n",
    "            \"issues\": final_state[\"issues\"]\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{'#'*60}\")\n",
    "        print(f\"WORKFLOW COMPLETE\")\n",
    "        print(f\"{'#'*60}\")\n",
    "        print(f\"Total Iterations: {results['iterations']}\")\n",
    "        print(f\"Final Grade: {results['final_grade'].upper()}\")\n",
    "        print(f\"Final Quality Score: {results['quality_score']}/10\")\n",
    "        print(f\"Quality Passed: {'✅ YES' if results['passed'] else '❌ NO'}\")\n",
    "        print(f\"{'#'*60}\\n\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def visualize(self, output_path: str = \"evaluator_optimizer_graph.png\"):\n",
    "        \"\"\"\n",
    "        Visualize the workflow graph.\n",
    "        \n",
    "        Args:\n",
    "            output_path: Path to save the graph image\n",
    "        \"\"\"\n",
    "        try:\n",
    "            img_data = self.workflow.get_graph().draw_mermaid_png()\n",
    "            with open(output_path, \"wb\") as f:\n",
    "                f.write(img_data)\n",
    "            print(f\"Workflow graph saved to: {output_path}\")\n",
    "            return Image(img_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not generate graph visualization: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e4a45b-ef90-44f3-94ac-3d37f2ca6c8b",
   "metadata": {},
   "source": [
    "# Step 0 - Orchestrator\n",
    "- initialize workers\n",
    "- planning - execute workers in order\n",
    "- routing - pass outputs from one worker to the next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a15e14a3-5cc7-4c5d-a7fb-82a830347ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Orchestrator Module ---\n",
    "class Orchestrator:\n",
    "    \"\"\"\n",
    "    Coordinates the workflow of all workers:\n",
    "    1. Ingestion: fetch financial and news data\n",
    "    2. Summarizer: generate summary from ingested news\n",
    "    3. Memory: store summary and metadata\n",
    "    4. EvaluatorOptimizer: refine the summary iteratively\n",
    "    \"\"\"\n",
    "\n",
    "    # -----------------------------\n",
    "    # Worker Initialization\n",
    "    # -----------------------------\n",
    "    def __init__(self):\n",
    "        self.workers = {\n",
    "            \"ingestion\": Ingestion(),\n",
    "            \"summarizer\": SummarizerWorker(),\n",
    "            \"memory\": MemoryWorker(),\n",
    "            \"evaluator_optimizer\": EvaluatorOptimizer(),\n",
    "        }\n",
    "\n",
    "    # -----------------------------\n",
    "    # Planning and Routing Execution\n",
    "    # -----------------------------\n",
    "    def execute(self, symbol: str, instructions: str) -> str:\n",
    "        \"\"\"Runs the full workflow and returns a formatted Markdown summary\"\"\"\n",
    "\n",
    "        # --- Step 1: Fetch financial and news data ---\n",
    "        ingestion_result = self.workers[\"ingestion\"].execute(symbol)\n",
    "        news_articles = (ingestion_result.get(\"news_data\") or {}).get(\"articles\", [])\n",
    "\n",
    "        # --- Step 2: Summarize news ---\n",
    "        summary_result = self.workers[\"summarizer\"].execute({\n",
    "            \"symbol\": symbol,\n",
    "            \"raw_news\": news_articles,\n",
    "            \"news_daily\": None,  # defaults confidence to 0.5 if not provided\n",
    "            \"window\": 7,\n",
    "            \"analysis_goal\": instructions\n",
    "        })\n",
    "\n",
    "        # --- Step 3: Store summary in memory ---\n",
    "        for note in summary_result.get(\"memory_writes\", []):\n",
    "            self.workers[\"memory\"].execute(\"add\", note, [symbol, \"summary\"])\n",
    "\n",
    "        # --- Step 4: Evaluator-Optimizer Workflow ---\n",
    "        evaluator = self.workers[\"evaluator_optimizer\"]\n",
    "        initial_state = {\n",
    "            \"symbol\": symbol,\n",
    "            \"instructions\": instructions,\n",
    "            \"context\": ingestion_result,\n",
    "            \"summary\": summary_result[\"summary\"],\n",
    "            \"feedback\": \"\",\n",
    "            \"grade\": \"\",\n",
    "            \"quality_score\": 0.0,\n",
    "            \"issues\": [],\n",
    "            \"iteration\": 0,\n",
    "            \"max_iterations\": evaluator.max_iterations,\n",
    "            \"history\": []\n",
    "        }\n",
    "\n",
    "        final_result = evaluator.workflow.invoke(initial_state)\n",
    "\n",
    "        # Step 5: Store final optimized summary in memory\n",
    "        final_summary_text = final_result[\"summary\"]\n",
    "        memory_note = f\"[{symbol}] Final Optimized Summary\"\n",
    "        self.workers[\"memory\"].execute(\"add\", memory_note, [symbol, \"summary\", final_summary_text])\n",
    "\n",
    "        return final_summary_text  # returns plain Markdown string\n",
    "\n",
    "# -----------------------------\n",
    "# Helper Function to Run Analysis\n",
    "# -----------------------------\n",
    "def run_investment_analysis(symbol: str, instructions: str) -> str:\n",
    "    orchestrator = Orchestrator()\n",
    "    return orchestrator.execute(symbol, instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "721c9582-9571-4781-861d-d292168c2a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "GENERATOR - Iteration 1\n",
      "============================================================\n",
      "User Request: Analyze stock for the past 10 days......\n",
      "Summary generated (3623 characters)\n",
      "\n",
      "============================================================\n",
      "EVALUATOR - Iteration 1\n",
      "============================================================\n",
      "Grade: PASS\n",
      "Quality Score: 8.0/10\n",
      "Issues Found: 0\n",
      "Feedback: ...\n",
      "\n",
      "Quality PASSED - Ending optimization\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Investment Research Summary: Apple Inc. (AAPL)\n",
       "\n",
       "## 1. Company Overview\n",
       "Apple Inc. (AAPL) is a leading technology company known for designing, manufacturing, and marketing consumer electronics, software, and online services. Its product portfolio includes the iPhone, Mac computers, iPad tablets, and a variety of wearables and accessories. The company operates in a highly competitive market, continually innovating to maintain its leadership position.\n",
       "\n",
       "## 2. Recent Financial Metrics\n",
       "- **Current Price**: $247.77\n",
       "- **Price Change**: $0.11 (0.04%)\n",
       "- **Volume**: 35,447,900 shares traded\n",
       "- **Market Cap**: $3.68 trillion\n",
       "- **P/E Ratio**: 29.82\n",
       "- **Dividend Yield**: 0.42%\n",
       "- **52-Week Range**: $169.21 - $260.10\n",
       "- **Beta**: 1.094 (indicating slightly higher volatility than the market)\n",
       "- **Sector**: Technology\n",
       "- **Industry**: Consumer Electronics\n",
       "- **30-Day Volatility**: 25.07%\n",
       "\n",
       "## 3. Recent Price Trend (Last 5 Days)\n",
       "| Day       | Close Price | Volume        |\n",
       "|-----------|-------------|---------------|\n",
       "| Day 1    | $258.06     | 36,496,900    |\n",
       "| Day 2    | $254.04     | 38,322,000    |\n",
       "| Day 3    | $245.27     | 61,999,100    |\n",
       "| Day 4    | $247.66     | 38,142,900    |\n",
       "| Day 5    | $247.77     | 35,447,900    |\n",
       "\n",
       "### Summary of Recent Price Movement\n",
       "- The stock experienced volatility over the last five trading days, with a peak closing price of $258.06 on Day 1 and a low of $245.27 on Day 3.\n",
       "- On average, the stock has shown resilience, recovering some losses to close at $247.77 on Day 5.\n",
       "\n",
       "## 4. News Analysis\n",
       "Recent news articles have highlighted several key developments impacting Apple:\n",
       "1. **Demand Surge**: Apple has seen a surge in demand for its iPhone 17 in China, which may positively influence sales and revenue.\n",
       "2. **Investment in China**: CEO Tim Cook has indicated plans to bolster investments in China, signaling confidence in one of its largest markets.\n",
       "3. **Tax Law Lobbying in India**: Apple is reportedly lobbying the Indian government to modify tax laws, which could enhance its operational efficiency in the country.\n",
       "\n",
       "These developments suggest a favorable sentiment towards Apple’s growth prospects, particularly in international markets.\n",
       "\n",
       "## 5. Risk Factors\n",
       "- **Market Volatility**: AAPL has a beta of 1.094, indicating that it is slightly more volatile than the overall market. Investors should be prepared for price fluctuations.\n",
       "- **Global Economic Conditions**: Apple’s performance is closely tied to global economic conditions and consumer spending trends.\n",
       "- **Regulatory Risks**: Ongoing lobbying efforts in India and potential regulatory changes in China could pose challenges.\n",
       "\n",
       "## 6. Investment Recommendation\n",
       "**Recommendation**: **Hold**\n",
       "\n",
       "### Rationale\n",
       "- **Positive Catalysts**: The strong demand for the iPhone 17 in China and potential regulatory changes in India could drive growth in revenue and market share.\n",
       "- **Volatility Awareness**: While the stock has shown short-term volatility, the fundamentals remain strong with a healthy P/E ratio and consistent demand for its products.\n",
       "- **Market Position**: Apple maintains a leading position in the technology sector, and any long-term investor may benefit from holding onto the stock, particularly given its history of innovation and brand loyalty.\n",
       "\n",
       "### Conclusion\n",
       "Given the current market conditions, recent price trends, and ongoing developments in Apple’s business strategy, a hold position is prudent. Investors should continue to monitor market conditions and company announcements, particularly regarding sales performance and global economic indicators, which could necessitate a reassessment of this recommendation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize orchestrator\n",
    "orchestrator = Orchestrator()\n",
    "\n",
    "# Run analysis\n",
    "result = run_investment_analysis(\"AAPL\", \"Analyze stock for the past 10 days...\")\n",
    "# Output result in Markdown format\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e0aaff-14b0-4d42-93a6-305af9a54b58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
