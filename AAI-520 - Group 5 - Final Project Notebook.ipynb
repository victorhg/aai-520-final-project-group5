{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45de57cb-1a4c-4092-a9a7-4e263931d43a",
   "metadata": {
    "id": "45de57cb-1a4c-4092-a9a7-4e263931d43a"
   },
   "source": [
    "USD AAI-520 - Natural Language Processing and GenAI\n",
    "\n",
    "## Financial Agentic System\n",
    "\n",
    "Group 5: Antonio Recalde, Ajmal Jalal, Darin Verduzco, Victor H. Germano\n",
    "\n",
    "GitHub: https://github.com/victorhg/aai-520-final-project-group5\n",
    "\n",
    "News sources: Yahoo Finance and NewsAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8a5adb-1146-4f3d-811b-0fa38885264b",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"overall_project_workflow.png\" width=\"70%\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320f9da7-a5eb-4512-93a7-2088e306475f",
   "metadata": {
    "id": "320f9da7-a5eb-4512-93a7-2088e306475f"
   },
   "source": [
    "## Project Requirements:\n",
    "Agent Functions\n",
    "1. Plans its research steps for a given stock symbol.\n",
    "2. Uses tools dynamically (APIs, datasets, retrieval).\n",
    "3. Self-reflects to assess the quality of its output.\n",
    "4. Learns across runs (e.g., keeps brief memories or notes to improve future analyses).\n",
    "\n",
    "Workflow Patterns\n",
    "1. Prompt Chaining: Ingest News → Preprocess → Classify → Extract → Summarize\n",
    "2. Routing: Direct content to the right specialist (e.g., earnings, news, or market analyzers).\n",
    "3. Evaluator–Optimizer: Generate analysis → evaluate quality → refine using feedback.\n",
    "\n",
    "## Table of Contents:\n",
    "1. MemoryAnalyzer - Checks if existing memory data is sufficient and fresh to answer a query.\n",
    "2. Data Ingestion - Fetches financial and news data from APIs and RSS feeds; handles errors and structures data.\n",
    "3. Summarizer - Generates concise summaries and routes headlines into categories based on keywords.\n",
    "4. Memory Management - Stores, retrieves, and organizes summaries and relevant metadata for reuse.\n",
    "5. EvaluatorOptimizer - Iteratively refines summaries for quality, consistency, and relevance.\n",
    "6. Orchestrator Agent - Coordinates the workflow of all workers, routing outputs and deciding which steps to skip.\n",
    "7. Run Analysis - Convenience function to execute the full orchestrator workflow and return display summary in Markdown format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7349fe60-ab27-4896-b466-1a343645e7ed",
   "metadata": {
    "id": "7349fe60-ab27-4896-b466-1a343645e7ed"
   },
   "source": [
    "## Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "060fb464",
   "metadata": {
    "id": "060fb464"
   },
   "outputs": [],
   "source": [
    "# !pip install yfinance feedparser requests python-dotenv typing_extensions pydantic langgraph IPython langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bc3a04-5c2e-4581-a002-205c3a2e54fb",
   "metadata": {
    "id": "54bc3a04-5c2e-4581-a002-205c3a2e54fb"
   },
   "source": [
    "## Import Libraries and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf510d52-8d34-41ac-b2b2-d4908db6a394",
   "metadata": {
    "id": "cf510d52-8d34-41ac-b2b2-d4908db6a394"
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Data Handling & I/O\n",
    "# ---------------------------\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import html\n",
    "import requests\n",
    "import feedparser\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "# ---------------------------\n",
    "# Typing & Preprocessing Utilities\n",
    "# ---------------------------\n",
    "from __future__ import annotations\n",
    "from typing_extensions import Annotated\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import Dict, Any, List, Optional, Union, TypedDict, Literal\n",
    "\n",
    "# ---------------------------\n",
    "# Models / Evaluation\n",
    "# ---------------------------\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display, Markdown\n",
    "\n",
    "# Load \"news_openai.env\" environment key file\n",
    "load_dotenv(\"news_openai.env\")\n",
    "# Set NewsAPI key\n",
    "NEWS_API_KEY = os.getenv(\"NEWS_API_KEY\")\n",
    "# OpenAI API key loaded via env variable \"OPENAI_API_KEY\" during \"MemoryAnalyzer\" and \"EvaluatorOptimizer\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f1559c-330a-4922-bf76-f582c4d334c6",
   "metadata": {
    "id": "a3f1559c-330a-4922-bf76-f582c4d334c6"
   },
   "source": [
    "# 1. MemoryAnalyzer\n",
    "\n",
    "<b>Memory = Sufficient:</b>\n",
    "\n",
    "assessment.is_sufficient = True if memory content alone can answer the query\n",
    "\n",
    "is_data_fresh = True if memory is recent enough when freshness is required\n",
    "\n",
    "<br><b>Memory = Insufficient:</b>\n",
    "\n",
    "missing_info = Lists specific content gaps; presence implies insufficient\n",
    "\n",
    "assessment.requires_fresh_data = True if the query needs fresh data\n",
    "\n",
    "is_data_fresh = False = insufficient\n",
    "\n",
    "age_hours / memory_timestamp,  if memory is too old or missing timestamp = insufficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5967fdd",
   "metadata": {
    "id": "e5967fdd"
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# worker/base_worker.py\n",
    "# ---------------------------\n",
    "# ==============================================================================\n",
    "# BASE WORKER CLASS\n",
    "# ==============================================================================\n",
    "# All worker classes inherit from BaseWorker and implement the execute() method\n",
    "\n",
    "class BaseWorker:\n",
    "    def execute(self, *inputs):\n",
    "        \"\"\"Execute the worker's main function. To be overridden by subclasses.\"\"\"\n",
    "        raise NotImplementedError(\"This method should be overridden by subclasses.\")\n",
    "\n",
    "class SufficiencyAssessment(BaseModel):\n",
    "    \"\"\"Structured assessment of whether memory data is sufficient to answer a query\"\"\"\n",
    "\n",
    "    is_sufficient: bool = Field(\n",
    "        description=\"Whether the memory data contains enough information to fully answer the query\"\n",
    "    )\n",
    "    requires_fresh_data: bool = Field(\n",
    "        description=\"Whether this query requires the most current data (e.g., current prices, recent news) or can use older data (e.g., follow-up questions, general analysis)\"\n",
    "    )\n",
    "    missing_information: list[str] = Field(\n",
    "        description=\"List of specific information gaps that would be needed to fully answer the query\",\n",
    "        default_factory=list\n",
    "    )\n",
    "\n",
    "# ---------------------------\n",
    "# src/memory_analyzer/memory_analyzer.py\n",
    "# ---------------------------\n",
    "\n",
    "class MemoryAnalyzer(BaseWorker):\n",
    "    \"\"\"\n",
    "    Analyzes whether existing memory data is sufficient to answer a user query.\n",
    "    Uses LangChain to perform intelligent assessment of data completeness.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the MemoryAnalyzer with OpenAI LLM\"\"\"\n",
    "        super().__init__()\n",
    "        self.llm = self._initialize_llm()\n",
    "        self.structured_llm = self.llm.with_structured_output(SufficiencyAssessment)\n",
    "\n",
    "    def _initialize_llm(self) -> ChatOpenAI:\n",
    "        \"\"\"Initialize OpenAI LLM with API key from environment\"\"\"\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "        if not api_key:\n",
    "            raise ValueError(\n",
    "                \"OPENAI_API_KEY not found in environment variables. \"\n",
    "                \"Please set it in your .env file.\"\n",
    "            )\n",
    "\n",
    "        return ChatOpenAI(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature=0.1,  # Low temperature for consistent analysis\n",
    "            api_key=api_key\n",
    "        )\n",
    "\n",
    "    def _extract_timestamp(self, memory_snapshot: str) -> datetime:\n",
    "        \"\"\"\n",
    "        Extract timestamp from memory snapshot.\n",
    "        Looks for patterns like \"Timestamp: YYYY-MM-DD HH:MM:SS\" or similar.\n",
    "\n",
    "        Returns:\n",
    "            datetime object if found, None otherwise\n",
    "        \"\"\"\n",
    "        # Common timestamp patterns\n",
    "        patterns = [\n",
    "            r'Timestamp:\\s*(\\d{4}-\\d{2}-\\d{2}[T\\s]\\d{2}:\\d{2}:\\d{2})',\n",
    "            r'Date:\\s*(\\d{4}-\\d{2}-\\d{2}[T\\s]\\d{2}:\\d{2}:\\d{2})',\n",
    "            r'Created:\\s*(\\d{4}-\\d{2}-\\d{2}[T\\s]\\d{2}:\\d{2}:\\d{2})',\n",
    "            r'Updated:\\s*(\\d{4}-\\d{2}-\\d{2}[T\\s]\\d{2}:\\d{2}:\\d{2})',\n",
    "            r'(\\d{4}-\\d{2}-\\d{2}[T\\s]\\d{2}:\\d{2}:\\d{2})',  # Just the timestamp\n",
    "        ]\n",
    "\n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, memory_snapshot, re.IGNORECASE)\n",
    "            if match:\n",
    "                timestamp_str = match.group(1)\n",
    "                try:\n",
    "                    # Handle both \"T\" separator and space separator\n",
    "                    if 'T' not in timestamp_str:\n",
    "                        timestamp_str = timestamp_str.replace(' ', 'T')\n",
    "                    return datetime.fromisoformat(timestamp_str)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "        return None\n",
    "\n",
    "    def execute(self, memory_snapshot: str, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Analyze if the memory snapshot contains sufficient data to answer the query,\n",
    "        considering both content relevance and data freshness.\n",
    "\n",
    "        Args:\n",
    "            memory_snapshot: String containing the current memory data\n",
    "            query: User's query/question\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with:\n",
    "                - is_sufficient: Boolean indicating if data is sufficient\n",
    "                - requires_fresh_data: Boolean indicating if query requires fresh data\n",
    "                - missing_information: List of gaps\n",
    "        \"\"\"\n",
    "        if not memory_snapshot or not memory_snapshot.strip():\n",
    "            return {\n",
    "                \"is_sufficient\": False,\n",
    "                \"requires_fresh_data\": True,  # Conservative assumption when no data exists\n",
    "                \"missing_information\": [\"All information - no memory data exists\"]\n",
    "            }\n",
    "\n",
    "        # Extract timestamp from memory snapshot\n",
    "        memory_timestamp = self._extract_timestamp(memory_snapshot)\n",
    "\n",
    "        # Define freshness threshold:\n",
    "        # Current datetime\n",
    "        current_dt = datetime.now()\n",
    "        # Freshness threshold in hours\n",
    "        freshness_threshold_hours = 24\n",
    "            \n",
    "        is_data_fresh = True\n",
    "        age_hours = 0\n",
    "\n",
    "        if memory_timestamp:\n",
    "            age_hours = (current_dt - memory_timestamp).total_seconds() / 3600\n",
    "            is_data_fresh = age_hours <= freshness_threshold_hours\n",
    "        else:\n",
    "            # If no timestamp found, assume data might be stale\n",
    "            is_data_fresh = False\n",
    "\n",
    "        # Create analysis prompt\n",
    "        data_age_info = f\"Data age: {age_hours:.1f} hours old\" if memory_timestamp else \"Data age: unknown\"\n",
    "        prompt = f\"\"\"You are an expert financial analyst evaluating whether existing research data is sufficient to answer a user's query.\n",
    "\n",
    "USER QUERY:\n",
    "{query}\n",
    "\n",
    "AVAILABLE MEMORY DATA:\n",
    "{memory_snapshot}\n",
    "\n",
    "DATA FRESHNESS: {data_age_info}\n",
    "\n",
    "ANALYSIS TASK:\n",
    "Determine if the available memory data contains enough information to fully and accurately answer the user's query.\n",
    "\n",
    "Consider:\n",
    "1. Does the data directly address the query topic?\n",
    "2. Are there specific facts, metrics, or analysis needed to answer the query?\n",
    "3. Does this query require the most current/fresh data (e.g., current prices, breaking news, real-time metrics) or can it use older data (e.g., follow-up questions, general analysis, historical context)?\n",
    "4. Are there any gaps that would require additional research?\n",
    "\n",
    "IMPORTANT: If you are uncertain whether the data is sufficient, err on the side of caution and mark it as insufficient.\n",
    "\n",
    "Provide a structured assessment of data sufficiency.\"\"\"\n",
    "\n",
    "        try:\n",
    "            # Get structured assessment from LLM\n",
    "            assessment = self.structured_llm.invoke(prompt)\n",
    "\n",
    "            # Consider both content sufficiency and data freshness\n",
    "            content_sufficient = assessment.is_sufficient\n",
    "            requires_fresh = assessment.requires_fresh_data\n",
    "\n",
    "            # Final sufficiency decision: content must be sufficient AND data must be fresh if required\n",
    "            final_sufficient = content_sufficient and (not requires_fresh or is_data_fresh)\n",
    "\n",
    "            # Build missing information list\n",
    "            missing_info = assessment.missing_information.copy()\n",
    "            if requires_fresh and not is_data_fresh:\n",
    "                age_msg = f\"Data is too old ({age_hours:.1f} hours > {freshness_threshold_hours} hour threshold)\"\n",
    "                if age_msg not in missing_info:\n",
    "                    missing_info.insert(0, age_msg)\n",
    "\n",
    "            # Convert to dictionary for return\n",
    "            result = {\n",
    "                \"is_sufficient\": final_sufficient,\n",
    "                \"requires_fresh_data\": requires_fresh,\n",
    "                \"missing_information\": missing_info\n",
    "            }\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            # Fallback to basic keyword matching if LLM fails\n",
    "            print(f\"LLM analysis failed: {e}\")\n",
    "            return {\n",
    "                \"is_sufficient\": False, # automatically mark as insufficient so data can be refetched\n",
    "                \"requires_fresh_data\": True,  # Conservative assumption when LLM fails\n",
    "                \"missing_information\": [\"LLM analysis unavailable\"]\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef09596-8c37-451f-b512-a863fc0bfac0",
   "metadata": {
    "id": "6ef09596-8c37-451f-b512-a863fc0bfac0"
   },
   "source": [
    "# 2. Data Ingestion\n",
    "\n",
    "## Overview\n",
    "The Ingestion module is responsible for collecting financial and news data from multiple sources in parallel. It consists of three main components:\n",
    "\n",
    "1. **FinancialDataIngestion**: Fetches stock data from Yahoo Finance (yfinance)\n",
    "   - Historical OHLCV data (Open, High, Low, Close, Volume)\n",
    "   - Company fundamentals (P/E ratio, market cap, beta, sector)\n",
    "   - Calculated metrics (volatility, price changes, volume averages)\n",
    "\n",
    "2. **NewsDataIngestion**: Fetches news articles from multiple sources\n",
    "   - NewsAPI for general financial news\n",
    "   - Yahoo Finance RSS feeds for stock-specific news\n",
    "   - Preprocesses text to remove HTML and malicious content\n",
    "\n",
    "3. **Ingestion (Coordinator)**: Orchestrates parallel data fetching\n",
    "   - Executes financial and news ingestion concurrently using ThreadPoolExecutor\n",
    "   - Handles partial failures gracefully\n",
    "   - Combines results into a unified data bundle\n",
    "\n",
    "## Data Flow\n",
    "```\n",
    "User Request → Ingestion.execute(symbol)\n",
    "    ├─→ FinancialDataIngestion.execute(symbol, period) [Thread 1]\n",
    "    │   └─→ Returns: financial metrics, historical data, company info\n",
    "    └─→ NewsDataIngestion.execute(symbol, limit) [Thread 2]\n",
    "        └─→ Returns: articles from NewsAPI + Yahoo RSS\n",
    "                ↓\n",
    "    Combined Result: {financial_data, news_data, errors, status}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e301856-1e5b-44f9-9acb-8531eead0979",
   "metadata": {
    "id": "4e301856-1e5b-44f9-9acb-8531eead0979"
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# (step 1)\n",
    "# ingestion/financial_data.py (file 1 of 3)\n",
    "# (Yahoo Finance data fetching, metrics calculation, and error handling, data structuring.)\n",
    "# ---------------------------\n",
    "\n",
    "# ==============================================================================\n",
    "# FINANCIAL DATA INGESTION WORKER\n",
    "# ==============================================================================\n",
    "# Purpose: Fetch stock data from Yahoo Finance API (yfinance)\n",
    "# Responsibilities:\n",
    "#   - Retrieve historical OHLCV data (Open, High, Low, Close, Volume)\n",
    "#   - Fetch company fundamentals (P/E ratio, market cap, sector, industry)\n",
    "#   - Calculate derived metrics (volatility, price changes, volume averages)\n",
    "#   - Handle API errors and return structured data\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"\n",
    "NLP-5: Financial Data Ingestion\n",
    "Fetch stock data from Yahoo Finance API (yfinance)\n",
    "\"\"\"\n",
    "\n",
    "class FinancialDataIngestion(BaseWorker):\n",
    "    \"\"\"\n",
    "    Fetches financial data from Yahoo Finance.\n",
    "\n",
    "    Responsibilities:\n",
    "    - Fetch historical OHLCV (Open, High, Low, Close, Volume) data\n",
    "    - Fetch company information and fundamentals\n",
    "    - Calculate basic metrics from raw data\n",
    "    - Handle API errors gracefully\n",
    "    - Return structured financial data\n",
    "    \"\"\"\n",
    "\n",
    "    def execute(self, *inputs) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Fetch financial data for a given stock symbol.\n",
    "\n",
    "        Args:\n",
    "            inputs[0] (str): Stock ticker symbol (e.g., \"AAPL\")\n",
    "            inputs[1] (str, optional): Time period (default: \"1mo\")\n",
    "                Valid periods: 1d, 5d, 1mo, 3mo, 6mo, 1y, 2y, 5y, 10y, ytd, max\n",
    "\n",
    "        Returns:\n",
    "            dict: Financial data bundle containing:\n",
    "                - symbol: Stock ticker\n",
    "                - price metrics: Current price, changes, highs/lows\n",
    "                - volume metrics: Current and average volume\n",
    "                - volatility: Calculated volatility\n",
    "                - fundamentals: P/E ratio, market cap, beta, etc.\n",
    "                - company info: Sector, industry, summary\n",
    "                - historical_data: Recent OHLCV records\n",
    "                - status: Success or error status\n",
    "        \"\"\"\n",
    "        try:\n",
    "            symbol = inputs[0]\n",
    "            period = inputs[1] if len(inputs) > 1 else \"1mo\"\n",
    "\n",
    "            stock = yf.Ticker(symbol)\n",
    "\n",
    "            # Fetch historical OHLCV data from Yahoo Finance\n",
    "            hist = stock.history(period=period)\n",
    "\n",
    "            if hist.empty:\n",
    "                return {\n",
    "                    \"source\": \"yahoo_finance\",\n",
    "                    \"symbol\": symbol,\n",
    "                    \"data\": None,\n",
    "                    \"status\": \"error\",\n",
    "                    \"error\": f\"No data found for symbol {symbol}\",\n",
    "                    \"timestamp\": datetime.now().isoformat()\n",
    "                }\n",
    "\n",
    "            info = stock.info\n",
    "\n",
    "            # Extract price metrics from historical data\n",
    "            current_price = hist['Close'].iloc[-1]\n",
    "            prev_close = info.get('previousClose', hist['Close'].iloc[-2] if len(hist) > 1 else current_price)\n",
    "            price_change = current_price - prev_close\n",
    "            price_change_pct = (price_change / prev_close) * 100 if prev_close != 0 else 0\n",
    "\n",
    "            # Calculate annualized volatility using standard deviation of returns\n",
    "            # Formula: std(daily_returns) * sqrt(trading_days_per_year)\n",
    "            returns = hist['Close'].pct_change().dropna()\n",
    "            volatility = returns.tail(30).std() * (252 ** 0.5) if len(returns) > 0 else 0\n",
    "\n",
    "            # Calculate volume statistics\n",
    "            avg_volume = hist['Volume'].tail(30).mean()\n",
    "            current_volume = hist['Volume'].iloc[-1]\n",
    "\n",
    "            # Raw data result from yf\n",
    "            result = {\n",
    "                \"source\": \"yahoo_finance\",\n",
    "                \"symbol\": symbol,\n",
    "                \"data\": {\n",
    "                    \"current_price\": float(current_price),\n",
    "                    \"price_change\": float(price_change),\n",
    "                    \"price_change_pct\": float(price_change_pct),\n",
    "                    \"volume\": int(current_volume),\n",
    "                    \"avg_volume_30d\": float(avg_volume),\n",
    "                    \"volatility_30d\": float(volatility),\n",
    "                    \"market_cap\": info.get(\"marketCap\"),\n",
    "                    \"pe_ratio\": info.get(\"forwardPE\"),\n",
    "                    \"dividend_yield\": info.get(\"dividendYield\"),\n",
    "                    \"52_week_high\": info.get(\"fiftyTwoWeekHigh\"),\n",
    "                    \"52_week_low\": info.get(\"fiftyTwoWeekLow\"),\n",
    "                    \"beta\": info.get(\"beta\"),\n",
    "                    \"sector\": info.get(\"sector\"),\n",
    "                    \"industry\": info.get(\"industry\"),\n",
    "                    \"company_summary\": info.get(\"longBusinessSummary\", \"\")[:500],\n",
    "                    \"historical_data\": hist.tail(30).to_dict('records')\n",
    "                },\n",
    "                \"status\": \"success\",\n",
    "                \"error\": None,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"source\": \"yahoo_finance\",\n",
    "                \"symbol\": inputs[0] if len(inputs) > 0 else \"UNKNOWN\",\n",
    "                \"data\": None,\n",
    "                \"status\": \"error\",\n",
    "                \"error\": str(e),\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "# ---------------------------\n",
    "# (step 1)\n",
    "# ingestion/news_data.py (file 2 of 3)\n",
    "# ---------------------------\n",
    "\n",
    "# ==============================================================================\n",
    "# NEWS DATA INGESTION WORKER\n",
    "# ==============================================================================\n",
    "# Purpose: Fetch news articles from multiple sources\n",
    "# Responsibilities:\n",
    "#   - Query NewsAPI for general financial news\n",
    "#   - Fetch Yahoo Finance RSS feeds for stock-specific news\n",
    "#   - Preprocess text to remove HTML tags and malicious content\n",
    "#   - Return structured list of articles with metadata\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"\n",
    "NLP-6: News Data Ingestion\n",
    "Fetch news articles from NewsAPI and Yahoo Finance RSS\n",
    "\"\"\"\n",
    "\n",
    "class NewsDataIngestion(BaseWorker):\n",
    "    \"\"\"\n",
    "    Fetches news data from multiple sources.\n",
    "\n",
    "    Responsibilities:\n",
    "    - Fetch news articles from NewsAPI\n",
    "    - Fetch news from Yahoo Finance RSS feeds\n",
    "    - Handle API errors and rate limits gracefully\n",
    "    - Return structured list of articles\n",
    "    \"\"\"\n",
    "\n",
    "    DEFAULT_LIMIT = 10\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def execute(self, *inputs) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Fetch news articles for a given stock symbol.\n",
    "\n",
    "        Args:\n",
    "            inputs[0] (str): Stock ticker symbol (e.g., \"AAPL\")\n",
    "            inputs[1] (int, optional): Number of articles to fetch (default: 10)\n",
    "\n",
    "        Returns:\n",
    "            dict: News data bundle containing:\n",
    "                - articles: List of articles from all sources\n",
    "                - sources_queried: Which sources were successfully queried\n",
    "                - total_count: Total number of articles fetched\n",
    "                - status: Success or error status\n",
    "        \"\"\"\n",
    "        try:\n",
    "            symbol = inputs[0]\n",
    "            limit = inputs[1] if len(inputs) > 1 else self.DEFAULT_LIMIT\n",
    "\n",
    "            articles = []\n",
    "            sources_queried = []\n",
    "            errors = []\n",
    "\n",
    "            # Fetch from Yahoo Finance RSS feed\n",
    "            # Allocate half of article limit to this source\n",
    "            try:\n",
    "                yahoo_articles = self._fetch_from_yahoo_rss(symbol, limit // 2)\n",
    "                articles.extend(yahoo_articles)\n",
    "                sources_queried.append(\"yahoo_rss\")\n",
    "            except Exception as e:\n",
    "                errors.append({\"source\": \"yahoo_rss\", \"error\": str(e)})\n",
    "\n",
    "            # Fetch from NewsAPI\n",
    "            # Allocate remaining half of article limit to this source\n",
    "            if NEWS_API_KEY:\n",
    "                try:\n",
    "                    newsapi_articles = self._fetch_from_newsapi(symbol, limit // 2)\n",
    "                    articles.extend(newsapi_articles)\n",
    "                    sources_queried.append(\"newsapi\")\n",
    "                except Exception as e:\n",
    "                    errors.append({\"source\": \"newsapi\", \"error\": str(e)})\n",
    "            else:\n",
    "                errors.append({\"source\": \"newsapi\", \"error\": \"NEWS_API_KEY not found in environment\"})\n",
    "\n",
    "            return {\n",
    "                \"source\": \"news_aggregated\",\n",
    "                \"symbol\": symbol,\n",
    "                \"data\": {\n",
    "                    \"articles\": articles,\n",
    "                    \"sources_queried\": sources_queried,\n",
    "                    \"total_count\": len(articles)\n",
    "                },\n",
    "                \"status\": \"success\" if len(articles) > 0 else \"partial_success\",\n",
    "                \"error\": errors if len(errors) > 0 else None,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"source\": \"news_aggregated\",\n",
    "                \"symbol\": inputs[0] if len(inputs) > 0 else \"UNKNOWN\",\n",
    "                \"data\": {\n",
    "                    \"articles\": [],\n",
    "                    \"sources_queried\": [],\n",
    "                    \"total_count\": 0\n",
    "                },\n",
    "                \"status\": \"error\",\n",
    "                \"error\": str(e),\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "\n",
    "    def _preprocess_text(self, text: str) -> str:\n",
    "        # Text preprocessing: Remove potentially malicious content and HTML\n",
    "        # Step 1: Remove script tags and their content\n",
    "        text = re.sub(r'(?is)<script.*?>.*?</script>', ' ', text)\n",
    "\n",
    "        # Remove any remaining HTML tags\n",
    "        text = re.sub(r'<[^>]+>', ' ', text)\n",
    "\n",
    "        # Remove javascript: URIs and inline event handlers like onload=, onclick= etc.\n",
    "        text = re.sub(r'(?i)javascript\\s*:', '', text)\n",
    "        text = re.sub(r'(?i)on\\w+\\s*=\\s*[\"\\'].*?[\"\\']', ' ', text)\n",
    "\n",
    "        # Remove control characters\n",
    "        text = re.sub(r'[\\x00-\\x1f\\x7f]', ' ', text)\n",
    "\n",
    "        # Unescape HTML entities then escape to ensure safe plain text\n",
    "        text = html.unescape(text)\n",
    "        text = html.escape(text)\n",
    "\n",
    "        # Collapse multiple whitespace to single space and trim\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "        return text\n",
    "\n",
    "    def _fetch_from_newsapi(self, symbol: str, limit: int) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Fetch articles from NewsAPI.\n",
    "\n",
    "        Args:\n",
    "            symbol: Stock ticker symbol\n",
    "            limit: Maximum number of articles\n",
    "\n",
    "        Returns:\n",
    "            List of article dictionaries\n",
    "        \"\"\"\n",
    "        url = \"https://newsapi.org/v2/everything\"\n",
    "        params = {\n",
    "            \"q\": f\"{symbol} stock\",\n",
    "            \"language\": \"en\",\n",
    "            \"sortBy\": \"publishedAt\",\n",
    "            \"pageSize\": limit,\n",
    "            \"apiKey\": NEWS_API_KEY\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            articles = []\n",
    "            for article in data.get(\"articles\", []):\n",
    "                processed_summary = self._preprocess_text(article.get(\"description\", \"\"))\n",
    "                articles.append({\n",
    "                    \"title\": article.get(\"title\", \"\"),\n",
    "                    \"link\": article.get(\"url\", \"\"),\n",
    "                    \"published\": article.get(\"publishedAt\", \"\"),\n",
    "                    \"summary\": processed_summary,\n",
    "                    \"source\": article.get(\"source\", {}).get(\"name\", \"NewsAPI\")\n",
    "                })\n",
    "            return articles\n",
    "        else:\n",
    "            raise Exception(f\"NewsAPI request failed with status {response.status_code}\")\n",
    "\n",
    "    def _fetch_from_yahoo_rss(self, symbol: str, limit: int) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Fetch articles from Yahoo Finance RSS feed.\n",
    "\n",
    "        Args:\n",
    "            symbol: Stock ticker symbol\n",
    "            limit: Maximum number of articles\n",
    "\n",
    "        Returns:\n",
    "            List of article dictionaries\n",
    "        \"\"\"\n",
    "        yahoo_rss = f\"https://feeds.finance.yahoo.com/rss/2.0/headline?s={symbol}&region=US&lang=en-US\"\n",
    "        feed = feedparser.parse(yahoo_rss)\n",
    "\n",
    "        articles = []\n",
    "        for entry in feed.entries[:limit]:\n",
    "            processed_summary = self._preprocess_text(entry.get(\"summary\", \"\"))\n",
    "            articles.append({\n",
    "                \"title\": entry.get(\"title\", \"\"),\n",
    "                \"link\": entry.get(\"link\", \"\"),\n",
    "                \"published\": entry.get(\"published\", \"\"),\n",
    "                \"summary\": processed_summary,\n",
    "                \"source\": \"Yahoo Finance\"\n",
    "            })\n",
    "\n",
    "        return articles\n",
    "\n",
    "# ---------------------------\n",
    "# (step 1)\n",
    "# ingestion/ingestion.py (file 3 of 3)\n",
    "# ---------------------------\n",
    "# ==============================================================================\n",
    "# INGESTION COORDINATOR\n",
    "# ==============================================================================\n",
    "# Purpose: Orchestrate parallel data fetching from multiple sources\n",
    "# Pattern: Uses ThreadPoolExecutor for concurrent API calls\n",
    "# Responsibilities:\n",
    "#   - Execute financial and news ingestion in parallel\n",
    "#   - Combine results into unified data bundle\n",
    "#   - Handle partial failures gracefully\n",
    "#   - Track errors from each source\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"\n",
    "Main Ingestion Coordinator\n",
    "Orchestrates parallel data fetching from financial and news sources (simplified)\n",
    "\"\"\"\n",
    "\n",
    "class Ingestion(BaseWorker):\n",
    "    \"\"\"\n",
    "    Main Ingestion Coordinator that fetches data from financial and news sources in parallel.\n",
    "\n",
    "    Responsibilities:\n",
    "    - Coordinate parallel data fetching from financial and news sources\n",
    "    - Combine results into single bundle\n",
    "    - Handle partial failures gracefully\n",
    "    - Track errors from each source\n",
    "    - Return complete data bundle\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize data ingestors.\"\"\"\n",
    "        self.financial_ingestor = FinancialDataIngestion()\n",
    "        self.news_ingestor = NewsDataIngestion()\n",
    "\n",
    "    def execute(self, *inputs) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Execute parallel data ingestion from all sources.\n",
    "\n",
    "        Args:\n",
    "            inputs[0] (str): Stock ticker symbol (e.g., \"AAPL\")\n",
    "            inputs[1] (str, optional): Time period for historical data (default: \"1mo\")\n",
    "            inputs[2] (int, optional): Number of news articles (default: 10)\n",
    "\n",
    "        Returns:\n",
    "            dict: Complete data bundle with financial and news data\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract parameters\n",
    "            symbol = inputs[0] if len(inputs) > 0 else \"AAPL\"\n",
    "            period = inputs[1] if len(inputs) > 1 else \"1mo\"\n",
    "            news_limit = inputs[2] if len(inputs) > 2 else 10\n",
    "\n",
    "            # Execute both financial and news ingestion concurrently\n",
    "            results = self._execute_parallel(symbol, period, news_limit)\n",
    "\n",
    "            # Combine results\n",
    "            bundle = self._combine_results(symbol, results)\n",
    "\n",
    "            return bundle\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"symbol\": inputs[0] if len(inputs) > 0 else \"UNKNOWN\",\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"financial_data\": None,\n",
    "                \"news_data\": None,\n",
    "                \"errors\": [{\"source\": \"ingestion_coordinator\", \"error\": str(e)}],\n",
    "                \"status\": \"error\"\n",
    "            }\n",
    "\n",
    "    def _execute_parallel(self, symbol: str, period: str, news_limit: int) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Execute all ingestion tasks in parallel.\n",
    "\n",
    "        Args:\n",
    "            symbol: Stock ticker symbol\n",
    "            period: Time period for historical data\n",
    "            news_limit: Number of news articles\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with results from all sources\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            \"financial\": None,\n",
    "            \"news\": None\n",
    "        }\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "            # Submit both ingestion tasks to thread pool for parallel execution\n",
    "            future_to_source = {\n",
    "                executor.submit(self.financial_ingestor.execute, symbol, period): \"financial\",\n",
    "                executor.submit(self.news_ingestor.execute, symbol, news_limit): \"news\"\n",
    "            }\n",
    "\n",
    "            # Collect results as they complete\n",
    "            for future in as_completed(future_to_source):\n",
    "                source = future_to_source[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    results[source] = result\n",
    "                except Exception as e:\n",
    "                    results[source] = {\n",
    "                        \"source\": source,\n",
    "                        \"data\": None,\n",
    "                        \"status\": \"error\",\n",
    "                        \"error\": str(e),\n",
    "                        \"timestamp\": datetime.now().isoformat()\n",
    "                    }\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _combine_results(self, symbol: str, results: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Combine results from all ingestors into single bundle.\n",
    "\n",
    "        Args:\n",
    "            symbol: Stock ticker symbol\n",
    "            results: Dictionary with results from each ingestor\n",
    "\n",
    "        Returns:\n",
    "            Combined data bundle\n",
    "        \"\"\"\n",
    "        errors = []\n",
    "\n",
    "        # Extract financial data\n",
    "        financial_result = results.get(\"financial\", {})\n",
    "        financial_data = financial_result.get(\"data\") if financial_result.get(\"status\") == \"success\" else None\n",
    "        if financial_result.get(\"error\"):\n",
    "            errors.append({\"source\": \"financial\", \"error\": financial_result.get(\"error\")})\n",
    "\n",
    "        # Extract news data\n",
    "        news_result = results.get(\"news\", {})\n",
    "        news_data = news_result.get(\"data\") if news_result.get(\"status\") in [\"success\", \"partial_success\"] else None\n",
    "        if news_result.get(\"error\"):\n",
    "            errors.append({\"source\": \"news\", \"error\": news_result.get(\"error\")})\n",
    "\n",
    "        # Determine overall status\n",
    "        status = self._determine_status(results)\n",
    "\n",
    "        return {\n",
    "            \"symbol\": symbol,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"financial_data\": financial_data,\n",
    "            \"news_data\": news_data,\n",
    "            \"errors\": errors,\n",
    "            \"status\": status\n",
    "        }\n",
    "\n",
    "    def _determine_status(self, results: Dict[str, Any]) -> str:\n",
    "        \"\"\"\n",
    "        Determine overall ingestion status based on results.\n",
    "\n",
    "        Args:\n",
    "            results: Dictionary with results from each ingestor\n",
    "\n",
    "        Returns:\n",
    "            Status string: \"success\", \"partial_success\", or \"error\"\n",
    "        \"\"\"\n",
    "        success_count = 0\n",
    "        total_count = len(results)\n",
    "\n",
    "        for source, result in results.items():\n",
    "            if result and result.get(\"status\") in [\"success\", \"partial_success\"]:\n",
    "                success_count += 1\n",
    "\n",
    "        if success_count == total_count:\n",
    "            return \"success\"\n",
    "        elif success_count > 0:\n",
    "            return \"partial_success\"\n",
    "        else:\n",
    "            return \"error\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c32775-fd8c-469b-ba12-901703ea82c7",
   "metadata": {
    "id": "17c32775-fd8c-469b-ba12-901703ea82c7"
   },
   "source": [
    "# 3. Summarizer\n",
    "- Formats Ingestion data into structured insight with routed notes\n",
    "- Summary is stored as summary_result[\"summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4282c88-d6bd-4620-b8fe-fe25bb90f3e0",
   "metadata": {
    "id": "b4282c88-d6bd-4620-b8fe-fe25bb90f3e0"
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# (step 2)\n",
    "# src/summarizer/summarizer.py\n",
    "# ---------------------------\n",
    "\n",
    "# --- Simple keyword routing for headlines ---\n",
    "ROUTES = {\n",
    "    \"earnings\": [\"eps\", \"guidance\", \"revenue\", \"call\", \"forecast\", \"beat\", \"miss\", \"margin\"],\n",
    "    \"macro\":    [\"fed\", \"rate\", \"cpi\", \"inflation\", \"jobs\", \"gdp\", \"unemployment\", \"yields\", \"oil\"],\n",
    "    \"company\":  [\"product\", \"launch\", \"recall\", \"supply\", \"lawsuit\", \"merger\", \"partnership\", \"contract\"],\n",
    "}\n",
    "\n",
    "def _route(text: str) -> str:\n",
    "    t = (text or \"\").lower()\n",
    "    for route, keys in ROUTES.items():\n",
    "        if any(k in t for k in keys):\n",
    "            return route\n",
    "    return \"company\"\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"You are a pragmatic equity analyst.\n",
    "Goal: {goal}\n",
    "Symbol: {symbol}\n",
    "\n",
    "Context (recent daily stats + sampled headlines):\n",
    "{context}\n",
    "\n",
    "Write 5–8 concise bullets on likely near-term price *drivers* and 2 bullets on *key risks*.\n",
    "Avoid hype; be specific. Include dates or sources inline when present.\n",
    "\"\"\"\n",
    "\n",
    "class SummarizerWorker(BaseWorker):\n",
    "    def __init__(self, name: str = \"summarizer\", role: str = \"news_summary\", model: str | None = None):\n",
    "        \"\"\"\n",
    "        Defensive init:\n",
    "        - Try BaseWorker's init; if different signature, just set attributes.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            super().__init__(name=name, role=role, model=model)  # type: ignore[misc]\n",
    "        except TypeError:\n",
    "            try:\n",
    "                super().__init__()  # type: ignore[misc]\n",
    "            except Exception:\n",
    "                pass\n",
    "            setattr(self, \"name\", name)\n",
    "            setattr(self, \"role\", role)\n",
    "            setattr(self, \"model\", model)\n",
    "\n",
    "    def execute(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Entry point for the summarizer agent.\n",
    "        \"\"\"\n",
    "        symbol: str = inputs[\"symbol\"]\n",
    "        news_daily = inputs.get(\"news_daily\")\n",
    "        raw_news   = inputs.get(\"raw_news\")\n",
    "        window     = int(inputs.get(\"window\", 7))\n",
    "        goal       = inputs.get(\"analysis_goal\", f\"Next-week price drivers for {symbol}\")\n",
    "\n",
    "        context = self._format_context(news_daily, raw_news, window)\n",
    "        routed  = self._route_headlines(raw_news)\n",
    "        prompt  = PROMPT_TEMPLATE.format(goal=goal, symbol=symbol, context=context)\n",
    "\n",
    "        # Stub—replace with your LLM call\n",
    "        summary_text = (\n",
    "            \"(Stubbed summary — replace with your LLM call)\\n\"\n",
    "            + prompt\n",
    "            + \"\\n- Headlines cluster around a few catalysts; monitor official updates.\\n\"\n",
    "              \"- Tone is slightly positive; momentum sensitive to macro prints.\\n\"\n",
    "              \"- Risks: guidance/margin pressure; policy surprises.\"\n",
    "        )\n",
    "\n",
    "        memory_writes = [\n",
    "            f\"[{symbol}] {window}d summary\",\n",
    "            f\"[{symbol}] Routes: \" + \", \".join([k for k, v in routed.items() if v])\n",
    "        ]\n",
    "\n",
    "        return {\n",
    "            \"symbol\": symbol,\n",
    "            \"summary\": summary_text,\n",
    "            \"routed_notes\": routed,\n",
    "            \"artifacts\": {\"prompt\": prompt},\n",
    "            \"memory_writes\": memory_writes,\n",
    "        }\n",
    "\n",
    "    # ----------------- Helpers -----------------\n",
    "    def _format_context(\n",
    "        self,\n",
    "        news_daily,\n",
    "        raw_news,  # Union[List[dict], \"pd.DataFrame\", None] at runtime\n",
    "        window: int\n",
    "    ) -> str:\n",
    "        parts: List[str] = []\n",
    "\n",
    "        # Daily aggregates\n",
    "        if news_daily is not None and hasattr(news_daily, \"tail\") and len(news_daily) > 0:\n",
    "            tail = news_daily.tail(window)\n",
    "            parts.append(\"Daily sentiment (most recent first):\")\n",
    "            for idx, row in tail.iloc[::-1].iterrows():\n",
    "                parts.append(\n",
    "                    f\"- {idx}: count={int(row.get('news_count', 0))}, \"\n",
    "                    f\"sent_mean={row.get('sent_mean', 0):+.3f}, decay={row.get('sent_decay', 0):+.3f}\"\n",
    "                )\n",
    "\n",
    "        # Recent headlines\n",
    "        if raw_news is not None:\n",
    "            try:\n",
    "                import pandas as pd\n",
    "                df = raw_news if isinstance(raw_news, pd.DataFrame) else pd.DataFrame(raw_news)\n",
    "                cols = [c for c in [\"date\", \"source\", \"title\"] if c in df.columns]\n",
    "                if cols:\n",
    "                    parts.append(\"\\nRecent headlines:\")\n",
    "                    for _, r in df.tail(min(12, len(df))).iloc[::-1].iterrows():\n",
    "                        parts.append(\"- \" + \" | \".join(str(r.get(c, \"\")) for c in cols))\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        return \"\\n\".join(parts) if parts else \"(No news context available)\"\n",
    "\n",
    "    def _route_headlines(self, raw_news) -> dict:\n",
    "        routed = {\"earnings\": [], \"macro\": [], \"company\": []}\n",
    "        if raw_news is None:\n",
    "            return routed\n",
    "        try:\n",
    "            import pandas as pd\n",
    "            df = raw_news if isinstance(raw_news, pd.DataFrame) else pd.DataFrame(raw_news)\n",
    "            for _, r in df.tail(50).iterrows():\n",
    "                ttl = str(r.get(\"title\", \"\")) or \"\"\n",
    "                routed[_route(ttl)].append(ttl)\n",
    "        except Exception:\n",
    "            pass\n",
    "        # Trim to a few examples per bucket\n",
    "        return {k: v[:5] for k, v in routed.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00b3074-3a35-4e1a-bc98-1ac816c3f7a7",
   "metadata": {
    "id": "b00b3074-3a35-4e1a-bc98-1ac816c3f7a7"
   },
   "source": [
    "# 4. Memory Management\n",
    "\n",
    "## Overview\n",
    "The Memory module provides persistent storage for agent learnings across multiple runs. It enables the system to:\n",
    "- Store analysis results and insights\n",
    "- Retrieve historical context for subsequent queries\n",
    "- Build knowledge over time for improved decision-making\n",
    "\n",
    "## Key Features\n",
    "\n",
    "### Storage\n",
    "- **File-based persistence**: Stores memories in `data/agent_memory.json`\n",
    "- **Tagged entries**: Each memory includes tags (e.g., symbol, category) for efficient retrieval\n",
    "- **Timestamped records**: All entries include ISO-formatted timestamps for freshness tracking\n",
    "\n",
    "### Operations\n",
    "1. **add**: Store new memory entries with text and optional tags\n",
    "2. **search**: Find memories by keyword matching in text or tags\n",
    "3. **get_recent**: Retrieve the N most recent memories\n",
    "4. **retrieve_by_symbol**: Get all memories related to a specific stock symbol\n",
    "\n",
    "## Memory Structure\n",
    "```python\n",
    "{\n",
    "  \"timestamp\": \"2025-10-19T10:30:00\",\n",
    "  \"text\": \"[AAPL] Investment recommendation: Hold\",\n",
    "  \"tags\": [\"AAPL\", \"summary\", \"optimized\"]\n",
    "}\n",
    "```\n",
    "\n",
    "## Usage in Workflow\n",
    "- **After Summarization**: Store summary notes and routing information\n",
    "- **After Optimization**: Store final optimized summaries\n",
    "- **Before New Analysis**: Retrieve previous context to avoid redundant API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a047ab3e-7e66-4fda-ab0b-090118714bbc",
   "metadata": {
    "id": "a047ab3e-7e66-4fda-ab0b-090118714bbc"
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# (step 3)\n",
    "# src/memory/memory.py\n",
    "# ---------------------------\n",
    "\n",
    "# ==============================================================================\n",
    "# MEMORY MANAGEMENT MODULE\n",
    "# ==============================================================================\n",
    "# Purpose: Provide persistent storage for agent learnings across runs\n",
    "# Storage: JSON file-based persistence in ./data/agent_memory.json\n",
    "# Structure: List of dictionaries with timestamp, text, and tags\n",
    "# ==============================================================================\n",
    "\n",
    "# Global in-memory store synchronized with JSON file\n",
    "_memory_store: List[Dict[str, Any]] = []\n",
    "MEMORY_FILE = \"./data/agent_memory.json\"\n",
    "\n",
    "def _load_memory():\n",
    "    \"\"\"\n",
    "    Load memories from JSON file into the in-memory list.\n",
    "    Called automatically when module is imported.\n",
    "    Handles missing files and JSON decode errors gracefully.\n",
    "    \"\"\"\n",
    "    global _memory_store\n",
    "    if not os.path.exists(MEMORY_FILE):\n",
    "        _memory_store = []\n",
    "        return\n",
    "    try:\n",
    "        with open(MEMORY_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            _memory_store = json.load(f)\n",
    "    except (json.JSONDecodeError, FileNotFoundError):\n",
    "        _memory_store = []\n",
    "\n",
    "def _save_memory():\n",
    "    \"\"\"\n",
    "    Save the in-memory list to JSON file for persistence.\n",
    "    Creates directory if it doesn't exist.\n",
    "    Called after each add operation.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(MEMORY_FILE), exist_ok=True)\n",
    "    with open(MEMORY_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(_memory_store, f, indent=2)\n",
    "\n",
    "# Load memory when module is imported\n",
    "_load_memory()\n",
    "\n",
    "class MemoryWorker(BaseWorker):\n",
    "    \"\"\"\n",
    "    Worker for managing agent memories with persistent storage.\n",
    "\n",
    "    A BaseWorker implementation for managing agent memories. It stores short text\n",
    "    memories with optional tags in a JSON file so the agent can learn across runs.\n",
    "    \"\"\"\n",
    "    def execute(self, *inputs) -> Any:\n",
    "        \"\"\"\n",
    "        Manages agent memories. The first input is the operation ('add', 'search', 'get_recent', 'retrieve_by_symbol').\n",
    "\n",
    "        Usage:\n",
    "            - execute('add', 'some memory text', ['tag1', 'tag2'])\n",
    "            - execute('search', 'query text')\n",
    "            - execute('get_recent', 5)\n",
    "            - execute('retrieve_by_symbol', 'AAPL')\n",
    "        \"\"\"\n",
    "        if not inputs:\n",
    "            raise ValueError(\"MemoryWorker requires at least one input for the operation.\")\n",
    "\n",
    "        operation = inputs[0]\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # ADD OPERATION: Store new memory entry\n",
    "        # ---------------------------------------------------------------------\n",
    "        if operation == 'add':\n",
    "            if len(inputs) < 2:\n",
    "                raise ValueError(\"The 'add' operation requires text for the memory.\")\n",
    "            text = inputs[1]\n",
    "            tags = inputs[2] if len(inputs) > 2 else []\n",
    "            # Create memory entry with ISO-formatted UTC timestamp\n",
    "            entry = {\n",
    "                \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "                \"text\": text,\n",
    "                \"tags\": tags,\n",
    "            }\n",
    "            _memory_store.append(entry)\n",
    "            _save_memory()\n",
    "            return entry\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # SEARCH OPERATION: Find memories by keyword matching\n",
    "        # ---------------------------------------------------------------------\n",
    "        elif operation == 'search':\n",
    "            if len(inputs) < 2:\n",
    "                raise ValueError(\"The 'search' operation requires a query string.\")\n",
    "            query = inputs[1].lower()\n",
    "            top_k = inputs[2] if len(inputs) > 2 else 5\n",
    "\n",
    "            # Search in reverse chronological order (most recent first)\n",
    "            # Match query against text content or tags (case-insensitive)\n",
    "            matches = [\n",
    "                m for m in reversed(_memory_store)\n",
    "                if query in m['text'].lower() or any(query in t.lower() for t in m.get('tags', []))\n",
    "            ]\n",
    "            return matches[:top_k]\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # GET_RECENT OPERATION: Retrieve N most recent memories\n",
    "        # ---------------------------------------------------------------------\n",
    "        elif operation == 'get_recent':\n",
    "            n = inputs[1] if len(inputs) > 1 else 5\n",
    "            return list(reversed(_memory_store))[:n]\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # RETRIEVE_BY_SYMBOL OPERATION: Get all memories for a stock symbol\n",
    "        # ---------------------------------------------------------------------\n",
    "        elif operation == 'retrieve_by_symbol':\n",
    "            if len(inputs) < 2:\n",
    "                raise ValueError(\"The 'retrieve_by_symbol' operation requires a symbol string.\")\n",
    "            symbol = inputs[1].upper()  # Normalize to uppercase for consistency\n",
    "\n",
    "            # Filter records by symbol (case-insensitive)\n",
    "            # Matches if symbol appears in tags or text content\n",
    "            matches = [\n",
    "                m for m in _memory_store\n",
    "                if symbol in [tag.upper() for tag in m.get('tags', [])] or symbol in m.get('text', '').upper()\n",
    "            ]\n",
    "            return matches\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown operation: {operation}. Available operations: 'add', 'search', 'get_recent', 'retrieve_by_symbol'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd32208-5143-4ade-a759-52d608c46854",
   "metadata": {
    "id": "ffd32208-5143-4ade-a759-52d608c46854"
   },
   "source": [
    "# 5. EvaluatorOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fccc6dbd-dcc8-47aa-9afa-351f05f2fea3",
   "metadata": {
    "id": "fccc6dbd-dcc8-47aa-9afa-351f05f2fea3"
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# (step 4)\n",
    "# evaluator_optimizer/init.py\n",
    "# ---------------------------\n",
    "\n",
    "\"\"\"\n",
    "Evaluator-Optimizer Module\n",
    "\n",
    "Implements the Evaluator-Optimizer workflow pattern for iterative refinement\n",
    "of investment research summaries.\n",
    "\"\"\"\n",
    "\n",
    "__all__ = [\"EvaluatorOptimizer\", \"Feedback\", \"State\"]\n",
    "\n",
    "# ---------------------------\n",
    "# (step 4)\n",
    "# evaluator_optimizer/evaluator_optimizer.py\n",
    "# ---------------------------\n",
    "\n",
    "\"\"\"\n",
    "Evaluator-Optimizer Module\n",
    "\n",
    "Implements the Evaluator-Optimizer workflow pattern where:\n",
    "1. Generator creates an investment research summary\n",
    "2. Evaluator assesses quality and provides feedback\n",
    "3. Loop continues until quality passes or max iterations reached\n",
    "4. Final summary is stored in memory\n",
    "\n",
    "Based on LangGraph pattern: https://langchain-ai.github.io/langgraph/tutorials/workflows/#evaluator-optimizer\n",
    "\"\"\"\n",
    "\n",
    "# --- State Definition ---\n",
    "class State(TypedDict):\n",
    "    \"\"\"Graph state for the Evaluator-Optimizer workflow\"\"\"\n",
    "    symbol: str  # Stock symbol being analyzed\n",
    "    instructions: str  # User's request/query about the stock\n",
    "    context: Dict[str, Any]  # Financial data context (news, prices, etc.)\n",
    "    summary: str  # Current investment research summary\n",
    "    feedback: str  # Feedback from evaluator\n",
    "    grade: str  # Quality grade: \"pass\" or \"fail\"\n",
    "    quality_score: float  # Numeric quality score (0-10)\n",
    "    issues: List[str]  # List of identified issues\n",
    "    iteration: int  # Current iteration number\n",
    "    max_iterations: int  # Maximum allowed iterations\n",
    "    history: List[Dict[str, Any]]  # History of iterations for tracking\n",
    "\n",
    "# --- Structured Output Schema for Evaluation ---\n",
    "class Feedback(BaseModel):\n",
    "    \"\"\"Structured evaluation feedback from the Evaluator\"\"\"\n",
    "\n",
    "    grade: Literal[\"pass\", \"fail\"] = Field(\n",
    "        description=\"Overall quality assessment: 'pass' if summary meets quality criteria, 'fail' otherwise\"\n",
    "    )\n",
    "    quality_score: float = Field(\n",
    "        description=\"Numeric quality score from 0-10, where 10 is excellent\",\n",
    "        ge=0.0,\n",
    "        le=10.0\n",
    "    )\n",
    "    feedback: str = Field(\n",
    "        description=\"Detailed, actionable feedback for improving the summary if grade is 'fail'\"\n",
    "    )\n",
    "    issues: List[str] = Field(\n",
    "        description=\"List of specific issues identified in the summary\",\n",
    "        default_factory=list\n",
    "    )\n",
    "\n",
    "# --- Evaluator-Optimizer Class ---\n",
    "class EvaluatorOptimizer(BaseWorker):\n",
    "    \"\"\"\n",
    "    Implements the Evaluator-Optimizer workflow pattern for investment research summaries.\n",
    "\n",
    "    The workflow:\n",
    "    1. Generator creates an initial summary from context data\n",
    "    2. Evaluator assesses quality against defined criteria\n",
    "    3. If quality fails, provides feedback and loops back to Generator\n",
    "    4. Continues until quality passes or max iterations reached\n",
    "    5. Returns final optimized summary\n",
    "    \"\"\"\n",
    "\n",
    "    # Quality criteria for investment research summaries\n",
    "    QUALITY_CRITERIA = \"\"\"\n",
    "    A high-quality investment research summary should:\n",
    "    1. COMPLETENESS: Cover key financial metrics, sentiment analysis, and risk factors\n",
    "    2. CLARITY: Be well-structured, concise, and easy to understand\n",
    "    3. ACTIONABILITY: Include a clear investment recommendation (buy/sell/hold) with rationale\n",
    "    4. EVIDENCE-BASED: Back claims with specific data from news and financial metrics\n",
    "    5. COHERENCE: Have logical flow without contradictions\n",
    "    6. RISK AWARENESS: Acknowledge both opportunities and risks\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, llm: Optional[ChatOpenAI] = None, max_iterations: int = 3):\n",
    "        \"\"\"\n",
    "        Initialize the Evaluator-Optimizer.\n",
    "\n",
    "        Args:\n",
    "            llm: Language model instance (if None, initializes OpenAI LLM from environment)\n",
    "            max_iterations: Maximum refinement iterations before stopping\n",
    "        \"\"\"\n",
    "        # Initialize LLM\n",
    "        if llm is not None:\n",
    "            self.llm = llm\n",
    "        else:\n",
    "            self.llm = self._initialize_openai_llm()\n",
    "\n",
    "        self.max_iterations = max_iterations\n",
    "\n",
    "        # Create structured output LLM for evaluation\n",
    "        self.evaluator_llm = self.llm.with_structured_output(Feedback)\n",
    "\n",
    "        # Build the workflow graph\n",
    "        self.workflow = self._build_workflow()\n",
    "\n",
    "    def _initialize_openai_llm(self) -> ChatOpenAI:\n",
    "        \"\"\"Initialize OpenAI LLM with API key from environment\"\"\"\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        llm = ChatOpenAI(\n",
    "            model=\"gpt-4o-mini\",  # Using gpt-4o-mini for cost efficiency\n",
    "            temperature=0.7,\n",
    "            api_key=api_key\n",
    "        )\n",
    "        return llm\n",
    "\n",
    "    def _build_workflow(self) -> StateGraph:\n",
    "        \"\"\"Builds the LangGraph workflow for Evaluator-Optimizer pattern\"\"\"\n",
    "\n",
    "        # Create the graph\n",
    "        builder = StateGraph(State)\n",
    "\n",
    "        # Add nodes\n",
    "        builder.add_node(\"generator\", self._generator_node)\n",
    "        builder.add_node(\"evaluator\", self._evaluator_node)\n",
    "\n",
    "        # Add edges\n",
    "        builder.add_edge(START, \"generator\")\n",
    "        builder.add_edge(\"generator\", \"evaluator\")\n",
    "\n",
    "        # Conditional edge: loop back to generator or end\n",
    "        builder.add_conditional_edges(\n",
    "            \"evaluator\",\n",
    "            self._should_continue,\n",
    "            {\n",
    "                \"continue\": \"generator\",  # Loop back with feedback\n",
    "                \"end\": END  # Quality passed or max iterations reached\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Compile the workflow\n",
    "        return builder.compile()\n",
    "\n",
    "    def _generator_node(self, state: State) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generator Node: Creates or refines the investment research summary.\n",
    "\n",
    "        On first iteration: creates initial summary from context\n",
    "        On subsequent iterations: refines summary based on evaluator feedback\n",
    "        \"\"\"\n",
    "        symbol = state[\"symbol\"]\n",
    "        context = state[\"context\"]\n",
    "        instructions = state.get(\"instructions\", \"\")\n",
    "        feedback = state.get(\"feedback\", \"\")\n",
    "        iteration = state.get(\"iteration\", 0)\n",
    "\n",
    "        # Format the context data\n",
    "        formatted_context = self._format_context(context)\n",
    "\n",
    "        # Build the prompt\n",
    "        if iteration == 0:\n",
    "            # Initial summary generation\n",
    "            prompt = f\"\"\"You are an expert financial analyst. Create a comprehensive investment research summary for {symbol}.\n",
    "\n",
    "USER REQUEST:\n",
    "{instructions}\n",
    "\n",
    "AVAILABLE DATA:\n",
    "{formatted_context}\n",
    "\n",
    "QUALITY REQUIREMENTS:\n",
    "{self.QUALITY_CRITERIA}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Directly address the user's request/questions in your analysis\n",
    "2. Use the provided financial data, news, and market context to support your analysis\n",
    "3. If the user asked specific questions, answer them explicitly\n",
    "4. If data is missing or unavailable, acknowledge it and work with what's available\n",
    "5. Provide a clear investment recommendation (buy/sell/hold) with detailed rationale\n",
    "6. Structure your response professionally with clear sections\n",
    "7. Back all claims with specific data points from the context\n",
    "\n",
    "Generate a well-structured investment research summary that meets all quality criteria and addresses the user's request.\"\"\"\n",
    "        else:\n",
    "            # Refinement based on feedback\n",
    "            current_summary = state[\"summary\"]\n",
    "            prompt = f\"\"\"You are refining an investment research summary for {symbol}.\n",
    "\n",
    "USER REQUEST:\n",
    "{instructions}\n",
    "\n",
    "CURRENT SUMMARY:\n",
    "{current_summary}\n",
    "\n",
    "EVALUATOR FEEDBACK:\n",
    "{feedback}\n",
    "\n",
    "ISSUES IDENTIFIED:\n",
    "{', '.join(state.get('issues', []))}\n",
    "\n",
    "AVAILABLE DATA:\n",
    "{formatted_context}\n",
    "\n",
    "QUALITY REQUIREMENTS:\n",
    "{self.QUALITY_CRITERIA}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Address all issues identified by the evaluator\n",
    "2. Ensure the user's original request is still fully addressed\n",
    "3. Improve clarity, completeness, and actionability\n",
    "4. Add missing data points or analysis where needed\n",
    "5. Maintain professional structure and tone\n",
    "\n",
    "Generate an improved version that addresses all feedback and meets quality standards.\"\"\"\n",
    "\n",
    "        # Generate summary\n",
    "        response = self.llm.invoke(prompt)\n",
    "        new_summary = response.content\n",
    "\n",
    "        # Update iteration count\n",
    "        new_iteration = iteration + 1\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"GENERATOR - Iteration {new_iteration}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"User Request: {instructions[:80]}...\")\n",
    "        print(f\"Summary generated ({len(new_summary)} characters)\")\n",
    "        if iteration > 0:\n",
    "            print(f\"Addressing feedback: {feedback[:100]}...\")\n",
    "\n",
    "        return {\n",
    "            \"summary\": new_summary,\n",
    "            \"iteration\": new_iteration\n",
    "        }\n",
    "\n",
    "    def _evaluator_node(self, state: State) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Evaluator Node: Assesses the quality of the summary and provides feedback.\n",
    "\n",
    "        Uses structured output to return:\n",
    "        - grade: \"pass\" or \"fail\"\n",
    "        - quality_score: 0-10\n",
    "        - feedback: actionable improvement suggestions\n",
    "        - issues: specific problems identified\n",
    "        \"\"\"\n",
    "        summary = state[\"summary\"]\n",
    "        symbol = state[\"symbol\"]\n",
    "        instructions = state.get(\"instructions\", \"\")\n",
    "        iteration = state[\"iteration\"]\n",
    "\n",
    "        # Evaluation prompt\n",
    "        prompt = f\"\"\"You are a senior financial analyst evaluating an investment research summary for {symbol}.\n",
    "\n",
    "USER'S ORIGINAL REQUEST:\n",
    "{instructions}\n",
    "\n",
    "SUMMARY TO EVALUATE:\n",
    "{summary}\n",
    "\n",
    "EVALUATION CRITERIA:\n",
    "{self.QUALITY_CRITERIA}\n",
    "\n",
    "ASSESSMENT REQUIREMENTS:\n",
    "1. Does the summary directly address the user's request/questions?\n",
    "2. Is the analysis backed by specific data points?\n",
    "3. Are all quality criteria met (completeness, clarity, actionability, evidence-based, coherence, risk awareness)?\n",
    "4. Is the investment recommendation clear and well-justified?\n",
    "5. Are there any contradictions or unsupported claims?\n",
    "6. Is the structure professional and easy to follow?\n",
    "\n",
    "Provide:\n",
    "- grade: \"pass\" if the summary meets professional standards and addresses the user's request, \"fail\" if significant improvements needed\n",
    "- quality_score: 0-10 (be generous with 7+ for good work, reserve 9+ for exceptional analysis)\n",
    "- feedback: Specific, actionable suggestions for improvement (if grade is \"fail\")\n",
    "- issues: List specific problems (missing data, unclear reasoning, unanswered questions, etc.)\n",
    "\n",
    "Be fair but thorough. A passing grade means the summary is publication-ready and fully addresses the user's needs.\"\"\"\n",
    "\n",
    "        # Get structured evaluation\n",
    "        evaluation = self.evaluator_llm.invoke(prompt)\n",
    "\n",
    "        # Track iteration history\n",
    "        history = state.get(\"history\", [])\n",
    "        history.append({\n",
    "            \"iteration\": iteration,\n",
    "            \"summary_length\": len(summary),\n",
    "            \"grade\": evaluation.grade,\n",
    "            \"quality_score\": evaluation.quality_score,\n",
    "            \"issues_count\": len(evaluation.issues)\n",
    "        })\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"EVALUATOR - Iteration {iteration}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Grade: {evaluation.grade.upper()}\")\n",
    "        print(f\"Quality Score: {evaluation.quality_score}/10\")\n",
    "        print(f\"Issues Found: {len(evaluation.issues)}\")\n",
    "        if evaluation.issues:\n",
    "            for i, issue in enumerate(evaluation.issues, 1):\n",
    "                print(f\"  {i}. {issue}\")\n",
    "        print(f\"Feedback: {evaluation.feedback[:150]}...\")\n",
    "\n",
    "        return {\n",
    "            \"grade\": evaluation.grade,\n",
    "            \"quality_score\": evaluation.quality_score,\n",
    "            \"feedback\": evaluation.feedback,\n",
    "            \"issues\": evaluation.issues,\n",
    "            \"history\": history\n",
    "        }\n",
    "\n",
    "    def _should_continue(self, state: State) -> Literal[\"continue\", \"end\"]:\n",
    "        \"\"\"\n",
    "        Conditional routing: decide whether to continue refinement or end.\n",
    "\n",
    "        Continue if:\n",
    "        - Grade is \"fail\" AND\n",
    "        - Haven't reached max iterations\n",
    "\n",
    "        End if:\n",
    "        - Grade is \"pass\" OR\n",
    "        - Max iterations reached\n",
    "        \"\"\"\n",
    "        grade = state[\"grade\"]\n",
    "        iteration = state[\"iteration\"]\n",
    "        max_iterations = state[\"max_iterations\"]\n",
    "\n",
    "        if grade == \"pass\":\n",
    "            print(f\"\\nQuality PASSED - Ending optimization\")\n",
    "            return \"end\"\n",
    "        elif iteration >= max_iterations:\n",
    "            print(f\"\\nMax iterations ({max_iterations}) reached - Ending optimization\")\n",
    "            return \"end\"\n",
    "        else:\n",
    "            print(f\"\\nQuality FAILED - Continuing to iteration {iteration + 1}\")\n",
    "            return \"continue\"\n",
    "\n",
    "    def _format_context(self, context: Dict[str, Any]) -> str:\n",
    "        \"\"\"Formats context data into a readable string for prompts\"\"\"\n",
    "        formatted = []\n",
    "\n",
    "        # Helper function to safely format numeric values\n",
    "        def safe_format(value, format_str=\".2f\", prefix=\"\", suffix=\"\"):\n",
    "            \"\"\"Safely format a value that might be None\"\"\"\n",
    "            if value is None:\n",
    "                return \"N/A\"\n",
    "            try:\n",
    "                return f\"{prefix}{value:{format_str}}{suffix}\"\n",
    "            except (ValueError, TypeError):\n",
    "                return \"N/A\"\n",
    "\n",
    "        # Format financial data with key metrics\n",
    "        if \"financial_data\" in context:\n",
    "            fin_data = context[\"financial_data\"]\n",
    "\n",
    "            # Handle memory-based data (different structure)\n",
    "            if fin_data and fin_data.get(\"source\") == \"memory\":\n",
    "                formatted.append(\"=== FINANCIAL METRICS (FROM MEMORY) ===\")\n",
    "                formatted.append(fin_data.get(\"summary\", \"No summary available\")[:1000])\n",
    "            else:\n",
    "                # Handle fresh ingestion data\n",
    "                formatted.append(\"=== FINANCIAL METRICS ===\")\n",
    "                formatted.append(f\"Symbol: {fin_data.get('symbol', 'N/A')}\")\n",
    "                formatted.append(f\"Current Price: {safe_format(fin_data.get('current_price'), '.2f', '$')}\")\n",
    "                formatted.append(f\"Price Change: {safe_format(fin_data.get('price_change'), '.2f', '$')} ({safe_format(fin_data.get('price_change_pct'), '.2f', '', '%')})\")\n",
    "                formatted.append(f\"Volume: {safe_format(fin_data.get('volume'), ',')}\")\n",
    "                formatted.append(f\"Market Cap: {safe_format(fin_data.get('market_cap'), ',', '$')}\")\n",
    "                formatted.append(f\"P/E Ratio: {safe_format(fin_data.get('pe_ratio'), '.2f')}\")\n",
    "                formatted.append(f\"Dividend Yield: {safe_format(fin_data.get('dividend_yield'), '.2f', '', '%')}\")\n",
    "                formatted.append(f\"52-Week Range: {safe_format(fin_data.get('52_week_low'), '.2f', '$')} - {safe_format(fin_data.get('52_week_high'), '.2f', '$')}\")\n",
    "                formatted.append(f\"Beta: {safe_format(fin_data.get('beta'), '.3f')}\")\n",
    "                formatted.append(f\"Sector: {fin_data.get('sector', 'N/A')}\")\n",
    "                formatted.append(f\"Industry: {fin_data.get('industry', 'N/A')}\")\n",
    "\n",
    "                # Add volatility if available\n",
    "                if 'volatility_30d' in fin_data and fin_data['volatility_30d'] is not None:\n",
    "                    formatted.append(f\"30-Day Volatility: {fin_data['volatility_30d']:.2%}\")\n",
    "\n",
    "                # Add company summary if available\n",
    "                if 'company_summary' in fin_data and fin_data['company_summary']:\n",
    "                    formatted.append(f\"\\nCompany Overview: {fin_data['company_summary'][:300]}...\")\n",
    "\n",
    "                # Add recent price trends from historical data\n",
    "                if 'historical_data' in fin_data and fin_data['historical_data']:\n",
    "                    hist = fin_data['historical_data']\n",
    "                    if len(hist) >= 5:\n",
    "                        formatted.append(\"\\nRecent Price Trend (Last 5 Days):\")\n",
    "                        for i, day in enumerate(hist[-5:], 1):\n",
    "                            close_price = safe_format(day.get('Close'), '.2f', '$')\n",
    "                            volume = safe_format(day.get('Volume'), ',')\n",
    "                            formatted.append(f\"  Day {i}: Close={close_price}, Volume={volume}\")\n",
    "\n",
    "        # Format news data\n",
    "        if \"news_data\" in context:\n",
    "            news = context[\"news_data\"]\n",
    "\n",
    "            # Handle memory-based news\n",
    "            if isinstance(news, dict) and news.get(\"source\") == \"memory\":\n",
    "                formatted.append(f\"\\n=== NEWS ANALYSIS (FROM MEMORY) ===\")\n",
    "                formatted.append(\"Using cached news data from previous analysis\")\n",
    "            elif isinstance(news, dict) and \"articles\" in news:\n",
    "                articles = news[\"articles\"]\n",
    "                total = news.get(\"total_count\", 0)\n",
    "                formatted.append(f\"\\n=== NEWS ANALYSIS ===\")\n",
    "                formatted.append(f\"Total Articles Found: {total}\")\n",
    "\n",
    "                if articles and len(articles) > 0:\n",
    "                    formatted.append(\"\\nRecent Headlines:\")\n",
    "                    for i, article in enumerate(articles[:5], 1):\n",
    "                        title = article.get('title', 'No title')\n",
    "                        date = article.get('publishedAt', article.get('published', article.get('date', 'N/A')))\n",
    "                        formatted.append(f\"  {i}. {title} ({date})\")\n",
    "                else:\n",
    "                    formatted.append(\"No recent news articles available.\")\n",
    "            else:\n",
    "                formatted.append(\"\\n=== NEWS ANALYSIS ===\")\n",
    "                formatted.append(\"News data format not recognized or unavailable.\")\n",
    "\n",
    "        # Format sentiment if available\n",
    "        if \"sentiment\" in context:\n",
    "            formatted.append(f\"\\n=== SENTIMENT ANALYSIS ===\")\n",
    "            formatted.append(f\"Overall Sentiment: {context['sentiment']}\")\n",
    "\n",
    "        # Add any errors encountered\n",
    "        if \"errors\" in context and context[\"errors\"]:\n",
    "            formatted.append(f\"\\n=== DATA COLLECTION NOTES ===\")\n",
    "            for error in context[\"errors\"]:\n",
    "                source = error.get(\"source\", \"unknown\")\n",
    "                error_detail = error.get(\"error\", error.get(\"note\", \"Unknown error\"))\n",
    "                formatted.append(f\"Note: {source} - {error_detail}\")\n",
    "\n",
    "        # Add timestamp\n",
    "        if \"timestamp\" in context:\n",
    "            formatted.append(f\"\\nData Retrieved: {context['timestamp']}\")\n",
    "\n",
    "        return \"\\n\".join(formatted) if formatted else \"No context data available\"\n",
    "\n",
    "    def execute(self, symbol: str, context: Dict[str, Any], instructions: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Execute the Evaluator-Optimizer workflow.\n",
    "\n",
    "        Args:\n",
    "            symbol: Stock symbol to analyze\n",
    "            context: Dictionary containing financial data, news, sentiment, etc.\n",
    "            instructions: User's request/query (e.g., \"Should I buy AAPL?\" or \"Analyze the potential for AAPL stock\")\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with:\n",
    "                - final_summary: The optimized summary\n",
    "                - quality_score: Final quality score\n",
    "                - iterations: Number of iterations performed\n",
    "                - history: Detailed iteration history\n",
    "                - passed: Whether quality criteria were met\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'#'*60}\")\n",
    "        print(f\"EVALUATOR-OPTIMIZER WORKFLOW\")\n",
    "        print(f\"Symbol: {symbol}\")\n",
    "        print(f\"User Request: {instructions}\")\n",
    "        print(f\"Max Iterations: {self.max_iterations}\")\n",
    "        print(f\"{'#'*60}\")\n",
    "\n",
    "        # Initialize state\n",
    "        initial_state = {\n",
    "            \"symbol\": symbol,\n",
    "            \"instructions\": instructions,\n",
    "            \"context\": context,\n",
    "            \"summary\": summary_result[\"summary\"], # Add initial summary from summarizer\n",
    "            \"feedback\": \"\",\n",
    "            \"grade\": \"\",\n",
    "            \"quality_score\": 0.0,\n",
    "            \"issues\": [],\n",
    "            \"iteration\": 0,\n",
    "            \"max_iterations\": self.max_iterations,\n",
    "            \"history\": []\n",
    "        }\n",
    "\n",
    "        # Run the workflow\n",
    "        final_state = self.workflow.invoke(initial_state)\n",
    "\n",
    "        # Prepare results\n",
    "        results = {\n",
    "            \"final_summary\": final_state[\"summary\"],\n",
    "            \"quality_score\": final_state[\"quality_score\"],\n",
    "            \"iterations\": final_state[\"iteration\"],\n",
    "            \"history\": final_state[\"history\"],\n",
    "            \"passed\": final_state[\"grade\"] == \"pass\",\n",
    "            \"final_grade\": final_state[\"grade\"],\n",
    "            \"issues\": final_state[\"issues\"]\n",
    "        }\n",
    "\n",
    "        print(f\"\\n{'#'*60}\")\n",
    "        print(f\"WORKFLOW COMPLETE\")\n",
    "        print(f\"{'#'*60}\")\n",
    "        print(f\"Total Iterations: {results['iterations']}\")\n",
    "        print(f\"Final Grade: {results['final_grade'].upper()}\")\n",
    "        print(f\"Final Quality Score: {results['quality_score']}/10\")\n",
    "        print(f\"Quality Passed: {' YES' if results['passed'] else ' NO'}\")\n",
    "        print(f\"{'#'*60}\\n\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def visualize(self, output_path: str = \"evaluator_optimizer_graph.png\"):\n",
    "        \"\"\"\n",
    "        Visualize the workflow graph.\n",
    "\n",
    "        Args:\n",
    "            output_path: Path to save the graph image\n",
    "        \"\"\"\n",
    "        try:\n",
    "            img_data = self.workflow.get_graph().draw_mermaid_png()\n",
    "            with open(output_path, \"wb\") as f:\n",
    "                f.write(img_data)\n",
    "            print(f\"Workflow graph saved to: {output_path}\")\n",
    "            return Image(img_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not generate graph visualization: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c841e54-4f43-4a35-8985-8914de5f578d",
   "metadata": {},
   "source": [
    "# 6. Orchestrator Agent\n",
    "- initializes workers\n",
    "- planning - execute workers in order:\n",
    "1) MemoryAnalyzer 2. Ingestion 3. SummarizerWorker 4. MemoryWorker (stores initial summary) 5. EvaluatorOptimizer 6. MemoryWorker (stores final optimized summary)\n",
    "- routing - passes outputs from one worker to the next\n",
    "- skips EvaluatorOptimizer if memory is sufficient and already optimized, found by tags: symbol, \"summary\", \"optimized\"\n",
    "- memory is sufficient when previously optimized, relevant to query, current, not missing info, snapshot addresses the qeury -> Skips eval optimizer\n",
    "- insufficient memory fetches new data via Ingestion, Summarizer and EvaluatorOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a15e14a3-5cc7-4c5d-a7fb-82a830347ddd",
   "metadata": {
    "id": "a15e14a3-5cc7-4c5d-a7fb-82a830347ddd"
   },
   "outputs": [],
   "source": [
    "# --- Orchestrator Agent ---\n",
    "class Orchestrator:\n",
    "    \"\"\"\n",
    "    Coordinates the workflow of all workers:\n",
    "    1. Retrieve memory for symbol\n",
    "    2. Analyze if memory is sufficient using MemoryAnalyzer\n",
    "    3. If sufficient: use cached optimized summary, else: fetch fresh data via Ingestion\n",
    "    4. Summarizer: generate summary from data\n",
    "    5. Memory: store summary and metadata\n",
    "    6. EvaluatorOptimizer: refine the summary iteratively (skipped if already optimized)\n",
    "    \"\"\"\n",
    "\n",
    "    # -----------------------------\n",
    "    # Worker Initialization\n",
    "    # -----------------------------\n",
    "    def __init__(self):\n",
    "        self.workers = {\n",
    "            \"analyzer\": MemoryAnalyzer(),\n",
    "            \"ingestion\": Ingestion(),\n",
    "            \"summarizer\": SummarizerWorker(),\n",
    "            \"memory\": MemoryWorker(),\n",
    "            \"evaluator_optimizer\": EvaluatorOptimizer()\n",
    "        }\n",
    "\n",
    "    # -----------------------------\n",
    "    # Planning and Routing Execution\n",
    "    # -----------------------------\n",
    "    def execute(self, symbol: str, instructions: str) -> str:\n",
    "        \"\"\"Runs the full workflow and returns a formatted Markdown summary\"\"\"\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\" ORCHESTRATOR STARTING\")\n",
    "        print(f\"Symbol: {symbol}\")\n",
    "        print(f\"Query: {instructions}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "        # --- Step 1: Retrieve previous memories for this symbol ---\n",
    "        previous_memories = self.workers[\"memory\"].execute(\"retrieve_by_symbol\", symbol)\n",
    "        print(f\" Retrieved {len(previous_memories)} previous memory entries for {symbol}\")\n",
    "        \n",
    "        # --- Step 2: Format memories into a snapshot for analysis ---\n",
    "        memory_snapshot = self._format_memory_snapshot(previous_memories, symbol)\n",
    "        \n",
    "        # --- Step 3: Analyze if existing memory is sufficient ---\n",
    "        print(f\"\\n Analyzing memory sufficiency...\")\n",
    "        analysis_result = self.workers[\"analyzer\"].execute(memory_snapshot, instructions)\n",
    "        \n",
    "        print(f\"Analysis Result:\")\n",
    "        print(f\"  - Data Sufficient: {analysis_result['is_sufficient']}\")\n",
    "        print(f\"  - Requires Fresh Data: {analysis_result['requires_fresh_data']}\")\n",
    "        if analysis_result.get('missing_information'):\n",
    "            print(f\"  - Missing Info: {', '.join(analysis_result['missing_information'][:3])}\")\n",
    "\n",
    "        # --- Step 4: Decide whether to use memory or fetch fresh data ---\n",
    "        optimized_memories = [\n",
    "            mem for mem in previous_memories\n",
    "            # Check summary with optimized tag\n",
    "            if \"summary\" in mem.get(\"tags\", []) and \"optimized\" in mem.get(\"tags\", [])\n",
    "        ]\n",
    "        # Verify optimized memories are sufficient\n",
    "        if optimized_memories and analysis_result[\"is_sufficient\"] and not analysis_result[\"requires_fresh_data\"]:\n",
    "            # Use cached summary only if memory is sufficient and fresh\n",
    "            print(f\"\\n Using previously optimized memory summary\")\n",
    "            most_recent_optimized = optimized_memories[-1]\n",
    "    \n",
    "            summary_result = {\n",
    "                \"symbol\": symbol,\n",
    "                \"summary\": most_recent_optimized.get(\"text\", \"\"),\n",
    "                \"routed_notes\": {},\n",
    "                \"artifacts\": {\"source\": \"memory\"},\n",
    "                \"memory_writes\": []\n",
    "            }\n",
    "            # Set to skip EvaluatorOptimizer\n",
    "            skip_optimizer = True\n",
    "            ingestion_result = {}  # no new ingestion needed\n",
    "        else:\n",
    "            # Memory insufficient, stale, or no optimized summary exists\n",
    "            print(f\"\\n Fetching fresh data (memory insufficient, stale, or not optimized)\")\n",
    "            \n",
    "            ingestion_result = self.workers[\"ingestion\"].execute(symbol)\n",
    "            \n",
    "            news_articles = (ingestion_result.get(\"news_data\") or {}).get(\"articles\", [])\n",
    "            \n",
    "            summary_result = self.workers[\"summarizer\"].execute({\n",
    "                \"symbol\": symbol,\n",
    "                \"raw_news\": news_articles,\n",
    "                \"window\": 7,\n",
    "                \"analysis_goal\": instructions\n",
    "            })\n",
    "            # Store new intitial summary in memory with summary tag\n",
    "            for note in summary_result.get(\"memory_writes\", []):\n",
    "                self.workers[\"memory\"].execute(\"add\", note, [symbol, \"summary\"])\n",
    "            # Set to false to run EvaluatorOptimizer\n",
    "            skip_optimizer = False\n",
    "\n",
    "        # --- Step 5: Evaluator-Optimizer Workflow ---\n",
    "        if not skip_optimizer:\n",
    "            evaluator = self.workers[\"evaluator_optimizer\"]\n",
    "            initial_state = {\n",
    "                \"symbol\": symbol,\n",
    "                \"instructions\": instructions,\n",
    "                \"context\": ingestion_result,\n",
    "                \"summary\": summary_result[\"summary\"],\n",
    "                \"feedback\": \"\",\n",
    "                \"grade\": \"\",\n",
    "                \"quality_score\": 0.0,\n",
    "                \"issues\": [],\n",
    "                \"iteration\": 0,\n",
    "                \"max_iterations\": evaluator.max_iterations,\n",
    "                \"history\": []\n",
    "            }\n",
    "            final_result = evaluator.workflow.invoke(initial_state)\n",
    "            final_summary_text = final_result[\"summary\"]\n",
    "            \n",
    "            # Store optimized summary in memory\n",
    "            memory_note = f\"[{symbol}] Final Optimized Summary (Query: {instructions[:50]}...)\"\n",
    "            timestamp = datetime.now().isoformat()\n",
    "            full_memory_entry = f\"{memory_note}\\nTimestamp: {timestamp}\\n\\n{final_summary_text}\"\n",
    "            # Store final optimized summary in memory with symbol, \"summary\" and \"optimized\" tags\n",
    "            self.workers[\"memory\"].execute(\"add\", full_memory_entry, [symbol, \"summary\", \"optimized\"])\n",
    "        else:\n",
    "            # Already optimized, skip optimizer\n",
    "            final_summary_text = summary_result[\"summary\"]\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\" ORCHESTRATOR COMPLETE\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "        return final_summary_text  # returns plain Markdown string\n",
    "\n",
    "    # -----------------------------\n",
    "    # Helper Methods - formatting memory snapshot\n",
    "    # -----------------------------\n",
    "    def _format_memory_snapshot(self, memories: List[Dict[str, Any]], symbol: str) -> str:\n",
    "        \"\"\"\n",
    "        Format an array of memory records into a single string snapshot for analysis.\n",
    "        Only include optimized summaries for memory-sufficient evaluation.\n",
    "        \"\"\"\n",
    "        if not memories:\n",
    "            return \"\"\n",
    "        \n",
    "        snapshot_parts = [f\"# Memory Data for {symbol}\\n\"]\n",
    "        \n",
    "        # Group memories by type (summary, routes, other)\n",
    "        summaries = [\n",
    "            f\"## Entry from {mem.get('timestamp','N/A')}\\n{mem.get('text','')}\\n\"\n",
    "            for mem in memories\n",
    "            if 'summary' in mem.get('tags', []) and 'optimized' in mem.get('tags', [])\n",
    "        ]\n",
    "        routes = []\n",
    "        other = []\n",
    "        \n",
    "        # Iteratre through each memory to categorize\n",
    "        for mem in memories:\n",
    "            text = mem.get('text', '')\n",
    "            timestamp = mem.get('timestamp', 'N/A')\n",
    "            \n",
    "            if 'Routes:' in text:\n",
    "                routes.append(f\"- {text} (at {timestamp})\")\n",
    "            elif 'summary' not in mem.get('tags', []):\n",
    "                other.append(f\"- {text} (at {timestamp})\")\n",
    "        \n",
    "        # If previous summaries exits, add header to include last 2\n",
    "        if summaries:\n",
    "            snapshot_parts.append(\"\\n## Previous Summaries\\n\")\n",
    "            snapshot_parts.extend(summaries[-2:])\n",
    "        # If previous routing notes exist, add header to include last 3\n",
    "        if routes:\n",
    "            snapshot_parts.append(\"\\n## Previous Routing Information\\n\")\n",
    "            snapshot_parts.extend(routes[-3:])\n",
    "        # If previous other notes exist, add section header to include last 5\n",
    "        if other:\n",
    "            snapshot_parts.append(\"\\n## Other Notes\\n\")\n",
    "            snapshot_parts.extend(other[-5:])\n",
    "        \n",
    "        return \"\\n\".join(snapshot_parts)\n",
    "        \n",
    "    # Retrieve most recent optimized summary tags\n",
    "    def _extract_from_memory(self, memories: List[Dict[str, Any]], symbol: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Extract ingestion-like data structure from memory records.\n",
    "        Only retrieves fully optimized summaries.\n",
    "        \"\"\"\n",
    "        most_recent = None\n",
    "        for mem in reversed(memories):\n",
    "            # Check memory entry tagged for summary and optimized\n",
    "            if 'summary' in mem.get('tags', []) and 'optimized' in mem.get('tags', []):\n",
    "                most_recent = mem\n",
    "                break\n",
    "        \n",
    "        # Return placeholder if no optimized summary found\n",
    "        if not most_recent:\n",
    "            return {\n",
    "                \"symbol\": symbol,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"financial_data\": None,\n",
    "                \"news_data\": None,\n",
    "                \"errors\": [{\"source\": \"memory\", \"note\": \"Using cached data\"}],\n",
    "                \"status\": \"success\"\n",
    "            }\n",
    "        # Return memory structure for most recent optimized summary\n",
    "        return {\n",
    "            \"symbol\": symbol,\n",
    "            \"timestamp\": most_recent.get('timestamp', datetime.now().isoformat()),\n",
    "            \"financial_data\": {\n",
    "                \"source\": \"memory\",\n",
    "                \"summary\": most_recent.get('text', '')\n",
    "            },\n",
    "            \"news_data\": {\n",
    "                \"source\": \"memory\",\n",
    "                \"articles\": []\n",
    "            },\n",
    "            \"errors\": [],\n",
    "            \"status\": \"success\"\n",
    "        }\n",
    "# -----------------------------\n",
    "# Helper Function to Run Analysis\n",
    "# -----------------------------\n",
    "def run_investment_analysis(symbol: str, instructions: str) -> str:\n",
    "    \"\"\"\n",
    "    Convenience function to run a complete investment analysis.\n",
    "    \n",
    "    Args:\n",
    "        symbol: Stock ticker symbol (e.g., \"AAPL\", \"TSLA\", \"NVDA\")\n",
    "        instructions: User's query or analysis request\n",
    "        \n",
    "    Returns:\n",
    "        Final optimized summary as Markdown string\n",
    "        \n",
    "    Example:\n",
    "        result = run_investment_analysis(\"AAPL\", \"Should I buy Apple stock now?\")\n",
    "        display(Markdown(result))\n",
    "    \"\"\"\n",
    "    orchestrator = Orchestrator()\n",
    "    \n",
    "    return orchestrator.execute(symbol, instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab6beea-3507-4f0c-96db-671df770d1c3",
   "metadata": {},
   "source": [
    "# 7. Run Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "721c9582-9571-4781-861d-d292168c2a3d",
   "metadata": {
    "id": "721c9582-9571-4781-861d-d292168c2a3d",
    "outputId": "cf4b0329-22a3-422e-fe1b-056ab8fa539d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " ORCHESTRATOR STARTING\n",
      "Symbol: TSLA\n",
      "Query: Should I buy Tesla stock?\n",
      "============================================================\n",
      "\n",
      " Retrieved 16 previous memory entries for TSLA\n",
      "\n",
      " Analyzing memory sufficiency...\n",
      "Analysis Result:\n",
      "  - Data Sufficient: True\n",
      "  - Requires Fresh Data: False\n",
      "\n",
      " Using previously optimized memory summary\n",
      "\n",
      "============================================================\n",
      " ORCHESTRATOR COMPLETE\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "[TSLA] Final Optimized Summary (Query: Should I buy Tesla stock?...)\n",
       "Timestamp: 2025-10-20T20:01:24.458878\n",
       "\n",
       "# Investment Research Summary: Tesla, Inc. (TSLA)\n",
       "\n",
       "## Current Investment Outlook\n",
       "**Recommendation: Hold**\n",
       "\n",
       "Tesla, Inc. (TSLA) is currently a stock that presents a mixed investment outlook. While there are significant opportunities for growth, investors should be cautious due to high valuations and market volatility.\n",
       "\n",
       "---\n",
       "\n",
       "## Key Financial Metrics\n",
       "- **Current Price:** $447.43\n",
       "- **Market Cap:** $1.49 trillion\n",
       "- **P/E Ratio:** 138.10\n",
       "- **52-Week Range:** $212.11 - $488.54\n",
       "- **Beta:** 2.086 (indicates high volatility relative to the market)\n",
       "- **30-Day Volatility:** 53.54%\n",
       "- **Volume:** 63,496,200 shares traded today\n",
       "\n",
       "### Recent Price Trend\n",
       "- **Day 1:** $429.24, Volume=72,669,400\n",
       "- **Day 2:** $435.15, Volume=71,558,200\n",
       "- **Day 3:** $428.75, Volume=77,189,900\n",
       "- **Day 4:** $439.31, Volume=89,331,600\n",
       "- **Day 5:** $447.43, Volume=63,496,200\n",
       "\n",
       "Tesla's price has shown an upward trend over the last five days, closing at $447.43, which indicates a recovery from previous fluctuations.\n",
       "\n",
       "---\n",
       "\n",
       "## Recent News Analysis\n",
       "The sentiment surrounding Tesla is shaped by several recent headlines:\n",
       "\n",
       "1. **Robotaxi Earnings Call**: Investors are keenly awaiting insights on the rollout of Tesla's robotaxi service, which could significantly enhance revenue streams.\n",
       "2. **Earnings Expectations**: Analysts are closely monitoring earnings reports from Tesla and other major automakers like GM and Ford, which will provide context for Tesla's market positioning.\n",
       "3. **Management Concerns**: Cathie Wood has criticized the compensation package for Elon Musk, suggesting that investor confidence could be affected by executive pay structures.\n",
       "\n",
       "These news articles reflect a strong interest in Tesla's future growth prospects, particularly in autonomous driving and electric vehicles (EVs), but also highlight potential governance issues.\n",
       "\n",
       "---\n",
       "\n",
       "## Opportunities\n",
       "1. **Market Leadership in EVs**: Tesla remains a leader in the electric vehicle market, with strong brand recognition and a robust product lineup.\n",
       "2. **Innovation Pipeline**: The anticipated rollout of new technologies, especially in the autonomous vehicle segment, could drive future growth.\n",
       "3. **Energy Solutions**: Beyond automotive, Tesla's energy generation and storage business offers additional revenue potential.\n",
       "\n",
       "---\n",
       "\n",
       "## Risks\n",
       "1. **High Valuation**: The current P/E ratio of 138.10 suggests that the stock is overvalued compared to the broader market, making it susceptible to corrections, especially in a volatile market.\n",
       "2. **Volatility**: With a beta of 2.086, TSLA is more volatile than the overall market. Investors should be prepared for significant price swings.\n",
       "3. **Competition**: The automotive industry is becoming increasingly competitive, with traditional automakers ramping up their EV offerings, which could erode Tesla's market share.\n",
       "\n",
       "---\n",
       "\n",
       "## Conclusion\n",
       "Given the current financial metrics, recent news, and market context, it is recommended to **hold** Tesla stock. The company exhibits strong growth potential driven by innovation and market leadership in electric vehicles; however, the high valuation and inherent volatility present significant risks. Investors should maintain their current positions while closely monitoring upcoming earnings reports and news related to Tesla's strategic initiatives, particularly around robotaxi developments and competitive dynamics in the EV market. \n",
       "\n",
       "In summary, while the outlook for Tesla remains positive, the risks associated with its high valuation and market volatility suggest a cautious approach for the time being."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Confirms sufficient data found for recent previously optimized summary\n",
    "result = run_investment_analysis(\"TSLA\", \"Should I buy Tesla stock?\")\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e040e3d-46ed-4532-a6b2-58002e0b0b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " ORCHESTRATOR STARTING\n",
      "Symbol: RIVN\n",
      "Query: Should I buy Rivian stock?\n",
      "============================================================\n",
      "\n",
      " Retrieved 0 previous memory entries for RIVN\n",
      "\n",
      " Analyzing memory sufficiency...\n",
      "Analysis Result:\n",
      "  - Data Sufficient: False\n",
      "  - Requires Fresh Data: True\n",
      "  - Missing Info: All information - no memory data exists\n",
      "\n",
      " Fetching fresh data (memory insufficient, stale, or not optimized)\n",
      "\n",
      "============================================================\n",
      "GENERATOR - Iteration 1\n",
      "============================================================\n",
      "User Request: Should I buy Rivian stock?...\n",
      "Summary generated (3372 characters)\n",
      "\n",
      "============================================================\n",
      "EVALUATOR - Iteration 1\n",
      "============================================================\n",
      "Grade: PASS\n",
      "Quality Score: 8.0/10\n",
      "Issues Found: 2\n",
      "  1. Lack of detailed competitive advantages or strategies for Rivian.\n",
      "  2. Limited context on the broader EV market conditions.\n",
      "Feedback: The investment research summary is well-structured and addresses the user's request effectively. However, it could be improved by providing more detai...\n",
      "\n",
      "Quality PASSED - Ending optimization\n",
      "\n",
      "============================================================\n",
      " ORCHESTRATOR COMPLETE\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Investment Research Summary for Rivian Automotive, Inc. (RIVN)\n",
       "\n",
       "#### Current Market Overview\n",
       "- **Current Price:** $13.08\n",
       "- **Price Change:** +$0.05 (0.38%)\n",
       "- **Market Cap:** $15.82 billion\n",
       "- **Volume:** 35,592,900\n",
       "- **52-Week Range:** $9.50 - $17.15\n",
       "- **P/E Ratio:** -4.77 (indicating losses)\n",
       "- **Beta:** 1.829 (indicating higher volatility than the market)\n",
       "- **30-Day Volatility:** 37.41%\n",
       "\n",
       "#### Company Overview\n",
       "Rivian Automotive, Inc. specializes in designing, developing, manufacturing, and selling electric vehicles (EVs) and accessories. Its primary offerings include the R1T pickup truck and the R1S sport utility vehicle. Rivian aims to compete in the rapidly growing EV market, which is becoming increasingly competitive.\n",
       "\n",
       "#### Recent Price Trend\n",
       "Over the past five days, Rivian's stock has shown fluctuations with a slight downtrend followed by a modest recovery:\n",
       "- **Day 1:** $13.12, Volume 25,298,100\n",
       "- **Day 2:** $13.41, Volume 36,082,900\n",
       "- **Day 3:** $12.91, Volume 28,327,500\n",
       "- **Day 4:** $13.03, Volume 24,003,800\n",
       "- **Day 5:** $13.08, Volume 35,592,900\n",
       "\n",
       "#### Sentiment Analysis\n",
       "Recent news articles present a mixed sentiment regarding Rivian:\n",
       "1. **Market Concerns**: Several analysts, including those from Mizuho, have downgraded Rivian, citing \"headwinds\" in EV sales and pressures from slowing demand, particularly in China. This suggests potential challenges ahead for the company in maintaining growth.\n",
       "2. **Stock Performance**: Rivian's stock price has seen a slight uptick recently, even in the face of negative sentiment, indicating a degree of resilience among investors.\n",
       "3. **Analyst Opinions**: Downgrades from reputable analysts may reflect a cautious outlook, suggesting that potential investors should approach with caution.\n",
       "\n",
       "#### Risk Factors\n",
       "1. **Negative Earnings**: Rivian's P/E ratio of -4.77 indicates ongoing losses, which is a red flag for potential investors.\n",
       "2. **High Volatility**: With a beta of 1.829 and a 30-day volatility of 37.41%, Rivian's stock is significantly more volatile than the broader market, suggesting a higher risk profile.\n",
       "3. **Competitive Landscape**: The EV market is becoming increasingly saturated with established players like Tesla and new entrants, which may impact Rivian’s market share and pricing power.\n",
       "\n",
       "#### Investment Recommendation\n",
       "**Recommendation: Hold**\n",
       "\n",
       "**Rationale:**\n",
       "- **Short-term Performance**: Rivian's recent stock performance shows a degree of stability despite market concerns. However, the lack of profitability and high volatility suggests that investors should be cautious.\n",
       "- **Market Sentiment**: Downgrades from analysts and the acknowledgment of potential headwinds indicate that the stock may face pressure in the short to medium term.\n",
       "- **Long-term Potential**: Rivian has strong brand recognition and innovative product offerings that could capture market share as the EV market grows. However, investors should wait for clearer signs of profitability and stability before making a buy decision.\n",
       "\n",
       "#### Conclusion\n",
       "While Rivian Automotive, Inc. presents an interesting opportunity within the EV sector, the current financial metrics and market sentiment suggest that it may be prudent to hold rather than buy at this time. Investors should monitor the company's performance closely and reassess their position as new data emerges regarding profitability and market conditions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Confirms insufficient data for new stock symbol and new data retrieved\n",
    "result = run_investment_analysis(\"RIVN\", \"Should I buy Rivian stock?\")\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a297d8a-987d-4016-81df-74fa13742b48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
