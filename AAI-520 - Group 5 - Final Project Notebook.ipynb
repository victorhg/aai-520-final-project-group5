{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45de57cb-1a4c-4092-a9a7-4e263931d43a",
   "metadata": {},
   "source": [
    "USD AAI-520 - Natural Language Processing and GenAI\n",
    "\n",
    "## Financial Agentic System\n",
    "\n",
    "Group 5: Antonio Recalde, Ajmal Jalal, Darin Verduzco, Victor H. Germano\n",
    "\n",
    "GitHub: https://github.com/victorhg/aai-520-final-project-group5\n",
    "\n",
    "News sources: Yahoo Finance and NewsAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0772e84-0bc1-4ae4-af56-67ff5e40031d",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"overall_project_workflow.png\" width=\"70%\">\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "320f9da7-a5eb-4512-93a7-2088e306475f",
   "metadata": {},
   "source": [
    "Agent Functions \n",
    "1. Plans its research steps for a given stock symbol.\n",
    "2. Uses tools dynamically (APIs, datasets, retrieval).\n",
    "3. Self-reflects to assess the quality of its output.\n",
    "4. Learns across runs (e.g., keeps brief memories or notes to improve future analyses).\n",
    "\n",
    "Workflow Patterns \n",
    "1. Prompt Chaining: Ingest News → Preprocess → Classify → Extract → Summarize\n",
    "2. Routing: Direct content to the right specialist (e.g., earnings, news, or market analyzers).\n",
    "3. Evaluator–Optimizer: Generate analysis → evaluate quality → refine using feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7349fe60-ab27-4896-b466-1a343645e7ed",
   "metadata": {},
   "source": [
    "## Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "060fb464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install yfinance feedparser requests python-dotenv typing_extensions pydantic langgraph IPython langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bc3a04-5c2e-4581-a002-205c3a2e54fb",
   "metadata": {},
   "source": [
    "## Import Libraries and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf510d52-8d34-41ac-b2b2-d4908db6a394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Data Handling & I/O\n",
    "# ---------------------------\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import html\n",
    "import requests\n",
    "import feedparser\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "# ---------------------------\n",
    "# Typing & Preprocessing Utilities\n",
    "# ---------------------------\n",
    "from __future__ import annotations\n",
    "from typing_extensions import Annotated\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import Dict, Any, List, Optional, Union, TypedDict, Literal\n",
    "\n",
    "# ---------------------------\n",
    "# Models / Evaluation\n",
    "# ---------------------------\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display, Markdown\n",
    "\n",
    "# Load \"news_openai.env\" environment key file\n",
    "load_dotenv(\"news_openai.env\")\n",
    "# Set NewsAPI key\n",
    "NEWS_API_KEY = os.getenv(\"NEWS_API_KEY\")\n",
    "# OpenAI API key loaded via env variable \"OPENAI_API_KEY\" during \"MemoryAnalyzer\" and \"EvaluatorOptimizer\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f1559c-330a-4922-bf76-f582c4d334c6",
   "metadata": {},
   "source": [
    "# MemoryAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5967fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# worker/base_worker.py\n",
    "# ---------------------------\n",
    "# ==============================================================================\n",
    "# BASE WORKER CLASS\n",
    "# ==============================================================================\n",
    "# All worker classes inherit from BaseWorker and implement the execute() method\n",
    "\n",
    "class BaseWorker:\n",
    "    def execute(self, *inputs):\n",
    "        \"\"\"Execute the worker's main function. To be overridden by subclasses.\"\"\"\n",
    "        raise NotImplementedError(\"This method should be overridden by subclasses.\")\n",
    "\n",
    "class SufficiencyAssessment(BaseModel):\n",
    "    \"\"\"Structured assessment of whether memory data is sufficient to answer a query\"\"\"\n",
    "\n",
    "    is_sufficient: bool = Field(\n",
    "        description=\"Whether the memory data contains enough information to fully answer the query\"\n",
    "    )\n",
    "    requires_fresh_data: bool = Field(\n",
    "        description=\"Whether this query requires the most current data (e.g., current prices, recent news) or can use older data (e.g., follow-up questions, general analysis)\"\n",
    "    )\n",
    "    missing_information: list[str] = Field(\n",
    "        description=\"List of specific information gaps that would be needed to fully answer the query\",\n",
    "        default_factory=list\n",
    "    )\n",
    "# ---------------------------\n",
    "# src/memory_analyzer\n",
    "# ---------------------------\n",
    "\n",
    "class MemoryAnalyzer(BaseWorker):\n",
    "    \"\"\"\n",
    "    Analyzes whether existing memory data is sufficient to answer a user query.\n",
    "    Uses LangChain to perform intelligent assessment of data completeness.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the MemoryAnalyzer with OpenAI LLM\"\"\"\n",
    "        super().__init__()\n",
    "        self.llm = self._initialize_llm()\n",
    "        self.structured_llm = self.llm.with_structured_output(SufficiencyAssessment)\n",
    "\n",
    "    def _initialize_llm(self) -> ChatOpenAI:\n",
    "        \"\"\"Initialize OpenAI LLM with API key from environment\"\"\"\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "        if not api_key:\n",
    "            raise ValueError(\n",
    "                \"OPENAI_API_KEY not found in environment variables. \"\n",
    "                \"Please set it in your .env file.\"\n",
    "            )\n",
    "\n",
    "        return ChatOpenAI(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature=0.1,  # Low temperature for consistent analysis\n",
    "            api_key=api_key\n",
    "        )\n",
    "\n",
    "    def _extract_timestamp(self, memory_snapshot: str) -> datetime:\n",
    "        \"\"\"\n",
    "        Extract timestamp from memory snapshot.\n",
    "        Looks for patterns like \"Timestamp: YYYY-MM-DD HH:MM:SS\" or similar.\n",
    "\n",
    "        Returns:\n",
    "            datetime object if found, None otherwise\n",
    "        \"\"\"\n",
    "        # Common timestamp patterns\n",
    "        patterns = [\n",
    "            r'Timestamp:\\s*(\\d{4}-\\d{2}-\\d{2}[T\\s]\\d{2}:\\d{2}:\\d{2})',\n",
    "            r'Date:\\s*(\\d{4}-\\d{2}-\\d{2}[T\\s]\\d{2}:\\d{2}:\\d{2})',\n",
    "            r'Created:\\s*(\\d{4}-\\d{2}-\\d{2}[T\\s]\\d{2}:\\d{2}:\\d{2})',\n",
    "            r'Updated:\\s*(\\d{4}-\\d{2}-\\d{2}[T\\s]\\d{2}:\\d{2}:\\d{2})',\n",
    "            r'(\\d{4}-\\d{2}-\\d{2}[T\\s]\\d{2}:\\d{2}:\\d{2})',  # Just the timestamp\n",
    "        ]\n",
    "\n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, memory_snapshot, re.IGNORECASE)\n",
    "            if match:\n",
    "                timestamp_str = match.group(1)\n",
    "                try:\n",
    "                    # Handle both \"T\" separator and space separator\n",
    "                    if 'T' not in timestamp_str:\n",
    "                        timestamp_str = timestamp_str.replace(' ', 'T')\n",
    "                    return datetime.fromisoformat(timestamp_str)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "        return None\n",
    "\n",
    "    def execute(self, memory_snapshot: str, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Analyze if the memory snapshot contains sufficient data to answer the query,\n",
    "        considering both content relevance and data freshness.\n",
    "\n",
    "        Args:\n",
    "            memory_snapshot: String containing the current memory data\n",
    "            query: User's query/question\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with:\n",
    "                - is_sufficient: Boolean indicating if data is sufficient\n",
    "                - requires_fresh_data: Boolean indicating if query requires fresh data\n",
    "                - missing_information: List of gaps\n",
    "        \"\"\"\n",
    "        if not memory_snapshot or not memory_snapshot.strip():\n",
    "            return {\n",
    "                \"is_sufficient\": False,\n",
    "                \"requires_fresh_data\": True,  # Conservative assumption when no data exists\n",
    "                \"missing_information\": [\"All information - no memory data exists\"]\n",
    "            }\n",
    "\n",
    "        # Extract timestamp from memory snapshot\n",
    "        memory_timestamp = self._extract_timestamp(memory_snapshot)\n",
    "        current_dt = datetime.now()\n",
    "\n",
    "        # Define freshness threshold: 24 hours for financial data\n",
    "        freshness_threshold_hours = 24\n",
    "        is_data_fresh = True\n",
    "        age_hours = 0\n",
    "\n",
    "        if memory_timestamp:\n",
    "            age_hours = (current_dt - memory_timestamp).total_seconds() / 3600\n",
    "            is_data_fresh = age_hours <= freshness_threshold_hours\n",
    "        else:\n",
    "            # If no timestamp found, assume data might be stale\n",
    "            is_data_fresh = False\n",
    "\n",
    "        # Create analysis prompt\n",
    "        data_age_info = f\"Data age: {age_hours:.1f} hours old\" if memory_timestamp else \"Data age: unknown\"\n",
    "        prompt = f\"\"\"You are an expert financial analyst evaluating whether existing research data is sufficient to answer a user's query.\n",
    "\n",
    "USER QUERY:\n",
    "{query}\n",
    "\n",
    "AVAILABLE MEMORY DATA:\n",
    "{memory_snapshot}\n",
    "\n",
    "DATA FRESHNESS: {data_age_info}\n",
    "\n",
    "ANALYSIS TASK:\n",
    "Determine if the available memory data contains enough information to fully and accurately answer the user's query.\n",
    "\n",
    "Consider:\n",
    "1. Does the data directly address the query topic?\n",
    "2. Are there specific facts, metrics, or analysis needed to answer the query?\n",
    "3. Does this query require the most current/fresh data (e.g., current prices, breaking news, real-time metrics) or can it use older data (e.g., follow-up questions, general analysis, historical context)?\n",
    "4. Are there any gaps that would require additional research?\n",
    "\n",
    "IMPORTANT: If you are uncertain whether the data is sufficient, err on the side of caution and mark it as insufficient.\n",
    "\n",
    "Provide a structured assessment of data sufficiency.\"\"\"\n",
    "\n",
    "        try:\n",
    "            # Get structured assessment from LLM\n",
    "            assessment = self.structured_llm.invoke(prompt)\n",
    "\n",
    "            # Consider both content sufficiency and data freshness\n",
    "            content_sufficient = assessment.is_sufficient\n",
    "            requires_fresh = assessment.requires_fresh_data\n",
    "\n",
    "            # Final sufficiency decision: content must be sufficient AND data must be fresh if required\n",
    "            final_sufficient = content_sufficient and (not requires_fresh or is_data_fresh)\n",
    "\n",
    "            # Build missing information list\n",
    "            missing_info = assessment.missing_information.copy()\n",
    "            if requires_fresh and not is_data_fresh:\n",
    "                age_msg = f\"Data is too old ({age_hours:.1f} hours > {freshness_threshold_hours} hour threshold)\"\n",
    "                if age_msg not in missing_info:\n",
    "                    missing_info.insert(0, age_msg)\n",
    "\n",
    "            # Convert to dictionary for return\n",
    "            result = {\n",
    "                \"is_sufficient\": final_sufficient,\n",
    "                \"requires_fresh_data\": requires_fresh,\n",
    "                \"missing_information\": missing_info\n",
    "            }\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            # Fallback to basic keyword matching if LLM fails\n",
    "            print(f\"LLM analysis failed: {e}\")\n",
    "            return {\n",
    "                \"is_sufficient\": False, # automatically mark as insufficient so data can be refetched\n",
    "                \"requires_fresh_data\": True,  # Conservative assumption when LLM fails\n",
    "                \"missing_information\": [\"LLM analysis unavailable\"]\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef09596-8c37-451f-b512-a863fc0bfac0",
   "metadata": {},
   "source": [
    "# Data Ingestion\n",
    "\n",
    "## Overview\n",
    "The Ingestion module is responsible for collecting financial and news data from multiple sources in parallel. It consists of three main components:\n",
    "\n",
    "1. **FinancialDataIngestion**: Fetches stock data from Yahoo Finance (yfinance)\n",
    "   - Historical OHLCV data (Open, High, Low, Close, Volume)\n",
    "   - Company fundamentals (P/E ratio, market cap, beta, sector)\n",
    "   - Calculated metrics (volatility, price changes, volume averages)\n",
    "\n",
    "2. **NewsDataIngestion**: Fetches news articles from multiple sources\n",
    "   - NewsAPI for general financial news\n",
    "   - Yahoo Finance RSS feeds for stock-specific news\n",
    "   - Preprocesses text to remove HTML and malicious content\n",
    "\n",
    "3. **Ingestion (Coordinator)**: Orchestrates parallel data fetching\n",
    "   - Executes financial and news ingestion concurrently using ThreadPoolExecutor\n",
    "   - Handles partial failures gracefully\n",
    "   - Combines results into a unified data bundle\n",
    "\n",
    "## Data Flow\n",
    "```\n",
    "User Request → Ingestion.execute(symbol)\n",
    "    ├─→ FinancialDataIngestion.execute(symbol, period) [Thread 1]\n",
    "    │   └─→ Returns: financial metrics, historical data, company info\n",
    "    └─→ NewsDataIngestion.execute(symbol, limit) [Thread 2]\n",
    "        └─→ Returns: articles from NewsAPI + Yahoo RSS\n",
    "                ↓\n",
    "    Combined Result: {financial_data, news_data, errors, status}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e301856-1e5b-44f9-9acb-8531eead0979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# (step 1)\n",
    "# ingestion/financial_data.py (file 1 of 3)\n",
    "# (Yahoo Finance data fetching, metrics calculation, and error handling, data structuring.)\n",
    "# ---------------------------\n",
    "\n",
    "# ==============================================================================\n",
    "# FINANCIAL DATA INGESTION WORKER\n",
    "# ==============================================================================\n",
    "# Purpose: Fetch stock data from Yahoo Finance API (yfinance)\n",
    "# Responsibilities:\n",
    "#   - Retrieve historical OHLCV data (Open, High, Low, Close, Volume)\n",
    "#   - Fetch company fundamentals (P/E ratio, market cap, sector, industry)\n",
    "#   - Calculate derived metrics (volatility, price changes, volume averages)\n",
    "#   - Handle API errors and return structured data\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"\n",
    "NLP-5: Financial Data Ingestion\n",
    "Fetch stock data from Yahoo Finance API (yfinance)\n",
    "\"\"\"\n",
    "\n",
    "class FinancialDataIngestion(BaseWorker):\n",
    "    \"\"\"\n",
    "    Fetches financial data from Yahoo Finance.\n",
    "    \n",
    "    Responsibilities:\n",
    "    - Fetch historical OHLCV (Open, High, Low, Close, Volume) data\n",
    "    - Fetch company information and fundamentals\n",
    "    - Calculate basic metrics from raw data\n",
    "    - Handle API errors gracefully\n",
    "    - Return structured financial data\n",
    "    \"\"\"\n",
    "    \n",
    "    def execute(self, *inputs) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Fetch financial data for a given stock symbol.\n",
    "        \n",
    "        Args:\n",
    "            inputs[0] (str): Stock ticker symbol (e.g., \"AAPL\")\n",
    "            inputs[1] (str, optional): Time period (default: \"1mo\")\n",
    "                Valid periods: 1d, 5d, 1mo, 3mo, 6mo, 1y, 2y, 5y, 10y, ytd, max\n",
    "        \n",
    "        Returns:\n",
    "            dict: Financial data bundle containing:\n",
    "                - symbol: Stock ticker\n",
    "                - price metrics: Current price, changes, highs/lows\n",
    "                - volume metrics: Current and average volume\n",
    "                - volatility: Calculated volatility\n",
    "                - fundamentals: P/E ratio, market cap, beta, etc.\n",
    "                - company info: Sector, industry, summary\n",
    "                - historical_data: Recent OHLCV records\n",
    "                - status: Success or error status\n",
    "        \"\"\"\n",
    "        try:\n",
    "            symbol = inputs[0]\n",
    "            period = inputs[1] if len(inputs) > 1 else \"1mo\"\n",
    "\n",
    "            stock = yf.Ticker(symbol)\n",
    "\n",
    "            # Fetch historical OHLCV data from Yahoo Finance\n",
    "            hist = stock.history(period=period)\n",
    "            \n",
    "            if hist.empty:\n",
    "                return {\n",
    "                    \"source\": \"yahoo_finance\",\n",
    "                    \"symbol\": symbol,\n",
    "                    \"data\": None,\n",
    "                    \"status\": \"error\",\n",
    "                    \"error\": f\"No data found for symbol {symbol}\",\n",
    "                    \"timestamp\": datetime.now().isoformat()\n",
    "                }\n",
    "            \n",
    "            info = stock.info\n",
    "            \n",
    "            # Extract price metrics from historical data\n",
    "            current_price = hist['Close'].iloc[-1]\n",
    "            prev_close = info.get('previousClose', hist['Close'].iloc[-2] if len(hist) > 1 else current_price)\n",
    "            price_change = current_price - prev_close\n",
    "            price_change_pct = (price_change / prev_close) * 100 if prev_close != 0 else 0\n",
    "            \n",
    "            # Calculate annualized volatility using standard deviation of returns\n",
    "            # Formula: std(daily_returns) * sqrt(trading_days_per_year)\n",
    "            returns = hist['Close'].pct_change().dropna()\n",
    "            volatility = returns.tail(30).std() * (252 ** 0.5) if len(returns) > 0 else 0\n",
    "            \n",
    "            # Calculate volume statistics\n",
    "            avg_volume = hist['Volume'].tail(30).mean()\n",
    "            current_volume = hist['Volume'].iloc[-1]\n",
    "\n",
    "            # Raw data result from yf\n",
    "            result = {\n",
    "                \"source\": \"yahoo_finance\",\n",
    "                \"symbol\": symbol,\n",
    "                \"data\": {\n",
    "                    \"current_price\": float(current_price),\n",
    "                    \"price_change\": float(price_change),\n",
    "                    \"price_change_pct\": float(price_change_pct),\n",
    "                    \"volume\": int(current_volume),\n",
    "                    \"avg_volume_30d\": float(avg_volume),\n",
    "                    \"volatility_30d\": float(volatility),\n",
    "                    \"market_cap\": info.get(\"marketCap\"),\n",
    "                    \"pe_ratio\": info.get(\"forwardPE\"),\n",
    "                    \"dividend_yield\": info.get(\"dividendYield\"),\n",
    "                    \"52_week_high\": info.get(\"fiftyTwoWeekHigh\"),\n",
    "                    \"52_week_low\": info.get(\"fiftyTwoWeekLow\"),\n",
    "                    \"beta\": info.get(\"beta\"),\n",
    "                    \"sector\": info.get(\"sector\"),\n",
    "                    \"industry\": info.get(\"industry\"),\n",
    "                    \"company_summary\": info.get(\"longBusinessSummary\", \"\")[:500],\n",
    "                    \"historical_data\": hist.tail(30).to_dict('records')\n",
    "                },\n",
    "                \"status\": \"success\",\n",
    "                \"error\": None,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"source\": \"yahoo_finance\",\n",
    "                \"symbol\": inputs[0] if len(inputs) > 0 else \"UNKNOWN\",\n",
    "                \"data\": None,\n",
    "                \"status\": \"error\",\n",
    "                \"error\": str(e),\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "# ---------------------------\n",
    "# (step 1)\n",
    "# ingestion/news_data.py (file 2 of 3)\n",
    "# ---------------------------\n",
    "\n",
    "# ==============================================================================\n",
    "# NEWS DATA INGESTION WORKER  \n",
    "# ==============================================================================\n",
    "# Purpose: Fetch news articles from multiple sources\n",
    "# Responsibilities:\n",
    "#   - Query NewsAPI for general financial news\n",
    "#   - Fetch Yahoo Finance RSS feeds for stock-specific news\n",
    "#   - Preprocess text to remove HTML tags and malicious content\n",
    "#   - Return structured list of articles with metadata\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"\n",
    "NLP-6: News Data Ingestion\n",
    "Fetch news articles from NewsAPI and Yahoo Finance RSS\n",
    "\"\"\"\n",
    "\n",
    "class NewsDataIngestion(BaseWorker):\n",
    "    \"\"\"\n",
    "    Fetches news data from multiple sources.\n",
    "    \n",
    "    Responsibilities:\n",
    "    - Fetch news articles from NewsAPI\n",
    "    - Fetch news from Yahoo Finance RSS feeds\n",
    "    - Handle API errors and rate limits gracefully\n",
    "    - Return structured list of articles\n",
    "    \"\"\"\n",
    "    \n",
    "    DEFAULT_LIMIT = 10\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def execute(self, *inputs) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Fetch news articles for a given stock symbol.\n",
    "        \n",
    "        Args:\n",
    "            inputs[0] (str): Stock ticker symbol (e.g., \"AAPL\")\n",
    "            inputs[1] (int, optional): Number of articles to fetch (default: 10)\n",
    "        \n",
    "        Returns:\n",
    "            dict: News data bundle containing:\n",
    "                - articles: List of articles from all sources\n",
    "                - sources_queried: Which sources were successfully queried\n",
    "                - total_count: Total number of articles fetched\n",
    "                - status: Success or error status\n",
    "        \"\"\"\n",
    "        try:\n",
    "            symbol = inputs[0]\n",
    "            limit = inputs[1] if len(inputs) > 1 else self.DEFAULT_LIMIT\n",
    "            \n",
    "            articles = []\n",
    "            sources_queried = []\n",
    "            errors = []\n",
    "            \n",
    "            # Fetch from Yahoo Finance RSS feed\n",
    "            # Allocate half of article limit to this source\n",
    "            try:\n",
    "                yahoo_articles = self._fetch_from_yahoo_rss(symbol, limit // 2)\n",
    "                articles.extend(yahoo_articles)\n",
    "                sources_queried.append(\"yahoo_rss\")\n",
    "            except Exception as e:\n",
    "                errors.append({\"source\": \"yahoo_rss\", \"error\": str(e)})\n",
    "            \n",
    "            # Fetch from NewsAPI\n",
    "            # Allocate remaining half of article limit to this source\n",
    "            if NEWS_API_KEY:\n",
    "                try:\n",
    "                    newsapi_articles = self._fetch_from_newsapi(symbol, limit // 2)\n",
    "                    articles.extend(newsapi_articles)\n",
    "                    sources_queried.append(\"newsapi\")\n",
    "                except Exception as e:\n",
    "                    errors.append({\"source\": \"newsapi\", \"error\": str(e)})\n",
    "            else:\n",
    "                errors.append({\"source\": \"newsapi\", \"error\": \"NEWS_API_KEY not found in environment\"})\n",
    "            \n",
    "            return {\n",
    "                \"source\": \"news_aggregated\",\n",
    "                \"symbol\": symbol,\n",
    "                \"data\": {\n",
    "                    \"articles\": articles,\n",
    "                    \"sources_queried\": sources_queried,\n",
    "                    \"total_count\": len(articles)\n",
    "                },\n",
    "                \"status\": \"success\" if len(articles) > 0 else \"partial_success\",\n",
    "                \"error\": errors if len(errors) > 0 else None,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"source\": \"news_aggregated\",\n",
    "                \"symbol\": inputs[0] if len(inputs) > 0 else \"UNKNOWN\",\n",
    "                \"data\": {\n",
    "                    \"articles\": [],\n",
    "                    \"sources_queried\": [],\n",
    "                    \"total_count\": 0\n",
    "                },\n",
    "                \"status\": \"error\",\n",
    "                \"error\": str(e),\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "    \n",
    "    def _preprocess_text(self, text: str) -> str:\n",
    "        # Text preprocessing: Remove potentially malicious content and HTML\n",
    "        # Step 1: Remove script tags and their content\n",
    "        text = re.sub(r'(?is)<script.*?>.*?</script>', ' ', text)\n",
    "\n",
    "        # Remove any remaining HTML tags\n",
    "        text = re.sub(r'<[^>]+>', ' ', text)\n",
    "\n",
    "        # Remove javascript: URIs and inline event handlers like onload=, onclick= etc.\n",
    "        text = re.sub(r'(?i)javascript\\s*:', '', text)\n",
    "        text = re.sub(r'(?i)on\\w+\\s*=\\s*[\"\\'].*?[\"\\']', ' ', text)\n",
    "\n",
    "        # Remove control characters\n",
    "        text = re.sub(r'[\\x00-\\x1f\\x7f]', ' ', text)\n",
    "\n",
    "        # Unescape HTML entities then escape to ensure safe plain text\n",
    "        text = html.unescape(text)\n",
    "        text = html.escape(text)\n",
    "\n",
    "        # Collapse multiple whitespace to single space and trim\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "        return text\n",
    "    \n",
    "    def _fetch_from_newsapi(self, symbol: str, limit: int) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Fetch articles from NewsAPI.\n",
    "        \n",
    "        Args:\n",
    "            symbol: Stock ticker symbol\n",
    "            limit: Maximum number of articles\n",
    "            \n",
    "        Returns:\n",
    "            List of article dictionaries\n",
    "        \"\"\"\n",
    "        url = \"https://newsapi.org/v2/everything\"\n",
    "        params = {\n",
    "            \"q\": f\"{symbol} stock\",\n",
    "            \"language\": \"en\",\n",
    "            \"sortBy\": \"publishedAt\",\n",
    "            \"pageSize\": limit,\n",
    "            \"apiKey\": NEWS_API_KEY\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            articles = []\n",
    "            for article in data.get(\"articles\", []):\n",
    "                processed_summary = self._preprocess_text(article.get(\"description\", \"\"))   \n",
    "                articles.append({\n",
    "                    \"title\": article.get(\"title\", \"\"),\n",
    "                    \"link\": article.get(\"url\", \"\"),\n",
    "                    \"published\": article.get(\"publishedAt\", \"\"),\n",
    "                    \"summary\": processed_summary,\n",
    "                    \"source\": article.get(\"source\", {}).get(\"name\", \"NewsAPI\")\n",
    "                })\n",
    "            return articles\n",
    "        else:\n",
    "            raise Exception(f\"NewsAPI request failed with status {response.status_code}\")\n",
    "    \n",
    "    def _fetch_from_yahoo_rss(self, symbol: str, limit: int) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Fetch articles from Yahoo Finance RSS feed.\n",
    "        \n",
    "        Args:\n",
    "            symbol: Stock ticker symbol\n",
    "            limit: Maximum number of articles\n",
    "            \n",
    "        Returns:\n",
    "            List of article dictionaries\n",
    "        \"\"\"\n",
    "        yahoo_rss = f\"https://feeds.finance.yahoo.com/rss/2.0/headline?s={symbol}&region=US&lang=en-US\"\n",
    "        feed = feedparser.parse(yahoo_rss)\n",
    "        \n",
    "        articles = []\n",
    "        for entry in feed.entries[:limit]:\n",
    "            processed_summary = self._preprocess_text(entry.get(\"summary\", \"\"))\n",
    "            articles.append({\n",
    "                \"title\": entry.get(\"title\", \"\"),\n",
    "                \"link\": entry.get(\"link\", \"\"),\n",
    "                \"published\": entry.get(\"published\", \"\"),\n",
    "                \"summary\": processed_summary,\n",
    "                \"source\": \"Yahoo Finance\"\n",
    "            })\n",
    "        \n",
    "        return articles\n",
    "\n",
    "# ---------------------------\n",
    "# (step 1)\n",
    "# ingestion/ingestion.py (file 3 of 3)\n",
    "# ---------------------------\n",
    "# ==============================================================================\n",
    "# INGESTION COORDINATOR\n",
    "# ==============================================================================\n",
    "# Purpose: Orchestrate parallel data fetching from multiple sources\n",
    "# Pattern: Uses ThreadPoolExecutor for concurrent API calls\n",
    "# Responsibilities:\n",
    "#   - Execute financial and news ingestion in parallel\n",
    "#   - Combine results into unified data bundle\n",
    "#   - Handle partial failures gracefully\n",
    "#   - Track errors from each source\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"\n",
    "Main Ingestion Coordinator\n",
    "Orchestrates parallel data fetching from financial and news sources (simplified)\n",
    "\"\"\"\n",
    "\n",
    "class Ingestion(BaseWorker):\n",
    "    \"\"\"\n",
    "    Main Ingestion Coordinator that fetches data from financial and news sources in parallel.\n",
    "    \n",
    "    Responsibilities:\n",
    "    - Coordinate parallel data fetching from financial and news sources\n",
    "    - Combine results into single bundle\n",
    "    - Handle partial failures gracefully\n",
    "    - Track errors from each source\n",
    "    - Return complete data bundle\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize data ingestors.\"\"\"\n",
    "        self.financial_ingestor = FinancialDataIngestion()\n",
    "        self.news_ingestor = NewsDataIngestion()\n",
    "    \n",
    "    def execute(self, *inputs) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Execute parallel data ingestion from all sources.\n",
    "        \n",
    "        Args:\n",
    "            inputs[0] (str): Stock ticker symbol (e.g., \"AAPL\")\n",
    "            inputs[1] (str, optional): Time period for historical data (default: \"1mo\")\n",
    "            inputs[2] (int, optional): Number of news articles (default: 10)\n",
    "        \n",
    "        Returns:\n",
    "            dict: Complete data bundle with financial and news data\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract parameters\n",
    "            symbol = inputs[0] if len(inputs) > 0 else \"AAPL\"\n",
    "            period = inputs[1] if len(inputs) > 1 else \"1mo\"\n",
    "            news_limit = inputs[2] if len(inputs) > 2 else 10\n",
    "            \n",
    "            # Execute both financial and news ingestion concurrently\n",
    "            results = self._execute_parallel(symbol, period, news_limit)\n",
    "            \n",
    "            # Combine results\n",
    "            bundle = self._combine_results(symbol, results)\n",
    "            \n",
    "            return bundle\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"symbol\": inputs[0] if len(inputs) > 0 else \"UNKNOWN\",\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"financial_data\": None,\n",
    "                \"news_data\": None,\n",
    "                \"errors\": [{\"source\": \"ingestion_coordinator\", \"error\": str(e)}],\n",
    "                \"status\": \"error\"\n",
    "            }\n",
    "    \n",
    "    def _execute_parallel(self, symbol: str, period: str, news_limit: int) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Execute all ingestion tasks in parallel.\n",
    "        \n",
    "        Args:\n",
    "            symbol: Stock ticker symbol\n",
    "            period: Time period for historical data\n",
    "            news_limit: Number of news articles\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with results from all sources\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            \"financial\": None,\n",
    "            \"news\": None\n",
    "        }\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "            # Submit both ingestion tasks to thread pool for parallel execution\n",
    "            future_to_source = {\n",
    "                executor.submit(self.financial_ingestor.execute, symbol, period): \"financial\",\n",
    "                executor.submit(self.news_ingestor.execute, symbol, news_limit): \"news\"\n",
    "            }\n",
    "            \n",
    "            # Collect results as they complete\n",
    "            for future in as_completed(future_to_source):\n",
    "                source = future_to_source[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    results[source] = result\n",
    "                except Exception as e:\n",
    "                    results[source] = {\n",
    "                        \"source\": source,\n",
    "                        \"data\": None,\n",
    "                        \"status\": \"error\",\n",
    "                        \"error\": str(e),\n",
    "                        \"timestamp\": datetime.now().isoformat()\n",
    "                    }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _combine_results(self, symbol: str, results: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Combine results from all ingestors into single bundle.\n",
    "        \n",
    "        Args:\n",
    "            symbol: Stock ticker symbol\n",
    "            results: Dictionary with results from each ingestor\n",
    "            \n",
    "        Returns:\n",
    "            Combined data bundle\n",
    "        \"\"\"\n",
    "        errors = []\n",
    "        \n",
    "        # Extract financial data\n",
    "        financial_result = results.get(\"financial\", {})\n",
    "        financial_data = financial_result.get(\"data\") if financial_result.get(\"status\") == \"success\" else None\n",
    "        if financial_result.get(\"error\"):\n",
    "            errors.append({\"source\": \"financial\", \"error\": financial_result.get(\"error\")})\n",
    "        \n",
    "        # Extract news data\n",
    "        news_result = results.get(\"news\", {})\n",
    "        news_data = news_result.get(\"data\") if news_result.get(\"status\") in [\"success\", \"partial_success\"] else None\n",
    "        if news_result.get(\"error\"):\n",
    "            errors.append({\"source\": \"news\", \"error\": news_result.get(\"error\")})\n",
    "        \n",
    "        # Determine overall status\n",
    "        status = self._determine_status(results)\n",
    "        \n",
    "        return {\n",
    "            \"symbol\": symbol,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"financial_data\": financial_data,\n",
    "            \"news_data\": news_data,\n",
    "            \"errors\": errors,\n",
    "            \"status\": status\n",
    "        }\n",
    "    \n",
    "    def _determine_status(self, results: Dict[str, Any]) -> str:\n",
    "        \"\"\"\n",
    "        Determine overall ingestion status based on results.\n",
    "        \n",
    "        Args:\n",
    "            results: Dictionary with results from each ingestor\n",
    "            \n",
    "        Returns:\n",
    "            Status string: \"success\", \"partial_success\", or \"error\"\n",
    "        \"\"\"\n",
    "        success_count = 0\n",
    "        total_count = len(results)\n",
    "        \n",
    "        for source, result in results.items():\n",
    "            if result and result.get(\"status\") in [\"success\", \"partial_success\"]:\n",
    "                success_count += 1\n",
    "        \n",
    "        if success_count == total_count:\n",
    "            return \"success\"\n",
    "        elif success_count > 0:\n",
    "            return \"partial_success\"\n",
    "        else:\n",
    "            return \"error\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c32775-fd8c-469b-ba12-901703ea82c7",
   "metadata": {},
   "source": [
    "# Summarizer \n",
    "- Formats data into structured insight with routed notes and confidence scores\n",
    "- stored as summary_result[\"summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4282c88-d6bd-4620-b8fe-fe25bb90f3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# (step 2)\n",
    "# src/summarizer/summarizer.py\n",
    "# ---------------------------\n",
    "\n",
    "# --- Simple keyword routing for headlines ---\n",
    "ROUTES = {\n",
    "    \"earnings\": [\"eps\", \"guidance\", \"revenue\", \"call\", \"forecast\", \"beat\", \"miss\", \"margin\"],\n",
    "    \"macro\":    [\"fed\", \"rate\", \"cpi\", \"inflation\", \"jobs\", \"gdp\", \"unemployment\", \"yields\", \"oil\"],\n",
    "    \"company\":  [\"product\", \"launch\", \"recall\", \"supply\", \"lawsuit\", \"merger\", \"partnership\", \"contract\"],\n",
    "}\n",
    "\n",
    "def _route(text: str) -> str:\n",
    "    t = (text or \"\").lower()\n",
    "    for route, keys in ROUTES.items():\n",
    "        if any(k in t for k in keys):\n",
    "            return route\n",
    "    return \"company\"\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"You are a pragmatic equity analyst.\n",
    "Goal: {goal}\n",
    "Symbol: {symbol}\n",
    "\n",
    "Context (recent daily stats + sampled headlines):\n",
    "{context}\n",
    "\n",
    "Write 5–8 concise bullets on likely near-term price *drivers* and 2 bullets on *key risks*.\n",
    "Avoid hype; be specific. Include dates or sources inline when present.\n",
    "\"\"\"\n",
    "\n",
    "class SummarizerWorker(BaseWorker):\n",
    "    def __init__(self, name: str = \"summarizer\", role: str = \"news_summary\", model: str | None = None):\n",
    "        \"\"\"\n",
    "        Defensive init:\n",
    "        - Tries super().__init__(name=..., role=..., model=...).\n",
    "        - If parent __init__ takes no args or is missing, call it without args (if present)\n",
    "          and set attributes locally as a fallback.\n",
    "        \"\"\"\n",
    "        # Try the most specific signature first\n",
    "        try:\n",
    "            super().__init__(name=name, role=role, model=model)  # type: ignore[misc]\n",
    "        except TypeError:\n",
    "            # Parent __init__ takes no args (or doesn't define one)\n",
    "            try:\n",
    "                super().__init__()  # type: ignore[misc]\n",
    "            except Exception:\n",
    "                pass\n",
    "            # Fallback: ensure attributes exist on self\n",
    "            setattr(self, \"name\", name)\n",
    "            setattr(self, \"role\", role)\n",
    "            setattr(self, \"model\", model)\n",
    "\n",
    "    def execute(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Entry point for the summarizer agent.\n",
    "\n",
    "        Fix included:\n",
    "        - If `news_daily` is missing but `raw_news` is provided, derive a daily\n",
    "          aggregate so confidence doesn't default to 0.50.\n",
    "        - Count rows per day with groupby().size() so missing titles don't drop counts.\n",
    "        \"\"\"\n",
    "        import pandas as pd\n",
    "\n",
    "        symbol: str = inputs[\"symbol\"]\n",
    "        news_daily = inputs.get(\"news_daily\")\n",
    "        raw_news   = inputs.get(\"raw_news\")\n",
    "        window     = int(inputs.get(\"window\", 7))\n",
    "        goal       = inputs.get(\"analysis_goal\", f\"Next-week price drivers for {symbol}\")\n",
    "\n",
    "        # ---- Build news_daily from raw_news if not provided ----\n",
    "        if news_daily is None and raw_news is not None:\n",
    "            try:\n",
    "                df = raw_news if isinstance(raw_news, pd.DataFrame) else pd.DataFrame(raw_news)\n",
    "\n",
    "                # Normalize/derive a date column\n",
    "                if \"published\" in df.columns:\n",
    "                    df[\"published\"] = pd.to_datetime(df[\"published\"], errors=\"coerce\")\n",
    "                    df[\"date\"] = df[\"published\"].dt.date\n",
    "                elif \"date\" in df.columns:\n",
    "                    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.date\n",
    "                else:\n",
    "                    df[\"date\"] = pd.Timestamp.today().date()  # fallback to today if no timestamp present\n",
    "\n",
    "                # Robust daily aggregation (counts all rows, even if title is NaN)\n",
    "                news_daily = (\n",
    "                    df.groupby(\"date\")\n",
    "                      .size()\n",
    "                      .rename(\"news_count\")\n",
    "                      .to_frame()\n",
    "                      .reset_index()\n",
    "                      .assign(sent_mean=0.0, sent_decay=0.0)\n",
    "                      .set_index(\"date\")\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Could not derive news_daily from raw_news: {e}\")\n",
    "                news_daily = None\n",
    "        # --------------------------------------------------------\n",
    "\n",
    "        context = self._format_context(news_daily, raw_news, window)\n",
    "        routed  = self._route_headlines(raw_news)\n",
    "        prompt  = PROMPT_TEMPLATE.format(goal=goal, symbol=symbol, context=context)\n",
    "\n",
    "        # NOTE: Replace this stub with your actual LLM call when ready.\n",
    "        summary_text = (\n",
    "            \"(Stubbed summary — replace with your LLM call)\\n\"\n",
    "            + prompt\n",
    "            + \"\\n- Headlines cluster around a few catalysts; monitor official updates.\\n\"\n",
    "              \"- Tone is slightly positive; momentum sensitive to macro prints.\\n\"\n",
    "              \"- Risks: guidance/margin pressure; policy surprises.\"\n",
    "        )\n",
    "\n",
    "        confidence = self._confidence_from_news(news_daily, window)\n",
    "        memory_writes = [\n",
    "            f\"[{symbol}] {window}d summary (conf={confidence:.2f})\",\n",
    "            f\"[{symbol}] Routes: \" + \", \".join([k for k, v in routed.items() if v])\n",
    "        ]\n",
    "\n",
    "        return {\n",
    "            \"symbol\": symbol,\n",
    "            \"summary\": summary_text,\n",
    "            \"routed_notes\": routed,\n",
    "            \"confidence\": confidence,\n",
    "            \"artifacts\": {\"prompt\": prompt},\n",
    "            \"memory_writes\": memory_writes,\n",
    "        }\n",
    "\n",
    "    # ----------------- Helpers -----------------\n",
    "    def _format_context(\n",
    "        self,\n",
    "        news_daily,\n",
    "        raw_news: Union[List[dict], \"pd.DataFrame\", None],\n",
    "        window: int\n",
    "    ) -> str:\n",
    "        parts: List[str] = []\n",
    "\n",
    "        # Daily aggregates\n",
    "        if news_daily is not None and hasattr(news_daily, \"tail\") and len(news_daily) > 0:\n",
    "            tail = news_daily.tail(window)\n",
    "            parts.append(\"Daily sentiment (most recent first):\")\n",
    "            for idx, row in tail.iloc[::-1].iterrows():\n",
    "                parts.append(\n",
    "                    f\"- {idx}: count={int(row.get('news_count', 0))}, \"\n",
    "                    f\"sent_mean={row.get('sent_mean', 0):+.3f}, decay={row.get('sent_decay', 0):+.3f}\"\n",
    "                )\n",
    "\n",
    "        # Recent headlines\n",
    "        if raw_news is not None:\n",
    "            try:\n",
    "                import pandas as pd\n",
    "                df = raw_news if isinstance(raw_news, pd.DataFrame) else pd.DataFrame(raw_news)\n",
    "                ts = \"published\" if \"published\" in df.columns else (\"date\" if \"date\" in df.columns else None)\n",
    "                if ts:\n",
    "                    df = df.sort_values(by=ts).tail(12)\n",
    "                parts.append(\"Recent headlines:\")\n",
    "                for _, r in df.iterrows():\n",
    "                    ttl = str(r.get(\"title\", \"\"))[:160]\n",
    "                    src = r.get(\"source\", \"\") or \"news\"\n",
    "                    dt  = r.get(\"published\", r.get(\"date\", \"\"))\n",
    "                    parts.append(f\"- [{dt}] ({src}) {ttl}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        return \"\\n\".join(parts) if parts else \"No recent news.\"\n",
    "\n",
    "    def _route_headlines(self, raw_news) -> dict:\n",
    "        routed = {\"earnings\": [], \"macro\": [], \"company\": []}\n",
    "        if raw_news is None:\n",
    "            return routed\n",
    "        try:\n",
    "            import pandas as pd\n",
    "            df = raw_news if isinstance(raw_news, pd.DataFrame) else pd.DataFrame(raw_news)\n",
    "            for _, r in df.tail(50).iterrows():\n",
    "                ttl = str(r.get(\"title\", \"\")) or \"\"\n",
    "                routed[_route(ttl)].append(ttl)\n",
    "        except Exception:\n",
    "            pass\n",
    "        # Trim to a few examples per bucket\n",
    "        return {k: v[:5] for k, v in routed.items()}\n",
    "\n",
    "    def _confidence_from_news(self, news_daily, window: int) -> float:\n",
    "        \"\"\"\n",
    "        Confidence rises modestly with average daily news volume.\n",
    "        If news_daily is missing or malformed, fall back to 0.50.\n",
    "        \"\"\"\n",
    "        if news_daily is None or not hasattr(news_daily, \"tail\") or len(news_daily) == 0:\n",
    "            return 0.50\n",
    "        try:\n",
    "            avg_cnt = float(news_daily.tail(window)[\"news_count\"].mean())\n",
    "            return round(min(1.0, 0.5 + 0.05 * avg_cnt), 2)\n",
    "        except Exception:\n",
    "            return 0.50\n",
    "\n",
    "# ---------------------------\n",
    "# (step 2)\n",
    "# tests/test_summarizer.py\n",
    "# ---------------------------\n",
    "\n",
    "def _toy_news(rows=8):\n",
    "    now = datetime.now()\n",
    "    return pd.DataFrame({\n",
    "        \"title\": [f\"T{i}\" for i in range(rows-1)] + [None],  # include a None to test robust counting\n",
    "        \"source\": [\"demo\"] * rows,\n",
    "        \"published\": [now - timedelta(days=i // 2) for i in range(rows)]\n",
    "    })\n",
    "\n",
    "def test_confidence_builds_from_raw_news():\n",
    "    w = SummarizerWorker()\n",
    "    out = w.execute({\"symbol\": \"TEST\", \"raw_news\": _toy_news(8), \"window\": 7})\n",
    "    assert out[\"confidence\"] > 0.50\n",
    "\n",
    "def test_confidence_falls_back_when_no_news():\n",
    "    w = SummarizerWorker()\n",
    "    out = w.execute({\"symbol\": \"TEST\", \"raw_news\": None, \"window\": 7})\n",
    "    assert out[\"confidence\"] == 0.50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00b3074-3a35-4e1a-bc98-1ac816c3f7a7",
   "metadata": {},
   "source": [
    "# Memory Management\n",
    "\n",
    "## Overview\n",
    "The Memory module provides persistent storage for agent learnings across multiple runs. It enables the system to:\n",
    "- Store analysis results and insights\n",
    "- Retrieve historical context for subsequent queries\n",
    "- Build knowledge over time for improved decision-making\n",
    "\n",
    "## Key Features\n",
    "\n",
    "### Storage\n",
    "- **File-based persistence**: Stores memories in `data/agent_memory.json`\n",
    "- **Tagged entries**: Each memory includes tags (e.g., symbol, category) for efficient retrieval\n",
    "- **Timestamped records**: All entries include ISO-formatted timestamps for freshness tracking\n",
    "\n",
    "### Operations\n",
    "1. **add**: Store new memory entries with text and optional tags\n",
    "2. **search**: Find memories by keyword matching in text or tags\n",
    "3. **get_recent**: Retrieve the N most recent memories\n",
    "4. **retrieve_by_symbol**: Get all memories related to a specific stock symbol\n",
    "\n",
    "## Memory Structure\n",
    "```python\n",
    "{\n",
    "  \"timestamp\": \"2025-10-19T10:30:00\",\n",
    "  \"text\": \"[AAPL] Investment recommendation: Hold\",\n",
    "  \"tags\": [\"AAPL\", \"summary\", \"optimized\"]\n",
    "}\n",
    "```\n",
    "\n",
    "## Usage in Workflow\n",
    "- **After Summarization**: Store summary notes and routing information\n",
    "- **After Optimization**: Store final optimized summaries\n",
    "- **Before New Analysis**: Retrieve previous context to avoid redundant API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a047ab3e-7e66-4fda-ab0b-090118714bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# (step 3)\n",
    "# src/memory/memory.py\n",
    "# ---------------------------\n",
    "\n",
    "# ==============================================================================\n",
    "# MEMORY MANAGEMENT MODULE\n",
    "# ==============================================================================\n",
    "# Purpose: Provide persistent storage for agent learnings across runs\n",
    "# Storage: JSON file-based persistence in ./data/agent_memory.json\n",
    "# Structure: List of dictionaries with timestamp, text, and tags\n",
    "# ==============================================================================\n",
    "\n",
    "# Global in-memory store synchronized with JSON file\n",
    "_memory_store: List[Dict[str, Any]] = []\n",
    "MEMORY_FILE = \"./data/agent_memory.json\"\n",
    "\n",
    "def _load_memory():\n",
    "    \"\"\"\n",
    "    Load memories from JSON file into the in-memory list.\n",
    "    Called automatically when module is imported.\n",
    "    Handles missing files and JSON decode errors gracefully.\n",
    "    \"\"\"\n",
    "    global _memory_store\n",
    "    if not os.path.exists(MEMORY_FILE):\n",
    "        _memory_store = []\n",
    "        return\n",
    "    try:\n",
    "        with open(MEMORY_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            _memory_store = json.load(f)\n",
    "    except (json.JSONDecodeError, FileNotFoundError):\n",
    "        _memory_store = []\n",
    "\n",
    "def _save_memory():\n",
    "    \"\"\"\n",
    "    Save the in-memory list to JSON file for persistence.\n",
    "    Creates directory if it doesn't exist.\n",
    "    Called after each add operation.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(MEMORY_FILE), exist_ok=True)\n",
    "    with open(MEMORY_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(_memory_store, f, indent=2)\n",
    "\n",
    "# Load memory when module is imported\n",
    "_load_memory()\n",
    "\n",
    "class MemoryWorker(BaseWorker):\n",
    "    \"\"\"\n",
    "    Worker for managing agent memories with persistent storage.\n",
    "    \n",
    "    A BaseWorker implementation for managing agent memories. It stores short text\n",
    "    memories with optional tags in a JSON file so the agent can learn across runs.\n",
    "    \"\"\"\n",
    "    def execute(self, *inputs) -> Any:\n",
    "        \"\"\"\n",
    "        Manages agent memories. The first input is the operation ('add', 'search', 'get_recent', 'retrieve_by_symbol').\n",
    "\n",
    "        Usage:\n",
    "            - execute('add', 'some memory text', ['tag1', 'tag2'])\n",
    "            - execute('search', 'query text')\n",
    "            - execute('get_recent', 5)\n",
    "            - execute('retrieve_by_symbol', 'AAPL')\n",
    "        \"\"\"\n",
    "        if not inputs:\n",
    "            raise ValueError(\"MemoryWorker requires at least one input for the operation.\")\n",
    "\n",
    "        operation = inputs[0]\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # ADD OPERATION: Store new memory entry\n",
    "        # ---------------------------------------------------------------------\n",
    "        if operation == 'add':\n",
    "            if len(inputs) < 2:\n",
    "                raise ValueError(\"The 'add' operation requires text for the memory.\")\n",
    "            text = inputs[1]\n",
    "            tags = inputs[2] if len(inputs) > 2 else []\n",
    "            # Create memory entry with ISO-formatted UTC timestamp\n",
    "            entry = {\n",
    "                \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "                \"text\": text,\n",
    "                \"tags\": tags,\n",
    "            }\n",
    "            _memory_store.append(entry)\n",
    "            _save_memory()\n",
    "            return entry\n",
    "        \n",
    "        # ---------------------------------------------------------------------\n",
    "        # SEARCH OPERATION: Find memories by keyword matching\n",
    "        # ---------------------------------------------------------------------\n",
    "        elif operation == 'search':\n",
    "            if len(inputs) < 2:\n",
    "                raise ValueError(\"The 'search' operation requires a query string.\")\n",
    "            query = inputs[1].lower()\n",
    "            top_k = inputs[2] if len(inputs) > 2 else 5\n",
    "            \n",
    "            # Search in reverse chronological order (most recent first)\n",
    "            # Match query against text content or tags (case-insensitive)\n",
    "            matches = [\n",
    "                m for m in reversed(_memory_store) \n",
    "                if query in m['text'].lower() or any(query in t.lower() for t in m.get('tags', []))\n",
    "            ]\n",
    "            return matches[:top_k]\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # GET_RECENT OPERATION: Retrieve N most recent memories\n",
    "        # ---------------------------------------------------------------------\n",
    "        elif operation == 'get_recent':\n",
    "            n = inputs[1] if len(inputs) > 1 else 5\n",
    "            return list(reversed(_memory_store))[:n]\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # RETRIEVE_BY_SYMBOL OPERATION: Get all memories for a stock symbol\n",
    "        # ---------------------------------------------------------------------\n",
    "        elif operation == 'retrieve_by_symbol':\n",
    "            if len(inputs) < 2:\n",
    "                raise ValueError(\"The 'retrieve_by_symbol' operation requires a symbol string.\")\n",
    "            symbol = inputs[1].upper()  # Normalize to uppercase for consistency\n",
    "            \n",
    "            # Filter records by symbol (case-insensitive)\n",
    "            # Matches if symbol appears in tags or text content\n",
    "            matches = [\n",
    "                m for m in _memory_store\n",
    "                if symbol in [tag.upper() for tag in m.get('tags', [])] or symbol in m.get('text', '').upper()\n",
    "            ]\n",
    "            return matches\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown operation: {operation}. Available operations: 'add', 'search', 'get_recent', 'retrieve_by_symbol'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd32208-5143-4ade-a759-52d608c46854",
   "metadata": {},
   "source": [
    "# Evaluator-optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fccc6dbd-dcc8-47aa-9afa-351f05f2fea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# (step 4)\n",
    "# evaluator_optimizer/init.py\n",
    "# ---------------------------\n",
    "\n",
    "\"\"\"\n",
    "Evaluator-Optimizer Module\n",
    "\n",
    "Implements the Evaluator-Optimizer workflow pattern for iterative refinement\n",
    "of investment research summaries.\n",
    "\"\"\"\n",
    "\n",
    "__all__ = [\"EvaluatorOptimizer\", \"Feedback\", \"State\"]\n",
    "\n",
    "# ---------------------------\n",
    "# (step 4)\n",
    "# evaluator_optimizer/evaluator_optimizer.py\n",
    "# ---------------------------\n",
    "\n",
    "\"\"\"\n",
    "Evaluator-Optimizer Module\n",
    "\n",
    "Implements the Evaluator-Optimizer workflow pattern where:\n",
    "1. Generator creates an investment research summary\n",
    "2. Evaluator assesses quality and provides feedback\n",
    "3. Loop continues until quality passes or max iterations reached\n",
    "4. Final summary is stored in memory\n",
    "\n",
    "Based on LangGraph pattern: https://langchain-ai.github.io/langgraph/tutorials/workflows/#evaluator-optimizer\n",
    "\"\"\"\n",
    "\n",
    "# --- State Definition ---\n",
    "class State(TypedDict):\n",
    "    \"\"\"Graph state for the Evaluator-Optimizer workflow\"\"\"\n",
    "    symbol: str  # Stock symbol being analyzed\n",
    "    instructions: str  # User's request/query about the stock\n",
    "    context: Dict[str, Any]  # Financial data context (news, prices, etc.)\n",
    "    summary: str  # Current investment research summary\n",
    "    feedback: str  # Feedback from evaluator\n",
    "    grade: str  # Quality grade: \"pass\" or \"fail\"\n",
    "    quality_score: float  # Numeric quality score (0-10)\n",
    "    issues: List[str]  # List of identified issues\n",
    "    iteration: int  # Current iteration number\n",
    "    max_iterations: int  # Maximum allowed iterations\n",
    "    history: List[Dict[str, Any]]  # History of iterations for tracking\n",
    "\n",
    "# --- Structured Output Schema for Evaluation ---\n",
    "class Feedback(BaseModel):\n",
    "    \"\"\"Structured evaluation feedback from the Evaluator\"\"\"\n",
    "    \n",
    "    grade: Literal[\"pass\", \"fail\"] = Field(\n",
    "        description=\"Overall quality assessment: 'pass' if summary meets quality criteria, 'fail' otherwise\"\n",
    "    )\n",
    "    quality_score: float = Field(\n",
    "        description=\"Numeric quality score from 0-10, where 10 is excellent\",\n",
    "        ge=0.0,\n",
    "        le=10.0\n",
    "    )\n",
    "    feedback: str = Field(\n",
    "        description=\"Detailed, actionable feedback for improving the summary if grade is 'fail'\"\n",
    "    )\n",
    "    issues: List[str] = Field(\n",
    "        description=\"List of specific issues identified in the summary\",\n",
    "        default_factory=list\n",
    "    )\n",
    "\n",
    "# --- Evaluator-Optimizer Class ---\n",
    "class EvaluatorOptimizer(BaseWorker):\n",
    "    \"\"\"\n",
    "    Implements the Evaluator-Optimizer workflow pattern for investment research summaries.\n",
    "    \n",
    "    The workflow:\n",
    "    1. Generator creates an initial summary from context data\n",
    "    2. Evaluator assesses quality against defined criteria\n",
    "    3. If quality fails, provides feedback and loops back to Generator\n",
    "    4. Continues until quality passes or max iterations reached\n",
    "    5. Returns final optimized summary\n",
    "    \"\"\"\n",
    "    \n",
    "    # Quality criteria for investment research summaries\n",
    "    QUALITY_CRITERIA = \"\"\"\n",
    "    A high-quality investment research summary should:\n",
    "    1. COMPLETENESS: Cover key financial metrics, sentiment analysis, and risk factors\n",
    "    2. CLARITY: Be well-structured, concise, and easy to understand\n",
    "    3. ACTIONABILITY: Include a clear investment recommendation (buy/sell/hold) with rationale\n",
    "    4. EVIDENCE-BASED: Back claims with specific data from news and financial metrics\n",
    "    5. COHERENCE: Have logical flow without contradictions\n",
    "    6. RISK AWARENESS: Acknowledge both opportunities and risks\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm: Optional[ChatOpenAI] = None, max_iterations: int = 3):\n",
    "        \"\"\"\n",
    "        Initialize the Evaluator-Optimizer.\n",
    "        \n",
    "        Args:\n",
    "            llm: Language model instance (if None, initializes OpenAI LLM from environment)\n",
    "            max_iterations: Maximum refinement iterations before stopping\n",
    "        \"\"\"\n",
    "        # Initialize LLM\n",
    "        if llm is not None:\n",
    "            self.llm = llm\n",
    "        else:\n",
    "            self.llm = self._initialize_openai_llm()\n",
    "        \n",
    "        self.max_iterations = max_iterations\n",
    "        \n",
    "        # Create structured output LLM for evaluation\n",
    "        self.evaluator_llm = self.llm.with_structured_output(Feedback)\n",
    "        \n",
    "        # Build the workflow graph\n",
    "        self.workflow = self._build_workflow()\n",
    "    \n",
    "    def _initialize_openai_llm(self) -> ChatOpenAI:\n",
    "        \"\"\"Initialize OpenAI LLM with API key from environment\"\"\"\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        llm = ChatOpenAI(\n",
    "            model=\"gpt-4o-mini\",  # Using gpt-4o-mini for cost efficiency\n",
    "            temperature=0.7,\n",
    "            api_key=api_key\n",
    "        )\n",
    "        return llm\n",
    "    \n",
    "    def _build_workflow(self) -> StateGraph:\n",
    "        \"\"\"Builds the LangGraph workflow for Evaluator-Optimizer pattern\"\"\"\n",
    "        \n",
    "        # Create the graph\n",
    "        builder = StateGraph(State)\n",
    "        \n",
    "        # Add nodes\n",
    "        builder.add_node(\"generator\", self._generator_node)\n",
    "        builder.add_node(\"evaluator\", self._evaluator_node)\n",
    "        \n",
    "        # Add edges\n",
    "        builder.add_edge(START, \"generator\")\n",
    "        builder.add_edge(\"generator\", \"evaluator\")\n",
    "        \n",
    "        # Conditional edge: loop back to generator or end\n",
    "        builder.add_conditional_edges(\n",
    "            \"evaluator\",\n",
    "            self._should_continue,\n",
    "            {\n",
    "                \"continue\": \"generator\",  # Loop back with feedback\n",
    "                \"end\": END  # Quality passed or max iterations reached\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Compile the workflow\n",
    "        return builder.compile()\n",
    "    \n",
    "    def _generator_node(self, state: State) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generator Node: Creates or refines the investment research summary.\n",
    "        \n",
    "        On first iteration: creates initial summary from context\n",
    "        On subsequent iterations: refines summary based on evaluator feedback\n",
    "        \"\"\"\n",
    "        symbol = state[\"symbol\"]\n",
    "        context = state[\"context\"]\n",
    "        instructions = state.get(\"instructions\", \"\")\n",
    "        feedback = state.get(\"feedback\", \"\")\n",
    "        iteration = state.get(\"iteration\", 0)\n",
    "        \n",
    "        # Format the context data\n",
    "        formatted_context = self._format_context(context)\n",
    "        \n",
    "        # Build the prompt\n",
    "        if iteration == 0:\n",
    "            # Initial summary generation\n",
    "            prompt = f\"\"\"You are an expert financial analyst. Create a comprehensive investment research summary for {symbol}.\n",
    "\n",
    "USER REQUEST:\n",
    "{instructions}\n",
    "\n",
    "AVAILABLE DATA:\n",
    "{formatted_context}\n",
    "\n",
    "QUALITY REQUIREMENTS:\n",
    "{self.QUALITY_CRITERIA}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Directly address the user's request/questions in your analysis\n",
    "2. Use the provided financial data, news, and market context to support your analysis\n",
    "3. If the user asked specific questions, answer them explicitly\n",
    "4. If data is missing or unavailable, acknowledge it and work with what's available\n",
    "5. Provide a clear investment recommendation (buy/sell/hold) with detailed rationale\n",
    "6. Structure your response professionally with clear sections\n",
    "7. Back all claims with specific data points from the context\n",
    "\n",
    "Generate a well-structured investment research summary that meets all quality criteria and addresses the user's request.\"\"\"\n",
    "        else:\n",
    "            # Refinement based on feedback\n",
    "            current_summary = state[\"summary\"]\n",
    "            prompt = f\"\"\"You are refining an investment research summary for {symbol}. \n",
    "\n",
    "USER REQUEST:\n",
    "{instructions}\n",
    "\n",
    "CURRENT SUMMARY:\n",
    "{current_summary}\n",
    "\n",
    "EVALUATOR FEEDBACK:\n",
    "{feedback}\n",
    "\n",
    "ISSUES IDENTIFIED:\n",
    "{', '.join(state.get('issues', []))}\n",
    "\n",
    "AVAILABLE DATA:\n",
    "{formatted_context}\n",
    "\n",
    "QUALITY REQUIREMENTS:\n",
    "{self.QUALITY_CRITERIA}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Address all issues identified by the evaluator\n",
    "2. Ensure the user's original request is still fully addressed\n",
    "3. Improve clarity, completeness, and actionability\n",
    "4. Add missing data points or analysis where needed\n",
    "5. Maintain professional structure and tone\n",
    "\n",
    "Generate an improved version that addresses all feedback and meets quality standards.\"\"\"\n",
    "        \n",
    "        # Generate summary\n",
    "        response = self.llm.invoke(prompt)\n",
    "        new_summary = response.content\n",
    "        \n",
    "        # Update iteration count\n",
    "        new_iteration = iteration + 1\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"GENERATOR - Iteration {new_iteration}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"User Request: {instructions[:80]}...\")\n",
    "        print(f\"Summary generated ({len(new_summary)} characters)\")\n",
    "        if iteration > 0:\n",
    "            print(f\"Addressing feedback: {feedback[:100]}...\")\n",
    "        \n",
    "        return {\n",
    "            \"summary\": new_summary,\n",
    "            \"iteration\": new_iteration\n",
    "        }\n",
    "    \n",
    "    def _evaluator_node(self, state: State) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Evaluator Node: Assesses the quality of the summary and provides feedback.\n",
    "        \n",
    "        Uses structured output to return:\n",
    "        - grade: \"pass\" or \"fail\"\n",
    "        - quality_score: 0-10\n",
    "        - feedback: actionable improvement suggestions\n",
    "        - issues: specific problems identified\n",
    "        \"\"\"\n",
    "        summary = state[\"summary\"]\n",
    "        symbol = state[\"symbol\"]\n",
    "        instructions = state.get(\"instructions\", \"\")\n",
    "        iteration = state[\"iteration\"]\n",
    "        \n",
    "        # Evaluation prompt\n",
    "        prompt = f\"\"\"You are a senior financial analyst evaluating an investment research summary for {symbol}.\n",
    "\n",
    "USER'S ORIGINAL REQUEST:\n",
    "{instructions}\n",
    "\n",
    "SUMMARY TO EVALUATE:\n",
    "{summary}\n",
    "\n",
    "EVALUATION CRITERIA:\n",
    "{self.QUALITY_CRITERIA}\n",
    "\n",
    "ASSESSMENT REQUIREMENTS:\n",
    "1. Does the summary directly address the user's request/questions?\n",
    "2. Is the analysis backed by specific data points?\n",
    "3. Are all quality criteria met (completeness, clarity, actionability, evidence-based, coherence, risk awareness)?\n",
    "4. Is the investment recommendation clear and well-justified?\n",
    "5. Are there any contradictions or unsupported claims?\n",
    "6. Is the structure professional and easy to follow?\n",
    "\n",
    "Provide:\n",
    "- grade: \"pass\" if the summary meets professional standards and addresses the user's request, \"fail\" if significant improvements needed\n",
    "- quality_score: 0-10 (be generous with 7+ for good work, reserve 9+ for exceptional analysis)\n",
    "- feedback: Specific, actionable suggestions for improvement (if grade is \"fail\")\n",
    "- issues: List specific problems (missing data, unclear reasoning, unanswered questions, etc.)\n",
    "\n",
    "Be fair but thorough. A passing grade means the summary is publication-ready and fully addresses the user's needs.\"\"\"\n",
    "        \n",
    "        # Get structured evaluation\n",
    "        evaluation = self.evaluator_llm.invoke(prompt)\n",
    "        \n",
    "        # Track iteration history\n",
    "        history = state.get(\"history\", [])\n",
    "        history.append({\n",
    "            \"iteration\": iteration,\n",
    "            \"summary_length\": len(summary),\n",
    "            \"grade\": evaluation.grade,\n",
    "            \"quality_score\": evaluation.quality_score,\n",
    "            \"issues_count\": len(evaluation.issues)\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"EVALUATOR - Iteration {iteration}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Grade: {evaluation.grade.upper()}\")\n",
    "        print(f\"Quality Score: {evaluation.quality_score}/10\")\n",
    "        print(f\"Issues Found: {len(evaluation.issues)}\")\n",
    "        if evaluation.issues:\n",
    "            for i, issue in enumerate(evaluation.issues, 1):\n",
    "                print(f\"  {i}. {issue}\")\n",
    "        print(f\"Feedback: {evaluation.feedback[:150]}...\")\n",
    "        \n",
    "        return {\n",
    "            \"grade\": evaluation.grade,\n",
    "            \"quality_score\": evaluation.quality_score,\n",
    "            \"feedback\": evaluation.feedback,\n",
    "            \"issues\": evaluation.issues,\n",
    "            \"history\": history\n",
    "        }\n",
    "    \n",
    "    def _should_continue(self, state: State) -> Literal[\"continue\", \"end\"]:\n",
    "        \"\"\"\n",
    "        Conditional routing: decide whether to continue refinement or end.\n",
    "        \n",
    "        Continue if:\n",
    "        - Grade is \"fail\" AND\n",
    "        - Haven't reached max iterations\n",
    "        \n",
    "        End if:\n",
    "        - Grade is \"pass\" OR\n",
    "        - Max iterations reached\n",
    "        \"\"\"\n",
    "        grade = state[\"grade\"]\n",
    "        iteration = state[\"iteration\"]\n",
    "        max_iterations = state[\"max_iterations\"]\n",
    "        \n",
    "        if grade == \"pass\":\n",
    "            print(f\"\\nQuality PASSED - Ending optimization\")\n",
    "            return \"end\"\n",
    "        elif iteration >= max_iterations:\n",
    "            print(f\"\\nMax iterations ({max_iterations}) reached - Ending optimization\")\n",
    "            return \"end\"\n",
    "        else:\n",
    "            print(f\"\\nQuality FAILED - Continuing to iteration {iteration + 1}\")\n",
    "            return \"continue\"\n",
    "    \n",
    "    def _format_context(self, context: Dict[str, Any]) -> str:\n",
    "        \"\"\"Formats context data into a readable string for prompts\"\"\"\n",
    "        formatted = []\n",
    "        \n",
    "        # Helper function to safely format numeric values\n",
    "        def safe_format(value, format_str=\".2f\", prefix=\"\", suffix=\"\"):\n",
    "            \"\"\"Safely format a value that might be None\"\"\"\n",
    "            if value is None:\n",
    "                return \"N/A\"\n",
    "            try:\n",
    "                return f\"{prefix}{value:{format_str}}{suffix}\"\n",
    "            except (ValueError, TypeError):\n",
    "                return \"N/A\"\n",
    "        \n",
    "        # Format financial data with key metrics\n",
    "        if \"financial_data\" in context:\n",
    "            fin_data = context[\"financial_data\"]\n",
    "            \n",
    "            # Handle memory-based data (different structure)\n",
    "            if fin_data and fin_data.get(\"source\") == \"memory\":\n",
    "                formatted.append(\"=== FINANCIAL METRICS (FROM MEMORY) ===\")\n",
    "                formatted.append(fin_data.get(\"summary\", \"No summary available\")[:1000])\n",
    "            else:\n",
    "                # Handle fresh ingestion data\n",
    "                formatted.append(\"=== FINANCIAL METRICS ===\")\n",
    "                formatted.append(f\"Symbol: {fin_data.get('symbol', 'N/A')}\")\n",
    "                formatted.append(f\"Current Price: {safe_format(fin_data.get('current_price'), '.2f', '$')}\")\n",
    "                formatted.append(f\"Price Change: {safe_format(fin_data.get('price_change'), '.2f', '$')} ({safe_format(fin_data.get('price_change_pct'), '.2f', '', '%')})\")\n",
    "                formatted.append(f\"Volume: {safe_format(fin_data.get('volume'), ',')}\")\n",
    "                formatted.append(f\"Market Cap: {safe_format(fin_data.get('market_cap'), ',', '$')}\")\n",
    "                formatted.append(f\"P/E Ratio: {safe_format(fin_data.get('pe_ratio'), '.2f')}\")\n",
    "                formatted.append(f\"Dividend Yield: {safe_format(fin_data.get('dividend_yield'), '.2f', '', '%')}\")\n",
    "                formatted.append(f\"52-Week Range: {safe_format(fin_data.get('52_week_low'), '.2f', '$')} - {safe_format(fin_data.get('52_week_high'), '.2f', '$')}\")\n",
    "                formatted.append(f\"Beta: {safe_format(fin_data.get('beta'), '.3f')}\")\n",
    "                formatted.append(f\"Sector: {fin_data.get('sector', 'N/A')}\")\n",
    "                formatted.append(f\"Industry: {fin_data.get('industry', 'N/A')}\")\n",
    "                \n",
    "                # Add volatility if available\n",
    "                if 'volatility_30d' in fin_data and fin_data['volatility_30d'] is not None:\n",
    "                    formatted.append(f\"30-Day Volatility: {fin_data['volatility_30d']:.2%}\")\n",
    "                \n",
    "                # Add company summary if available\n",
    "                if 'company_summary' in fin_data and fin_data['company_summary']:\n",
    "                    formatted.append(f\"\\nCompany Overview: {fin_data['company_summary'][:300]}...\")\n",
    "                \n",
    "                # Add recent price trends from historical data\n",
    "                if 'historical_data' in fin_data and fin_data['historical_data']:\n",
    "                    hist = fin_data['historical_data']\n",
    "                    if len(hist) >= 5:\n",
    "                        formatted.append(\"\\nRecent Price Trend (Last 5 Days):\")\n",
    "                        for i, day in enumerate(hist[-5:], 1):\n",
    "                            close_price = safe_format(day.get('Close'), '.2f', '$')\n",
    "                            volume = safe_format(day.get('Volume'), ',')\n",
    "                            formatted.append(f\"  Day {i}: Close={close_price}, Volume={volume}\")\n",
    "        \n",
    "        # Format news data\n",
    "        if \"news_data\" in context:\n",
    "            news = context[\"news_data\"]\n",
    "            \n",
    "            # Handle memory-based news\n",
    "            if isinstance(news, dict) and news.get(\"source\") == \"memory\":\n",
    "                formatted.append(f\"\\n=== NEWS ANALYSIS (FROM MEMORY) ===\")\n",
    "                formatted.append(\"Using cached news data from previous analysis\")\n",
    "            elif isinstance(news, dict) and \"articles\" in news:\n",
    "                articles = news[\"articles\"]\n",
    "                total = news.get(\"total_count\", 0)\n",
    "                formatted.append(f\"\\n=== NEWS ANALYSIS ===\")\n",
    "                formatted.append(f\"Total Articles Found: {total}\")\n",
    "                \n",
    "                if articles and len(articles) > 0:\n",
    "                    formatted.append(\"\\nRecent Headlines:\")\n",
    "                    for i, article in enumerate(articles[:5], 1):\n",
    "                        title = article.get('title', 'No title')\n",
    "                        date = article.get('publishedAt', article.get('published', article.get('date', 'N/A')))\n",
    "                        formatted.append(f\"  {i}. {title} ({date})\")\n",
    "                else:\n",
    "                    formatted.append(\"No recent news articles available.\")\n",
    "            else:\n",
    "                formatted.append(\"\\n=== NEWS ANALYSIS ===\")\n",
    "                formatted.append(\"News data format not recognized or unavailable.\")\n",
    "        \n",
    "        # Format sentiment if available\n",
    "        if \"sentiment\" in context:\n",
    "            formatted.append(f\"\\n=== SENTIMENT ANALYSIS ===\")\n",
    "            formatted.append(f\"Overall Sentiment: {context['sentiment']}\")\n",
    "        \n",
    "        # Add any errors encountered\n",
    "        if \"errors\" in context and context[\"errors\"]:\n",
    "            formatted.append(f\"\\n=== DATA COLLECTION NOTES ===\")\n",
    "            for error in context[\"errors\"]:\n",
    "                source = error.get(\"source\", \"unknown\")\n",
    "                error_detail = error.get(\"error\", error.get(\"note\", \"Unknown error\"))\n",
    "                formatted.append(f\"Note: {source} - {error_detail}\")\n",
    "        \n",
    "        # Add timestamp\n",
    "        if \"timestamp\" in context:\n",
    "            formatted.append(f\"\\nData Retrieved: {context['timestamp']}\")\n",
    "        \n",
    "        return \"\\n\".join(formatted) if formatted else \"No context data available\"\n",
    "\n",
    "    def execute(self, symbol: str, context: Dict[str, Any], instructions: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Execute the Evaluator-Optimizer workflow.\n",
    "        \n",
    "        Args:\n",
    "            symbol: Stock symbol to analyze\n",
    "            context: Dictionary containing financial data, news, sentiment, etc.\n",
    "            instructions: User's request/query (e.g., \"Should I buy AAPL?\" or \"Analyze the potential for AAPL stock\")\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with:\n",
    "                - final_summary: The optimized summary\n",
    "                - quality_score: Final quality score\n",
    "                - iterations: Number of iterations performed\n",
    "                - history: Detailed iteration history\n",
    "                - passed: Whether quality criteria were met\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'#'*60}\")\n",
    "        print(f\"EVALUATOR-OPTIMIZER WORKFLOW\")\n",
    "        print(f\"Symbol: {symbol}\")\n",
    "        print(f\"User Request: {instructions}\")\n",
    "        print(f\"Max Iterations: {self.max_iterations}\")\n",
    "        print(f\"{'#'*60}\")\n",
    "        \n",
    "        # Initialize state\n",
    "        initial_state = {\n",
    "            \"symbol\": symbol,\n",
    "            \"instructions\": instructions,\n",
    "            \"context\": context,\n",
    "            \"summary\": summary_result[\"summary\"], # Add initial summary from summarizer\n",
    "            \"feedback\": \"\",\n",
    "            \"grade\": \"\",\n",
    "            \"quality_score\": 0.0,\n",
    "            \"issues\": [],\n",
    "            \"iteration\": 0,\n",
    "            \"max_iterations\": self.max_iterations,\n",
    "            \"history\": []\n",
    "        }\n",
    "        \n",
    "        # Run the workflow\n",
    "        final_state = self.workflow.invoke(initial_state)\n",
    "        \n",
    "        # Prepare results\n",
    "        results = {\n",
    "            \"final_summary\": final_state[\"summary\"],\n",
    "            \"quality_score\": final_state[\"quality_score\"],\n",
    "            \"iterations\": final_state[\"iteration\"],\n",
    "            \"history\": final_state[\"history\"],\n",
    "            \"passed\": final_state[\"grade\"] == \"pass\",\n",
    "            \"final_grade\": final_state[\"grade\"],\n",
    "            \"issues\": final_state[\"issues\"]\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{'#'*60}\")\n",
    "        print(f\"WORKFLOW COMPLETE\")\n",
    "        print(f\"{'#'*60}\")\n",
    "        print(f\"Total Iterations: {results['iterations']}\")\n",
    "        print(f\"Final Grade: {results['final_grade'].upper()}\")\n",
    "        print(f\"Final Quality Score: {results['quality_score']}/10\")\n",
    "        print(f\"Quality Passed: {' YES' if results['passed'] else ' NO'}\")\n",
    "        print(f\"{'#'*60}\\n\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def visualize(self, output_path: str = \"evaluator_optimizer_graph.png\"):\n",
    "        \"\"\"\n",
    "        Visualize the workflow graph.\n",
    "        \n",
    "        Args:\n",
    "            output_path: Path to save the graph image\n",
    "        \"\"\"\n",
    "        try:\n",
    "            img_data = self.workflow.get_graph().draw_mermaid_png()\n",
    "            with open(output_path, \"wb\") as f:\n",
    "                f.write(img_data)\n",
    "            print(f\"Workflow graph saved to: {output_path}\")\n",
    "            return Image(img_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not generate graph visualization: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e4a45b-ef90-44f3-94ac-3d37f2ca6c8b",
   "metadata": {},
   "source": [
    "# Orchestrator\n",
    "- initialize workers\n",
    "- planning - execute workers in order\n",
    "- routing - pass outputs from one worker to the next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a15e14a3-5cc7-4c5d-a7fb-82a830347ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Orchestrator Module ---\n",
    "class Orchestrator:\n",
    "    \"\"\"\n",
    "    Coordinates the workflow of all workers:\n",
    "    1. Retrieve memory for symbol\n",
    "    2. Analyze if memory is sufficient using MemoryAnalyzer\n",
    "    3. If sufficient: use cached data, else: fetch fresh data via Ingestion\n",
    "    4. Summarizer: generate summary from data\n",
    "    5. Memory: store summary and metadata\n",
    "    6. EvaluatorOptimizer: refine the summary iteratively\n",
    "    \"\"\"\n",
    "\n",
    "    # -----------------------------\n",
    "    # Worker Initialization\n",
    "    # -----------------------------\n",
    "    def __init__(self):\n",
    "        self.workers = {\n",
    "            \"analyzer\": MemoryAnalyzer(),\n",
    "            \"ingestion\": Ingestion(),\n",
    "            \"summarizer\": SummarizerWorker(),\n",
    "            \"memory\": MemoryWorker(),\n",
    "            \"evaluator_optimizer\": EvaluatorOptimizer()\n",
    "        }\n",
    "\n",
    "    # -----------------------------\n",
    "    # Planning and Routing Execution\n",
    "    # -----------------------------\n",
    "    def execute(self, symbol: str, instructions: str) -> str:\n",
    "        \"\"\"Runs the full workflow and returns a formatted Markdown summary\"\"\"\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\" ORCHESTRATOR STARTING\")\n",
    "        print(f\"Symbol: {symbol}\")\n",
    "        print(f\"Query: {instructions}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "        # --- Step 1: Retrieve previous memories for this symbol ---\n",
    "        previous_memories = self.workers[\"memory\"].execute(\"retrieve_by_symbol\", symbol)\n",
    "        print(f\" Retrieved {len(previous_memories)} previous memory entries for {symbol}\")\n",
    "        \n",
    "        # --- Step 2: Format memories into a snapshot for analysis ---\n",
    "        memory_snapshot = self._format_memory_snapshot(previous_memories, symbol)\n",
    "        \n",
    "        # --- Step 3: Analyze if existing memory is sufficient ---\n",
    "        \n",
    "        print(f\"\\n Analyzing memory sufficiency...\")\n",
    "        analysis_result = self.workers[\"analyzer\"].execute(memory_snapshot, instructions)\n",
    "        \n",
    "        print(f\"Analysis Result:\")\n",
    "        print(f\"  - Data Sufficient: {analysis_result['is_sufficient']}\")\n",
    "        print(f\"  - Requires Fresh Data: {analysis_result['requires_fresh_data']}\")\n",
    "        if analysis_result['missing_information']:\n",
    "            print(f\"  - Missing Info: {', '.join(analysis_result['missing_information'][:3])}\")\n",
    "        \n",
    "        # --- Step 4: Decide whether to use memory or fetch fresh data ---\n",
    "        # Create memory snapshot if memory is sufficient\n",
    "        if analysis_result['is_sufficient']:\n",
    "            print(f\"\\n Using existing memory data (sufficient)\")\n",
    "            # Extract the most recent optimized summary from memory\n",
    "            ingestion_result = self._extract_from_memory(previous_memories, symbol)\n",
    "            summary_result = {\n",
    "                \"symbol\": symbol,\n",
    "                \"summary\": memory_snapshot,\n",
    "                \"routed_notes\": {},\n",
    "                \"confidence\": 0.85,\n",
    "                \"artifacts\": {\"source\": \"memory\"},\n",
    "                \"memory_writes\": []\n",
    "            }\n",
    "            \n",
    "        else:\n",
    "            # Run Ingestion and Summarizer if memory is insufficient\n",
    "            print(f\"\\n Fetching fresh data (memory insufficient)\")\n",
    "            \n",
    "            # --- Step 4a: Fetch financial and news data ---\n",
    "            ingestion_result = self.workers[\"ingestion\"].execute(symbol)\n",
    "            \n",
    "            news_articles = (ingestion_result.get(\"news_data\") or {}).get(\"articles\", [])\n",
    "            \n",
    "            # --- Step 4b: Summarize news ---\n",
    "            summary_result = self.workers[\"summarizer\"].execute({\n",
    "                \"symbol\": symbol,\n",
    "                \"raw_news\": news_articles,\n",
    "                \"window\": 7,\n",
    "                \"analysis_goal\": instructions\n",
    "            })\n",
    "            \n",
    "            # --- Step 4c: Store summary in memory ---\n",
    "            for note in summary_result.get(\"memory_writes\", []):\n",
    "                self.workers[\"memory\"].execute(\"add\", note, [symbol, \"summary\"])\n",
    "\n",
    "        # --- Step 5: Evaluator-Optimizer Workflow ---\n",
    "        evaluator = self.workers[\"evaluator_optimizer\"]\n",
    "        initial_state = {\n",
    "            \"symbol\": symbol,\n",
    "            \"instructions\": instructions,\n",
    "            \"context\": ingestion_result,\n",
    "            \"summary\": summary_result[\"summary\"],\n",
    "            \"feedback\": \"\",\n",
    "            \"grade\": \"\",\n",
    "            \"quality_score\": 0.0,\n",
    "            \"issues\": [],\n",
    "            \"iteration\": 0,\n",
    "            \"max_iterations\": evaluator.max_iterations,\n",
    "            \"history\": []\n",
    "        }\n",
    "\n",
    "        final_result = evaluator.workflow.invoke(initial_state)\n",
    "\n",
    "        # --- Step 6: Store final optimized summary in memory ---\n",
    "        final_summary_text = final_result[\"summary\"]\n",
    "        memory_note = f\"[{symbol}] Final Optimized Summary (Query: {instructions[:50]}...)\"\n",
    "        \n",
    "        # Create a structured memory entry with timestamp\n",
    "        timestamp = datetime.now().isoformat()\n",
    "        full_memory_entry = f\"{memory_note}\\nTimestamp: {timestamp}\\n\\n{final_summary_text}\"\n",
    "        \n",
    "        self.workers[\"memory\"].execute(\"add\", full_memory_entry, [symbol, \"summary\", \"optimized\"])\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\" ORCHESTRATOR COMPLETE\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "        return final_summary_text  # returns plain Markdown string\n",
    "\n",
    "    # -----------------------------\n",
    "    # Helper Methods\n",
    "    # -----------------------------\n",
    "    def _format_memory_snapshot(self, memories: List[Dict[str, Any]], symbol: str) -> str:\n",
    "        \"\"\"\n",
    "        Format an array of memory records into a single string snapshot for analysis.\n",
    "        \n",
    "        Args:\n",
    "            memories: List of memory records from retrieve_by_symbol\n",
    "            symbol: Stock symbol\n",
    "            \n",
    "        Returns:\n",
    "            Formatted string containing all relevant memory data\n",
    "        \"\"\"\n",
    "        if not memories:\n",
    "            return \"\"\n",
    "        \n",
    "        snapshot_parts = [f\"# Memory Data for {symbol}\\n\"]\n",
    "        \n",
    "        # Group memories by type (summary, routes, other)\n",
    "        summaries = []\n",
    "        routes = []\n",
    "        other = []\n",
    "        \n",
    "        for mem in memories:\n",
    "            text = mem.get('text', '')\n",
    "            tags = mem.get('tags', [])\n",
    "            timestamp = mem.get('timestamp', 'N/A')\n",
    "            \n",
    "            if 'summary' in tags or 'Summary' in text:\n",
    "                summaries.append(f\"## Entry from {timestamp}\\n{text}\\n\")\n",
    "            elif 'Routes:' in text:\n",
    "                routes.append(f\"- {text} (at {timestamp})\")\n",
    "            else:\n",
    "                other.append(f\"- {text} (at {timestamp})\")\n",
    "        \n",
    "        # Add summaries first (most important)\n",
    "        if summaries:\n",
    "            snapshot_parts.append(\"\\n## Previous Summaries\\n\")\n",
    "            # Use the most recent summaries (last 2)\n",
    "            snapshot_parts.extend(summaries[-2:])\n",
    "        \n",
    "        # Add routing info\n",
    "        if routes:\n",
    "            snapshot_parts.append(\"\\n## Previous Routing Information\\n\")\n",
    "            snapshot_parts.extend(routes[-3:])\n",
    "        \n",
    "        # Add other info\n",
    "        if other:\n",
    "            snapshot_parts.append(\"\\n## Other Notes\\n\")\n",
    "            snapshot_parts.extend(other[-5:])\n",
    "        \n",
    "        return \"\\n\".join(snapshot_parts)\n",
    "\n",
    "    def _extract_from_memory(self, memories: List[Dict[str, Any]], symbol: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Extract ingestion-like data structure from memory records.\n",
    "        Used when memory is sufficient and we don't need to fetch fresh data.\n",
    "        \n",
    "        Args:\n",
    "            memories: List of memory records\n",
    "            symbol: Stock symbol\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mimicking ingestion result structure\n",
    "        \"\"\"\n",
    "        \n",
    "        # Find the most recent comprehensive summary\n",
    "        most_recent = None\n",
    "        for mem in reversed(memories):\n",
    "            if 'summary' in mem.get('tags', []) and len(mem.get('text', '')) > 500:\n",
    "                most_recent = mem\n",
    "                break\n",
    "        \n",
    "        if not most_recent:\n",
    "            # Fallback: create minimal structure\n",
    "            return {\n",
    "                \"symbol\": symbol,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"financial_data\": None,\n",
    "                \"news_data\": None,\n",
    "                \"errors\": [{\"source\": \"memory\", \"note\": \"Using cached data\"}],\n",
    "                \"status\": \"success\"\n",
    "            }\n",
    "        \n",
    "        # Parse the memory text to extract structured data\n",
    "        return {\n",
    "            \"symbol\": symbol,\n",
    "            \"timestamp\": most_recent.get('timestamp', datetime.now().isoformat()),\n",
    "            \"financial_data\": {\n",
    "                \"source\": \"memory\",\n",
    "                \"summary\": most_recent.get('text', '')\n",
    "            },\n",
    "            \"news_data\": {\n",
    "                \"source\": \"memory\",\n",
    "                \"articles\": []\n",
    "            },\n",
    "            \"errors\": [],\n",
    "            \"status\": \"success\"\n",
    "        }\n",
    "\n",
    "# -----------------------------\n",
    "# Helper Function to Run Analysis\n",
    "# -----------------------------\n",
    "def run_investment_analysis(symbol: str, instructions: str) -> str:\n",
    "    \"\"\"\n",
    "    Convenience function to run a complete investment analysis.\n",
    "    \n",
    "    Args:\n",
    "        symbol: Stock ticker symbol (e.g., \"AAPL\", \"TSLA\", \"NVDA\")\n",
    "        instructions: User's query or analysis request\n",
    "        \n",
    "    Returns:\n",
    "        Final optimized summary as Markdown string\n",
    "        \n",
    "    Example:\n",
    "        result = run_investment_analysis(\"AAPL\", \"Should I buy Apple stock now?\")\n",
    "        display(Markdown(result))\n",
    "    \"\"\"\n",
    "    orchestrator = Orchestrator()\n",
    "    \n",
    "    return orchestrator.execute(symbol, instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "721c9582-9571-4781-861d-d292168c2a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " ORCHESTRATOR STARTING\n",
      "Symbol: AAPL\n",
      "Query: What is the investment outlook for Apple?\n",
      "============================================================\n",
      "\n",
      " Retrieved 39 previous memory entries for AAPL\n",
      "\n",
      " Analyzing memory sufficiency...\n",
      "Analysis Result:\n",
      "  - Data Sufficient: False\n",
      "  - Requires Fresh Data: True\n",
      "  - Missing Info: Data is too old (49.7 hours > 24 hour threshold), Current stock price of AAPL, Recent analyst ratings or price targets\n",
      "\n",
      " Fetching fresh data (memory insufficient)\n",
      "\n",
      "============================================================\n",
      "GENERATOR - Iteration 1\n",
      "============================================================\n",
      "User Request: What is the investment outlook for Apple?...\n",
      "Summary generated (3194 characters)\n",
      "\n",
      "============================================================\n",
      "EVALUATOR - Iteration 1\n",
      "============================================================\n",
      "Grade: PASS\n",
      "Quality Score: 9.0/10\n",
      "Issues Found: 0\n",
      "Feedback: The summary is well-structured and provides a comprehensive analysis of Apple's investment outlook, addressing the user's request effectively. It cove...\n",
      "\n",
      "Quality PASSED - Ending optimization\n",
      "\n",
      "============================================================\n",
      " ORCHESTRATOR COMPLETE\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Investment Research Summary: Apple Inc. (AAPL)\n",
       "\n",
       "## Investment Outlook\n",
       "\n",
       "Apple Inc. (AAPL) has shown strong performance in the technology sector, particularly in consumer electronics. This summary aims to provide an investment outlook based on financial metrics, recent trends, news sentiment, and risk factors.\n",
       "\n",
       "### 1. Financial Metrics\n",
       "\n",
       "- **Current Price**: $252.29\n",
       "- **Market Capitalization**: $3.74 trillion\n",
       "- **P/E Ratio**: 30.36\n",
       "- **Dividend Yield**: 0.41%\n",
       "- **52-Week Range**: $169.21 - $260.10\n",
       "- **Volume**: 48,876,500 shares (recent)\n",
       "- **Beta**: 1.094 (indicating volatility slightly above the market average)\n",
       "- **30-Day Volatility**: 25.91%\n",
       "\n",
       "#### Recent Price Trend (Last 5 Days):\n",
       "- Day 1: $247.66\n",
       "- Day 2: $247.77\n",
       "- Day 3: $249.34\n",
       "- Day 4: $247.45\n",
       "- Day 5: $252.29 (notable increase)\n",
       "\n",
       "### 2. Market Sentiment and News Analysis\n",
       "\n",
       "Recent headlines reflect a positive sentiment towards Apple, particularly in relation to its potential in AI technology. An analyst has projected a target price of $270, suggesting a growth potential of approximately 7% from the current price. The tech sector is gearing up for a year-end rally, which could benefit Apple as a leading player.\n",
       "\n",
       "**Key Headlines:**\n",
       "- \"Analyst Sees Apple as ‘Eventual Winner on AI at the Edge,’ Keeps $270 Target\" (Sat, 18 Oct 2025)\n",
       "- \"Tech Rally into Year-End: Momentum is Building for 3 Tech Stock Giants\" (Daniel Ives) (Sat, 18 Oct 2025)\n",
       "\n",
       "### 3. Investment Recommendation\n",
       "\n",
       "**Recommendation**: **Buy**\n",
       "\n",
       "#### Rationale:\n",
       "- **Growth Potential**: Apple’s current P/E ratio of 30.36, while high, reflects investor confidence in its growth trajectory. The target price of $270 from analysts presents a healthy upside.\n",
       "- **Market Position**: Apple continues to dominate the consumer electronics industry with a strong brand and loyal customer base, supporting consistent revenue growth.\n",
       "- **Technological Leadership**: The company’s advancements in AI and integration with its existing ecosystem may provide a competitive edge, as indicated by recent analyst commentary.\n",
       "\n",
       "### 4. Risks\n",
       "\n",
       "While the outlook is positive, investors should be aware of several risks:\n",
       "- **Valuation Concerns**: The high P/E ratio may deter value investors, indicating that the stock could be overvalued if growth does not meet expectations.\n",
       "- **Market Volatility**: A beta of 1.094 indicates that Apple shares are slightly more volatile than the market, which could pose risks during market downturns.\n",
       "- **Competitive Pressures**: The tech industry is highly competitive, and shifts in consumer preferences or advancements by competitors could impact Apple’s market share.\n",
       "\n",
       "### 5. Conclusion\n",
       "\n",
       "In summary, Apple Inc. presents a compelling investment opportunity driven by its market leadership, growth potential in AI, and overall positive sentiment in the tech sector. While acknowledging risks associated with valuation and market volatility, the current analysis leads to a **buy recommendation**, suggesting that investors consider adding AAPL to their portfolios for potential capital appreciation.\n",
       "\n",
       "--- \n",
       "\n",
       "This analysis is based on the most recent available data and should be revisited periodically to account for market changes and new information."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " ORCHESTRATOR STARTING\n",
      "Symbol: AAPL\n",
      "Query: What was the recommendation?\n",
      "============================================================\n",
      "\n",
      " Retrieved 42 previous memory entries for AAPL\n",
      "\n",
      " Analyzing memory sufficiency...\n",
      "Analysis Result:\n",
      "  - Data Sufficient: True\n",
      "  - Requires Fresh Data: False\n",
      "\n",
      " Using existing memory data (sufficient)\n",
      "\n",
      "============================================================\n",
      "GENERATOR - Iteration 1\n",
      "============================================================\n",
      "User Request: What was the recommendation?...\n",
      "Summary generated (3347 characters)\n",
      "\n",
      "============================================================\n",
      "EVALUATOR - Iteration 1\n",
      "============================================================\n",
      "Grade: PASS\n",
      "Quality Score: 9.0/10\n",
      "Issues Found: 0\n",
      "Feedback: ...\n",
      "\n",
      "Quality PASSED - Ending optimization\n",
      "\n",
      "============================================================\n",
      " ORCHESTRATOR COMPLETE\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Investment Research Summary: Apple Inc. (AAPL)\n",
       "\n",
       "## Investment Outlook\n",
       "\n",
       "As of October 19, 2025, Apple Inc. (AAPL) continues to demonstrate robust performance within the technology sector, driven by its innovative product offerings and strong brand loyalty. This summary provides a comprehensive investment outlook based on financial metrics, market sentiment, and risk factors.\n",
       "\n",
       "### 1. Financial Metrics\n",
       "\n",
       "- **Current Price**: $252.29\n",
       "- **Market Capitalization**: $3.74 trillion\n",
       "- **Price-to-Earnings (P/E) Ratio**: 30.36\n",
       "- **Dividend Yield**: 0.41%\n",
       "- **52-Week Price Range**: $169.21 - $260.10\n",
       "- **Recent Trading Volume**: 48,876,500 shares\n",
       "- **Beta**: 1.094 (indicating slightly higher volatility than the market)\n",
       "- **30-Day Volatility**: 25.91%\n",
       "\n",
       "#### Recent Price Trend (Last 5 Days):\n",
       "- Day 1: $247.66\n",
       "- Day 2: $247.77\n",
       "- Day 3: $249.34\n",
       "- Day 4: $247.45\n",
       "- Day 5: $252.29 (notable increase, indicating positive momentum)\n",
       "\n",
       "### 2. Market Sentiment and News Analysis\n",
       "\n",
       "Recent news coverage reflects a predominantly positive sentiment towards AAPL, with analysts highlighting strong sales figures for the iPhone 15 series and increased market share in both wearables and services sectors. The company’s commitment to sustainability and innovation has further bolstered its reputation among consumers and investors.\n",
       "\n",
       "Key developments include:\n",
       "- Strong quarterly earnings driven by robust iPhone sales, which exceeded analyst expectations.\n",
       "- Expansion into new markets and product categories, including augmented reality (AR) and health technology, which present growth opportunities.\n",
       "- Continued share repurchase programs and dividend increases, appealing to income-focused investors.\n",
       "\n",
       "### 3. Risks and Considerations\n",
       "\n",
       "Despite the positive outlook, several risks must be considered:\n",
       "- **Market Volatility**: With a beta of 1.094, AAPL shares are subject to market fluctuations, which could impact short-term performance.\n",
       "- **Competition**: The technology sector remains highly competitive, with pressures from rivals such as Samsung and emerging players in the smartphone and software markets.\n",
       "- **Supply Chain Challenges**: Potential disruptions in the supply chain could affect product availability and margins.\n",
       "\n",
       "### 4. Investment Recommendation\n",
       "\n",
       "**Recommendation**: **Buy**\n",
       "\n",
       "**Rationale**:\n",
       "- **Growth Potential**: AAPL's P/E ratio of 30.36, while higher than the industry average, reflects strong growth potential. Recent product launches and expansions into new sectors are likely to drive continued revenue growth.\n",
       "- **Positive Market Sentiment**: The recent increase in stock price and favorable news coverage suggest strong investor confidence and momentum.\n",
       "- **Strong Financials**: A market capitalization of $3.74 trillion underscores AAPL's position as a market leader, supported by consistent earnings and cash flow generation.\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "Given the combination of solid financial metrics, positive market sentiment, and growth opportunities, AAPL presents a compelling investment case for both growth and income-focused investors. The recommendation to buy is supported by the company's strategic initiatives and robust market presence, with a keen eye on potential risks.\n",
       "\n",
       "---\n",
       "\n",
       "This investment research summary is designed to provide a clear and actionable outlook for AAPL, addressing key factors that influence investment decisions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " ORCHESTRATOR STARTING\n",
      "Symbol: TSLA\n",
      "Query: Should I buy Tesla stock?\n",
      "============================================================\n",
      "\n",
      " Retrieved 6 previous memory entries for TSLA\n",
      "\n",
      " Analyzing memory sufficiency...\n",
      "Analysis Result:\n",
      "  - Data Sufficient: False\n",
      "  - Requires Fresh Data: True\n",
      "  - Missing Info: Data is too old (342.8 hours > 24 hour threshold), Current stock price of Tesla (TSLA), Recent news affecting Tesla's stock performance\n",
      "\n",
      " Fetching fresh data (memory insufficient)\n",
      "\n",
      "============================================================\n",
      "GENERATOR - Iteration 1\n",
      "============================================================\n",
      "User Request: Should I buy Tesla stock?...\n",
      "Summary generated (3828 characters)\n",
      "\n",
      "============================================================\n",
      "EVALUATOR - Iteration 1\n",
      "============================================================\n",
      "Grade: PASS\n",
      "Quality Score: 8.0/10\n",
      "Issues Found: 0\n",
      "Feedback: ...\n",
      "\n",
      "Quality PASSED - Ending optimization\n",
      "\n",
      "============================================================\n",
      " ORCHESTRATOR COMPLETE\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Investment Research Summary for Tesla, Inc. (TSLA)\n",
       "\n",
       "**Current Price**: $439.31  \n",
       "**Market Cap**: $1.46 trillion  \n",
       "**P/E Ratio**: 135.59  \n",
       "**Dividend Yield**: N/A  \n",
       "**52-Week Range**: $212.11 - $488.54  \n",
       "**Beta**: 2.086  \n",
       "**Sector**: Consumer Cyclical  \n",
       "**Industry**: Auto Manufacturers  \n",
       "**30-Day Volatility**: 52.67%\n",
       "\n",
       "---\n",
       "\n",
       "#### Company Overview\n",
       "Tesla, Inc. is a leading manufacturer of electric vehicles (EVs) and energy solutions. The company operates through two main segments: Automotive and Energy Generation and Storage. Tesla's commitment to innovation and sustainability has positioned it as a key player in the global transition toward renewable energy and electric mobility.\n",
       "\n",
       "---\n",
       "\n",
       "#### Recent Price Trend\n",
       "In the past five trading days, Tesla's stock has seen fluctuations with the following closing prices:\n",
       "- **Day 1**: $435.90\n",
       "- **Day 2**: $429.24\n",
       "- **Day 3**: $435.15\n",
       "- **Day 4**: $428.75\n",
       "- **Day 5**: $439.31\n",
       "\n",
       "The stock has demonstrated volatility, particularly with a beta of 2.086, indicating higher risk compared to the overall market.\n",
       "\n",
       "---\n",
       "\n",
       "#### Financial Metrics Analysis\n",
       "- **P/E Ratio**: At 135.59, Tesla's P/E ratio is significantly above the industry average, suggesting that the stock may be overvalued or that investors are expecting high growth rates.\n",
       "- **Market Cap**: A market cap of $1.46 trillion reflects strong investor confidence but also raises questions about long-term growth sustainability.\n",
       "- **52-Week Range**: The stock's range from $212.11 to $488.54 indicates substantial volatility, which may present both opportunities and risks.\n",
       "\n",
       "---\n",
       "\n",
       "#### News Analysis\n",
       "Recent news articles have highlighted several important factors:\n",
       "1. **Earnings Expectations**: There is heightened interest in Tesla's upcoming earnings report, with analysts speculating on potential impacts from broader economic conditions and competition.\n",
       "2. **Model Affordability**: Feedback on Tesla's efforts to produce more affordable models has been mixed, suggesting that market demand might not be as robust as expected.\n",
       "3. **Analyst Ratings**: Exane BNP Paribas has initiated coverage of Tesla with an 'Underperform' rating and a price target of $307, indicating concerns about valuation and growth prospects.\n",
       "\n",
       "---\n",
       "\n",
       "#### Risk Factors\n",
       "- **High Valuation**: The elevated P/E ratio and market cap raise concerns about Tesla's ability to meet growth expectations, particularly in a competitive market.\n",
       "- **Market Volatility**: The stock's high beta and recent price fluctuations indicate significant volatility, which may not be suitable for all investors.\n",
       "- **Economic Conditions**: Broader economic factors like inflation and rising interest rates could impact consumer spending on high-ticket items such as electric vehicles.\n",
       "\n",
       "---\n",
       "\n",
       "#### Investment Recommendation: **Hold**\n",
       "\n",
       "**Rationale**:\n",
       "- **Valuation Concerns**: Given the high P/E ratio and mixed sentiment from analysts, Tesla's stock may be overvalued at current levels. Investors should be cautious about entering at this price point.\n",
       "- **Market Dynamics**: The automotive industry is evolving rapidly, and while Tesla remains a leader, competition is increasing. Ongoing feedback on product affordability may impact future sales.\n",
       "- **Volatility**: The stock's high volatility (30-day volatility of 52.67%) suggests that while there is potential for upside, the risks are equally significant. A hold strategy allows investors to assess how upcoming earnings reports and market conditions unfold.\n",
       "\n",
       "---\n",
       "\n",
       "### Conclusion\n",
       "While Tesla remains a pivotal player in the EV and clean energy sectors, the current market conditions and valuation metrics suggest a cautious approach. A hold strategy is recommended as investors monitor earnings reports and broader market trends, allowing for a clearer picture of the company’s potential for sustainable growth moving forward."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run analysis - will use cache if available and sufficient\n",
    "result = run_investment_analysis(\"AAPL\", \"What is the investment outlook for Apple?\")\n",
    "display(Markdown(result))\n",
    "\n",
    "# Run follow-up question - should use cache\n",
    "result = run_investment_analysis(\"AAPL\", \"What was the recommendation?\")\n",
    "display(Markdown(result))\n",
    "\n",
    "# Different stock - will fetch fresh data\n",
    "result = run_investment_analysis(\"TSLA\", \"Should I buy Tesla stock?\")\n",
    "display(Markdown(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
